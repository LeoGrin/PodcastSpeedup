,title,summary,image,url,uploaded
0,Benjamin Todd on the core of effective altruism and how to argue for it (80k team chat #3),"Today’s episode is the latest conversation between Arden Koehler, and our CEO, Ben Todd.<br><br> 

Ben’s been thinking a lot about effective altruism recently, including what it really is, how it's framed, and how people misunderstand it.<br><br>  

We recently released an article on <b><a href=""https://80000hours.org/2020/08/misconceptions-effective-altruism/"">misconceptions about effective altruism</a></b> – based on Will MacAskill’s recent paper <em><b><a href=""https://80k.link/DEA"">The Definition of Effective Altruism</a></b></em> – and this episode can act as a companion piece.<br><br> 

<a href=""https://80000hours.org/podcast/episodes/ben-todd-on-the-core-of-effective-altruism/?utm_campaign=podcast__ben-todd&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>Links to learn more, summary and full transcript.</b></a><br><br>

Arden and Ben cover a bunch of topics related to effective altruism:<br><br> 

• How it isn’t just about donating money to fight poverty<br> 
• Whether it includes a moral obligation to give<br> 
• The rigorous argument for its importance<br> 
• Objections to that argument<br> 
• How to talk about effective altruism for people who aren't already familiar with it<br><br> 

Given that we’re in the same office, it’s relatively easy to record conversations between two 80k team members — so if you enjoy these types of bonus episodes, let us know at podcast@80000hours.org, and we might make them a more regular feature.<br><br> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b><br><br>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/e85892a2-fd1d-11ea-b69a-12230287f561/images/main.jpeg?1601340448869,http://feedproxy.google.com/~r/80000HoursPodcast/~5/WnRgWiRs3XM/ben-todd-on-the-core-of-effective-altruism-80k-team-chat-mp3.mp3,False
1,Ideas for high impact careers beyond our priority paths (Article),"<p>Today’s release is the latest in our series of audio versions of our articles.<br><br> 

In this one, we go through some more career options beyond our priority paths that seem promising to us for positively influencing the long-term future.<br><br> 

Some of these are likely to be written up as priority paths in the future, or wrapped into existing ones, but we haven’t written full profiles for them yet—for example policy careers outside AI and biosecurity policy that seem promising from a longtermist perspective.<br><br> 

Others, like information security, we think might be as promising for many people as our priority paths, but because we haven’t investigated them much we’re still unsure.<br><br> 

Still others seem like they’ll typically be less impactful than our priority paths for people who can succeed equally in either, but still seem high-impact to us and like they could be top options for a substantial number of people, depending on personal fit—for example research management.<br><br> 

Finally some—like becoming a public intellectual—clearly have the potential for a lot of impact, but we can’t recommend them widely because they don’t have the capacity to absorb a large number of people, are particularly risky, or both.<br><br> 

If you want to check out the links in today’s article, you can find those <a href=""https://80000hours.org/2020/08/ideas-for-high-impact-careers-beyond-our-priority-paths/?utm_campaign=podcast__other-problems&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>here.</b></a><br><br> 

Our <a href=""http://80000hours.org/survey/?utm_campaign=podcast__other-paths&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>annual user survey</b></a> is also now open for submissions.<br><br> 

Once a year for two weeks we ask all of you, our podcast listeners, article readers, advice receivers, and so on, so let us know how we've helped or hurt you.<br><br> 

80,000 Hours now offers many different services, and your feedback helps us figure out which programs to keep, which to cut, and which to expand.<br><br> 

This year we have a new section covering the podcast, asking what kinds of episodes you liked the most and want to see more of, what extra resources you use, and some other questions too.<br><br> 

We're always especially interested to hear ways that our work has influenced what you plan to do with your life or career, whether that impact was positive, neutral, or negative.<br><br> 

That might be a different focus in your existing job, or a decision to study something different or look for a new job. Alternatively, maybe you're now planning to volunteer somewhere, or donate more, or donate to a different organisation.<br><br> 

Your responses to the survey will be carefully read as part of our upcoming annual review, and we'll use them to help decide what 80,000 Hours should do differently next year.<br><br> 

So please do take a moment to fill out the user survey before it closes&nbsp;on Sunday (13th of September).<br><br> 

You can find it at <a href=""http://80000hours.org/survey/?utm_campaign=podcast__other-paths&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>80000hours.org/survey</b></a><br><br> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b><br><br>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em></p>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/f6f5e4e2-eed4-11ea-b33d-0ef24749efad/images/main.jpg?1601340448965,http://feedproxy.google.com/~r/80000HoursPodcast/~5/WDNorQZUNFQ/ideas-for-high-impact-careers-beyond-our-priority-paths-article-mp3.mp3,False
2,"Benjamin Todd on varieties of longtermism and things 80,000 Hours might be getting wrong (80k team chat #2)","Today’s bonus episode is a conversation between Arden Koehler, and our CEO, Ben Todd.<br><br> 

Ben’s been doing a bunch of research recently, and we thought it’d be interesting to hear about how he’s currently thinking about a couple of different topics – including different types of longtermism, and things 80,000 Hours might be getting wrong.<br><br>  

<a href=""https://80000hours.org/podcast/episodes/ben-todd-on-varieties-of-longtermism/?utm_campaign=podcast__guest-name&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>Links to learn more, summary and full transcript.</b></a><br><br>

This is very off-the-cut compared to our regular episodes, and just 54 minutes long.<br><br> 

In the first half, Arden and Ben talk about varieties of longtermism:<br><br> 

• Patient longtermism<br> 
• Broad urgent longtermism<br> 
• Targeted urgent longtermism focused on existential risks<br> 
• Targeted urgent longtermism focused on other trajectory changes<br> 
• And their distinctive implications for people trying to do good with their careers.<br><br>  

In the second half, they move on to:<br><br> 

• How to trade-off transferable versus specialist career capital<br> 
• How much weight to put on personal fit<br> 
• Whether we might be highlighting the wrong problems and career paths.<br><br> 

Given that we’re in the same office, it’s relatively easy to record conversations between two 80k team members — so if you enjoy these types of bonus episodes, let us know at podcast@80000hours.org, and we might make them a more regular feature.<br><br> 

Our <a href=""http://80000hours.org/survey/?utm_campaign=podcast__bt-1&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>annual user survey</b></a> is also now open for submissions.<br><br> 

Once a year for two weeks we ask all of you, our podcast listeners, article readers, advice receivers, and so on, so let us know how we've helped or hurt you.<br><br> 

80,000 Hours now offers many different services, and your feedback helps us figure out which programs to keep, which to cut, and which to expand.<br><br> 

This year we have a new section covering the podcast, asking what kinds of episodes you liked the most and want to see more of, what extra resources you use, and some other questions too.<br><br> 

We're always especially interested to hear ways that our work has influenced what you plan to do with your life or career, whether that impact was positive, neutral, or negative.<br><br> 

That might be a different focus in your existing job, or a decision to study something different or look for a new job. Alternatively, maybe you're now planning to volunteer somewhere, or donate more, or donate to a different organisation.<br><br> 

Your responses to the survey will be carefully read as part of our upcoming annual review, and we'll use them to help decide what 80,000 Hours should do differently next year.<br><br> 

So please do take a moment to fill out the user survey.<br><br> 

You can find it at <a href=""http://80000hours.org/survey/?utm_campaign=podcast__bt-1&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>80000hours.org/survey</b></a><br><br> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b><br><br>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/d36b2f70-ec66-11ea-a788-0e9351e7db55/images/main.jpg?1601340449062,http://feedproxy.google.com/~r/80000HoursPodcast/~5/ELm7TFXA3-U/ben-todd-on-varieties-of-longtermism-and-things-80000-hours-might-be-getting-wrong-mp3.mp3,False
3,"Global issues beyond 80,000 Hours’ current priorities (Article)","Today’s release is the latest in our series of audio versions of our articles.<br><br> 

In this one, we go through 30 global issues beyond the ones we usually prioritize most highly in our work, and that you might consider focusing your career on tackling.<br><br> 

Although we spend the majority of our time at 80,000 Hours on our highest priority problem areas, and we recommend working on them to many of our readers, these are just the most promising issues among those we’ve spent time investigating. There are many other global issues that we haven’t properly investigated, and which might be very promising for more people to work on.<br><br> 

In fact, we think working on some of the issues in this article could be as high-impact for some people as working on our priority problem areas — though we haven’t looked into them enough to be confident.<br><br> 

If you want to check out the links in today’s article, you can find those <a href=""https://80000hours.org/2020/08/other-problems/?utm_campaign=podcast__other-problems&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>here.</b></a><br><br> 

Our <a href=""http://80000hours.org/survey/?utm_campaign=podcast__other-problems&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>annual user survey</b></a> is also now open for submissions.<br><br> 

Once a year for two weeks we ask all of you, our podcast listeners, article readers, advice receivers, and so on, so let us know how we've helped or hurt you.<br><br> 

80,000 Hours now offers many different services, and your feedback helps us figure out which programs to keep, which to cut, and which to expand.<br><br> 

This year we have a new section covering the podcast, asking what kinds of episodes you liked the most and want to see more of, what extra resources you use, and some other questions too.<br><br> 

We're always especially interested to hear ways that our work has influenced what you plan to do with your life or career, whether that impact was positive, neutral, or negative.<br><br> 

That might be a different focus in your existing job, or a decision to study something different or look for a new job. Alternatively, maybe you're now planning to volunteer somewhere, or donate more, or donate to a different organisation.<br><br> 

Your responses to the survey will be carefully read as part of our upcoming annual review, and we'll use them to help decide what 80,000 Hours should do differently next year.<br><br> 

So please do take a moment to fill out the user survey.<br><br> 

You can find it at <a href=""http://80000hours.org/survey/?utm_campaign=podcast__other-problems&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>80000hours.org/survey</b></a><br><br> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b><br><br>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/1ad3f726-e941-11ea-85ac-12037350e273/images/main.jpg?1601340449151,http://feedproxy.google.com/~r/80000HoursPodcast/~5/Bn_9eLGOiUY/other-problems-article-mp3.mp3,False
4,"#85 - Mark Lynas on climate change, societal collapse & nuclear energy","A golf-ball sized lump of uranium can deliver more than enough power to cover all of your lifetime energy use. To get the same energy from coal, you’d need 3,200 tonnes of black rock — a mass equivalent to 800 adult elephants, which would produce more than 11,000 tonnes of CO2. That’s about 11,000 tonnes more than the uranium.<br><br>  

Many people aren’t comfortable with the danger posed by nuclear power. But given the climatic stakes, it’s worth asking: Just how much more dangerous is it compared to fossil fuels?<br><br> 

According to today’s guest, Mark Lynas — author of <em><b><a href=""https://80k.link/OFW"">Six Degrees: Our Future on a Hotter Planet</a></b></em> (winner of the prestigious Royal Society Prizes for Science Books) and <em><b><a href=""https://80k.link/N20"">Nuclear 2.0</a></b></em>&nbsp;— it’s actually much, much <i>safer</i>.<br><br> 

<a href=""https://80k.link/MLPod""><b>Links to learn more, summary and full transcript.</b></a><br><br>  

Climatologists James Hansen and Pushker Kharecha calculated that the use of nuclear power between 1971 and 2009 avoided the premature deaths of 1.84 million people by avoiding air pollution from burning coal.<br><br>  

What about radiation or nuclear disasters? According to Our World In Data, in generating a given amount of electricity, nuclear, wind, and solar all cause about the same number of deaths — and it's a tiny number.<br><br> 

So what’s going on? Why isn’t everyone demanding a massive scale-up of nuclear energy to save lives and stop climate change? Mark and many other activists believe that unchecked climate change will result in the collapse of human civilization, so the stakes could not be higher.<br><br> 


Mark says that many environmentalists — including him — simply grew up with anti-nuclear attitudes all around them (possibly stemming from a conflation of nuclear weapons and nuclear energy) and haven't thought to question them.<br><br>  

But he thinks that once you believe in the climate emergency, you have to rethink your opposition to nuclear energy.<br><br>  

At 80,000 Hours we haven’t analysed the merits and flaws of the case for nuclear energy — especially compared to wind and solar paired with gas, hydro, or battery power to handle intermittency — but Mark is convinced.<br><br>  

He says it comes down to physics: Nuclear power is just so much denser.<br><br> 

We need to find an energy source that provides carbon-free power to ~10 billion people, and we need to do it while humanity is doubling or tripling (or more) its energy demand.<br><br>  

How do you do that without destroying the world's ecology? Mark thinks that nuclear is the only way.<br><br>  

<a href=""https://80k.link/MLPod""><b>Read a more in-depth version of the case for nuclear energy in the full blog post.</b></a><br><br> 

For Mark, the only argument against nuclear power is a political one -- that people won't want or accept it.<br><br> 

He says that he knows people in all kinds of mainstream environmental groups — such as Greenpeace — who agree that nuclear must be a vital part of any plan to solve climate change. But, because they think they'll be ostracized if they speak up, they keep their mouths shut.<br><br> 

Mark thinks this willingness to indulge beliefs that contradict scientific evidence stands in the way of actually fully addressing climate change, and so he’s helping to build a movement of folks who are out and proud about their support for nuclear energy.<br><br> 

This is only one topic of many in today’s interview. Arden, Rob, and Mark also discuss:<br><br> 

• At what degrees of warming does societal collapse become likely<br> 
• Whether climate change could lead to human extinction<br> 
• What environmentalists are getting wrong about climate change<br>  
• And much more.<br><br> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b><br><br>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/dfec50fc-e30c-11ea-a7df-0e0ca07a19b3/images/main.jpg?1601340449293,http://feedproxy.google.com/~r/80000HoursPodcast/~5/Vs7jCSzAXMw/85--mark-lynas-mp3.mp3,False
5,#84 - Shruti Rajagopalan on what India did to stop COVID-19 and how well it worked,"When COVID-19 struck the US, everyone was told that hand sanitizer needed to be saved for healthcare professionals, so they should just wash their hands instead. But in India, many homes lack reliable piped water, so they had to do the opposite: distribute hand sanitizer as widely as possible.<br><br>

American advocates for banning single-use plastic straws might be outraged at the widespread adoption of single-use hand sanitizer sachets in India. But the US and India are very different places, and it might be the only way out when you're facing a pandemic without running water.<br><br>

According to today’s guest, Shruti Rajagopalan, Senior Research Fellow at the <i>Mercatus Center</i> at George Mason University, that's typical and context is key to policy-making. This prompted Shruti to propose a set of <a href=""https://80k.link/srpaper"">policy responses</a> designed for India specifically back in April.<br><br>

Unfortunately she thinks it's surprisingly hard to know what one should and shouldn't imitate from overseas.<br><br>

<a href=""https://80k.link/sr""><b>Links to learn more, summary and full transcript.</b></a><br><br>

For instance, some places in India installed shared handwashing stations in bus stops and train stations, which is something no developed country would advise. But in India, you can't necessarily wash your hands at home — so shared faucets might be the lesser of two evils. (Though note scientists have downgraded the importance of hand hygiene lately.)<br><br>

Stay-at-home orders offer a more serious example. Developing countries find themselves in a serious bind that rich countries do not.<br><br>
 
With nearly no slack in healthcare capacity, India lacks equipment to treat even a small number of COVID-19 patients. That suggests strict controls on movement and economic activity might be necessary to control the pandemic.<br><br>
 
But many people in India and elsewhere can't afford to shelter in place for weeks, let alone months. And governments in poorer countries may not be able to afford to send everyone money — even where they have the infrastructure to do so fast enough.<br><br>
 
India ultimately did impose strict lockdowns, lasting almost 70 days, but the human toll has been larger than in rich countries, with vast numbers of migrant workers stranded far from home with limited if any income support.<br><br>

There were no trains or buses, and the government made no provision to deal with the situation. Unable to afford rent where they were, many people had to walk hundreds of kilometers to reach home, carrying children and belongings with them. <br><br>
 
But in some other ways the context of developing countries is more promising. In the US many people melted down when asked to wear facemasks. But in South Asia, people just wore them.<br><br>
 
Shruti isn’t sure whether that's because of existing challenges with high pollution, past experiences with pandemics, or because intergenerational living makes the wellbeing of others more salient, but the end result is that masks weren’t politicised in the way they were in the US.<br><br>
 
In addition, despite the suffering caused by India's policy response to COVID-19, public support for the measures and the government remains high — and India's population is much younger and so less affected by the virus.<br><br>
 
In this episode, Howie and Shruti explore the unique policy challenges facing India in its battle with COVID-19, what they've tried to do, and how it has gone.<br><br>

They also cover:<br><br>

• What an economist can bring to the table during a pandemic<br>
• The mystery of India’s surprisingly low mortality rate<br>
• Policies that should be implemented today<br>
• What makes a good constitution<br><br>

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app.</b><br><br>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/3b705026-dda5-11ea-afb8-0e4d332dadc1/images/main.jpg?1601340449409,http://feedproxy.google.com/~r/80000HoursPodcast/~5/Rb-VIPpzxmQ/84--shruti-rajagopalan-on-indias-response-to-covid-19.mp3,False
6,#83 - Prof Jennifer Doleac on preventing crime without police and prisons,"The killing of George Floyd has prompted a great deal of debate over whether the US should reduce the size of its police departments. The research literature suggests that the presence of police officers does reduce crime, though they're expensive and as is increasingly recognised, impose substantial harms on the populations they are meant to be protecting, especially communities of colour.<br><br> 

So maybe we ought to shift our focus to effective but unconventional approaches to crime prevention, approaches that don't require police or prisons and the human toll they bring with them.<br><br> 

Today’s guest, Jennifer Doleac — Associate Professor of Economics at Texas A&amp;M University, and Director of the <i>Justice Tech Lab</i> — is an expert on empirical research into policing, law and incarceration. In this extensive interview, she highlights three alternative ways to effectively prevent crime: better street lighting, cognitive behavioral therapy, and lead reduction.<br><br> 

One of Jennifer’s papers used switches into and out of daylight saving time as a 'natural experiment' to measure the effect of light levels on crime. One day the sun sets at 5pm; the next day it sets at 6pm. When that evening hour is dark instead of light, robberies during it roughly double.<br><br> 

<a href=""https://80k.link/jdpod""><b>Links to sources for the claims in these show notes, other resources to learn more, and a full transcript.</b></a><br><br> 

The idea here is that if you try to rob someone in broad daylight, they might see you coming, and witnesses might later be able to identify you. You're just more likely to get caught.<br><br> 

You might think: ""Well, people will just commit crime in the morning instead"". But it looks like criminals aren’t early risers, and that doesn’t happen.<br><br> 

On her unusually rigorous podcast <a href=""https://80k.link/pcac"">Probable Causation</a>, Jennifer spoke to one of the authors of a related study, in which very bright streetlights were randomly added to some public housing complexes but not others. They found the lights reduced outdoor night-time crime by 36%, at little cost. <br><br> 

The next best thing to sun-light is human-light, so just installing more streetlights might be one of the easiest ways to cut crime, without having to hassle or punish anyone.<br><br> 

The second approach is cognitive behavioral therapy (CBT), in which you're taught to slow down your decision-making, and think through your assumptions before acting.<br><br> 

There was a randomised controlled trial done in schools, as well as juvenile detention facilities in Chicago, where the kids assigned to get CBT were followed over time and compared with those who were not assigned to receive CBT. They found the CBT course reduced rearrest rates by a third, and lowered the likelihood of a child returning to a juvenile detention facility by 20%.<br><br> 

Jennifer says that the program isn’t that expensive, and the benefits are massive. Everyone would probably benefit from being able to talk through their problems but the gains are especially large for people who've grown up with the trauma of violence in their lives.<br><br> 

Finally, Jennifer thinks that lead reduction might be the best buy of all in crime prevention…<br><br> 

<b>Blog post truncated due to length limits. <a href=""https://80k.link/jdpod"">Finish reading the full post here.</a></b><br><br> 

In today’s conversation, Rob and Jennifer also cover, among many other things:<br><br> 

• Misconduct, hiring practices and accountability among US police<br>
• Procedural justice training<br>
• Overrated policy ideas<br>
• Policies to try to reduce racial discrimination<br>
• The effects of DNA databases<br>
• Diversity in economics<br>
• The quality of social science research<br><br> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app.</b><br><br>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/732ff4bc-d364-11ea-9f3a-12e29931da63/images/main.jpg?1601340449500,http://feedproxy.google.com/~r/80000HoursPodcast/~5/j-hcD-2S9oE/83---jennifer-doleac-on-unconventional-approaches-to-prevent-crime.mp3,False
7,#82 - Prof James Forman Jr on reducing the cruelty of the US criminal legal system,"No democracy has ever incarcerated as many people as the United States. To get its incarceration rate down to the global average, the US would have to release 3 in 4 people in its prisons today. <br><br> 

The effects on Black Americans have been especially severe — Black people make up 12% of the US population but 33% of its prison population. In the early 2000's when incarceration reached its peak, the US government estimated that 32% of Black boys would go to prison at some point in their lives, 5.5 times the figure for whites. <br><br>

Contrary to popular understanding, nonviolent drug offenders make up <a href=""https://www.prisonpolicy.org/reports/pie2020.html""><b>less than a fifth</b></a> of the incarcerated population. The only way to get its incarceration rate near the global average will be to shorten prison sentences for so-called 'violent criminals' — a politically toxic idea. But could we change that?<br><br> 

According to today’s guest, Professor James Forman Jr — a former public defender in Washington DC, Pulitzer Prize-winning author of <em><b><a href=""https://80k.link/JFJ"">Locking Up Our Own: Crime and Punishment in Black America</a></b></em>, and now a professor at Yale Law School — there are two things we have to do to make that happen.<br><br> 

<a href=""https://80k.link/JFJpod""><b>Links to learn more, summary and full transcript.</b></a><br><br> 

First, he thinks we should lose the term 'violent offender', and maybe even 'violent crime'. When you say 'violent crime', most people immediately think of murder and rape — but they're only a small fraction of the crimes that the law deems as violent.<br><br> 

In reality, the crime that puts the most people in prison in the US is robbery. And the law says that robbery is a violent crime whether a weapon is involved or not. By moving away from the catch-all category of 'violent criminals' we can judge the risk posed by individual people more sensibly.<br><br>  

Second, he thinks we should embrace the restorative justice movement. Instead of asking ""What was the law? Who broke it? What should the punishment be"", restorative justice asks ""Who was harmed? Who harmed them? And what can we as a society, including the person who committed the harm, do to try to remedy that harm?""<br><br> 

Instead of being narrowly focused on how many years people should spend in prison as retribution, it starts a different conversation.<br><br>  

You might think this apparently softer approach would be unsatisfying to victims of crime.  But James has discovered that a lot of victims of crime find that the current system doesn't help them in any meaningful way. What they primarily want to know is: why did this happen to me?<br><br>  

The best way to find that out is to actually talk to the person who harmed them, and in doing so gain a better understanding of the underlying factors behind the crime. The restorative justice approach facilitates these conversations in a way the current system doesn't allow, and can include restitution, apologies, and face-to-face reconciliation.<br><br> 

That’s just one topic of many covered in today’s episode, with much of the conversation focusing on Professor Forman’s 2018 book <em>Locking Up Our Own</em> — an examination of the historical roots of contemporary criminal justice practices in the US, and his experience setting up a charter school for at-risk youth in DC.<br><br> 

Rob and James also discuss:<br><br> 

• How racism shaped the US criminal legal system<br> 
• How Black America viewed policing through the 20th century<br> 
• How class divisions fostered a 'tough on crime' approach<br> 
• How you can have a positive impact as a public prosecutor<br><br> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b><br><br>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/b7f03eb0-d042-11ea-8e60-0ef97b41ad5b/images/main.jpg?1601340449652,http://feedproxy.google.com/~r/80000HoursPodcast/~5/rKZQ6THTFEo/82---james-forman-jr-on-the-cruelty-of-the-us-criminal-legal-system.mp3,False
8,#81 - Ben Garfinkel on scrutinising classic AI risk arguments,"<p>80,000 Hours, along with many other members of the effective altruism movement, has argued that helping to positively shape the development of artificial intelligence may be one of the best ways to have a lasting, positive impact on the long-term future. Millions of dollars in philanthropic spending, as well as lots of career changes, have been motivated by these arguments.<br><br> 

Today’s guest, Ben Garfinkel, Research Fellow at Oxford’s Future of Humanity Institute, supports the continued expansion of AI safety as a field and believes working on AI is among the very best ways to have a positive impact on the long-term future. But he also believes the classic AI risk arguments have been subject to insufficient scrutiny given this level of investment.<br><br> 

In particular, the case for working on AI if you care about the long-term future has often been made on the basis of concern about AI accidents; it’s actually quite difficult to design systems that you can feel confident will behave the way you want them to in all circumstances.<br><br> 

<a href=""https://80k.link/BostromTT"">Nick Bostrom</a> wrote the most fleshed out version of the argument in his book, Superintelligence. But Ben reminds us that, apart from Bostrom’s book and essays by Eliezer Yudkowsky, there's very little existing writing on existential accidents.<br><br> 

<a href=""https://80k.link/BGpod""><b>Links to learn more, summary and full transcript.</b></a><br><br>

There have also been very few skeptical experts that have actually sat down and fully engaged with it, writing down point by point where they disagree or where they think the mistakes are. This means that Ben has probably scrutinised classic AI risk arguments as carefully as almost anyone else in the world.<br><br> 

He thinks that most of the arguments for existential accidents often rely on fuzzy, abstract concepts like optimisation power or general intelligence or goals, and <a href=""https://80k.link/pcm"">toy thought experiments</a>. And he doesn’t think it’s clear we should take these as a strong source of evidence.<br><br> 

Ben’s also concerned that these scenarios often involve massive jumps in the capabilities of a single system, but it's really not clear that we should expect such jumps or find them plausible.

These toy examples also focus on the idea that because human preferences are so nuanced and so hard to state precisely, it should be quite difficult to get a machine that can understand how to obey them.<br><br>  

But Ben points out that it's also the case in machine learning that we can train lots of systems to engage in behaviours that are actually quite nuanced and that we can't specify precisely. If AI systems can recognise faces from images, and fly helicopters, why don’t we think they’ll be able to understand human preferences?<br><br> 

Despite these concerns, Ben is still fairly optimistic about the value of working on AI safety or governance.<br><br>  

He doesn’t think that there are any slam-dunks for improving the future, and so the fact that there are at least plausible pathways for impact by working on AI safety and AI governance, in addition to it still being a very neglected area, puts it head and shoulders above most areas you might choose to work in.<br><br>  

This is the second episode hosted by our Strategy Advisor Howie Lempel, and he and Ben cover, among many other things:<br><br> 

• The threat of AI systems increasing the risk of permanently damaging conflict or collapse<br> 
• The possibility of permanently locking in a positive or negative future<br> 
• Contenders for types of advanced systems<br> 
• What role AI should play in the effective altruism portfolio<br><br> 

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b><br><br>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em></p>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/c2242b1a-c1f3-11ea-9bab-0e13d48c95ef/images/main.jpg?1601340449742,http://feedproxy.google.com/~r/80000HoursPodcast/~5/qRJ36Chpibs/81---ben-garfinkel-on-scrutinising-classic-ai-risk-arguments-mp3.mp3,False
9,Advice on how to read our advice (Article),"<p><i>This is the fourth release in our new series of audio articles.&nbsp;If you want to read the original article or check out the links within it, you can find them&nbsp;<a href=""https://80000hours.org/articles/advice-on-how-to-read-our-advice/?utm_campaign=podcast__advice-on-our-advice-audio&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>here</b></a>. </i><br><br>

""We’ve found that readers sometimes interpret or apply our advice in ways we didn’t anticipate and wouldn’t exactly recommend. That’s hard to avoid when you’re writing for a range of people with different personalities and initial views. <br><br>

To help get on the same page, here’s some advice about our advice, for those about to launch into reading our site. <br><br>

We want our writing to inform people’s views, but only in proportion to the likelihood that we’re actually right. So we need to make sure you have a balanced perspective on how compelling the evidence is for the different claims we make on the site, and how much weight to put on our advice in your situation. <br><br>

This piece includes a list of points to bear in mind when reading our site, and some thoughts on how to avoid the communication problems we face..."" <br><br>

As the title suggests, this was written with our web site content in mind, but plenty of it applies to the careers sections of the podcast too — as well as our bonus episodes with members of the 80,000 Hours team, such as Arden and Rob’s episode on demandingness, work-life balance and injustice, which aired on February 25th of this year. <br><br>

And if you have feedback on these, positive or negative, it’d be great if you could email us at podcast@80000hours.org.</p>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/b77b1f98-ba37-11ea-a113-0e5125d2c025/images/main.png?1601340449830,http://feedproxy.google.com/~r/80000HoursPodcast/~5/1dxWzxscnBA/how-to-read-our-advice-jun-26-mp3.mp3,False
10,#80 - Professor Stuart Russell on why our approach to AI is broken and how to fix it,"Stuart Russell, Professor at UC Berkeley and co-author of the <a href=""http://aima.cs.berkeley.edu/""><b>most popular AI textbook</b></a>, thinks the way we approach machine learning today is fundamentally flawed.<br><br>

In his new book, <em><b><a href=""https://80k.link/hcbook"">Human Compatible</a></b></em>, he outlines the 'standard model' of AI development, in which intelligence is measured as the ability to achieve some definite, completely-known objective that we've stated explicitly. This is so obvious it almost doesn't even seem like a design choice, but it is.<br><br>

Unfortunately there's a big problem with this approach: it's incredibly hard to say exactly what you want. AI today lacks common sense, and simply does whatever we've asked it to. That's true even if the goal isn't what we really want, or the methods it's choosing are ones we would never accept.<br><br>

We already see <a href=""https://80k.link/aiwe""><b>AIs misbehaving</b></a> for this reason. Stuart points to the example of YouTube's recommender algorithm, which reportedly nudged users towards extreme political views because that made it easier to keep them on the site. This isn't something we wanted, but it helped achieve the algorithm's objective: maximise viewing time.<br><br>

Like King Midas, who asked to be able to turn everything into gold but ended up unable to eat, we get too much of what we've asked for.<br><br>

<a href=""https://80k.link/srpod""><b>Links to learn more, summary and full transcript.</b></a><br><br>

This 'alignment' problem will get more and more severe as machine learning is embedded in more and more places: recommending us news, operating power grids, deciding prison sentences, doing surgery, and fighting wars. If we're ever to hand over much of the economy to thinking machines, we can't count on ourselves correctly saying exactly what we want the AI to do every time.<br><br>

Stuart isn't just dissatisfied with the current model though, he has a specific solution. According to him we need to redesign AI around 3 principles:<br><br>

1. The AI system's objective is to achieve what humans want.<br>
2. But the system isn't sure what we want.<br>
3. And it figures out what we want by observing our behaviour.<br>

Stuart thinks this design architecture, if implemented, would be a big step forward towards reliably beneficial AI. <br><br>

For instance, a machine built on these principles would be happy to be turned off if that's what its owner thought was best, while one built on the standard model should resist being turned off because being deactivated prevents it from achieving its goal. As Stuart says, ""you can't fetch the coffee if you're dead.""<br><br>

These principles lend themselves towards machines that are modest and cautious, and check in when they aren't confident they're truly achieving what we want.<br><br>

We've made progress toward putting these principles into practice, but the remaining engineering problems are substantial. Among other things, the resulting AIs need to be able to interpret what people really mean to say based on the context of a situation. And they need to guess when we've rejected an option because we've considered it and decided it's a bad idea, and when we simply haven't thought about it at all.<br><br>

Stuart thinks all of these problems are surmountable, if we put in the work. The harder problems may end up being social and political.<br><br>

When each of us can have an AI of our own — one smarter than any person — how do we resolve conflicts between people and their AI agents?

And if AIs end up doing most work that people do today, how can humans avoid becoming enfeebled, like lazy children tended to by machines, but not intellectually developed enough to know what they really want?<br><br>

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b><br><br>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/14261466-b4d7-11ea-a57b-0e9ad5d55cf9/images/main.jpg?1601340449918,http://feedproxy.google.com/~r/80000HoursPodcast/~5/p5vGK1Txchg/80---stuart-russell-on-how-our-model-of-ai-is-broken-and-how-to-fix-it.mp3,False
11,What anonymous contributors think about important life and career questions (Article),"Today we’re launching the final entry of our ‘anonymous answers' series on the website.<br><br> 

It features answers to 23 different questions including <i>“How have you seen talented people fail in their work?”</i> and <i>“What’s one way to be successful you don’t think people talk about enough?”</i>, from anonymous people whose work we admire.<br><br> 

We thought a lot of the responses were really interesting; some were provocative, others just surprising. And as intended, they span a very wide range of opinions.<br><br> 

So we decided to share some highlights here with you podcast subscribers. This is only a sample though, including a few answers from just 10 of those 23 questions.<br><br> 

You can find the rest of the answers at <a href=""https://80000hours.org/articles/anonymous-answers/?utm_campaign=podcast__anonymous-answers-audio&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>80000hours.org/anonymous</b></a> or follow a link here to an individual entry:<br><br> 

1. What's good career <a href=""https://80000hours.org/2019/10/anonymous-advice-careers/""><b>advice you wouldn’t want to have your name on?</b></a><br>
2. How have you seen <a href=""https://80000hours.org/2019/10/anonymous-advice-fail-at-work/""><b>talented people fail in their work?</b></a><br>
3. What’s the thing <a href=""https://80000hours.org/2019/11/anonymous-answers-most-overrated/""><b>people most overrate in their career?</b></a><br>
4. If you were at the start of your career again, <a href=""https://80000hours.org/2019/11/anonymous-answers-personal-reflections/""><b>what would you do differently this time?</b></a> <br>
5. If you're a talented young person <a href=""https://80000hours.org/2019/11/anonymous-answers-risk-aversion/""><b>how risk averse should you be?</b></a><br>
6. Among people trying to improve the world, <a href=""https://80000hours.org/2019/12/anonymous-answers-bad-habits/""><b>what are the bad habits you see most often?</b></a><br>
7. What mistakes do people most often make <a href=""https://80000hours.org/2019/12/anon-answers-what-to-work-on/""><b>when deciding what work to do?</b></a><br>
8. What's one way to be successful you don't think people <a href=""https://80000hours.org/2020/01/anon-answers-one-way-successful/""><b>talk about enough?</b></a><br>
9. How honest &amp; candid should high-profile people <a href=""https://80000hours.org/2020/02/anon-answers-honesty/""><b>really be?</b></a><br>
10. What’s some underrated <a href=""https://80000hours.org/2020/02/anonymous-answers-general-life-advice/""><b>general life advice?</b></a><br>
11. Should the effective altruism community grow <a href=""https://80000hours.org/2020/02/anonymous-answers-effective-altruism-community-and-growth/""><b>faster or slower? And should it be broader, or narrower?</b></a><br>
12. What are the <a href=""https://80000hours.org/2020/02/anonymous-answers-flaws-80000hours/""><b>biggest flaws of 80,000 Hours?</b></a><br>
13. What are the <a href=""https://80000hours.org/2020/02/anonymous-answers-flaws-effective-altruism-community""><b>biggest flaws of the effective altruism community?</b></a><br>
14. How should the effective altruism community <a href=""https://80000hours.org/2020/04/anonymous-answers-diversity/""><b>think about diversity?</b></a><br>
15. Are there any myths that you feel obligated to support publicly? <a href=""https://80000hours.org/2020/06/anonymous-answers-myths-and-other-questions/""><b>And five other questions.</b></a><br><br>

Finally, if you’d like us to produce more or less content like this, please let us know your opinion podcast@80000hours.org.",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/da846b14-a742-11ea-8629-1292b68da39f/images/main.jpg?1601340450004,http://feedproxy.google.com/~r/80000HoursPodcast/~5/sn6ocyCpZHo/article--anonymous-answers-jun-4-128-kbps-mp3.mp3,False
12,"#79 - A.J. Jacobs on radical honesty, following the whole Bible, and reframing global problems as puzzles","Today’s guest, <i>New York Times</i> bestselling author A.J. Jacobs, always hated Judge Judy. But after he found out that she was his seventh cousin, he thought, ""You know what, she's not so bad"".<br><br> 

Hijacking this bias towards family and trying to broaden it to everyone led to his three-year adventure to help build the <a href=""https://80k.link/AJ1""><i>biggest family tree in history</i></a>.<br><br> 

He’s also spent months saying whatever was on his mind, tried to become the healthiest person in the world, read 33,000 pages of facts, spent a year following the Bible literally, thanked everyone involved in making his morning cup of coffee, and tried to figure out how to do the most good. His next book will ask: if we reframe global problems as puzzles, would the world be a better place?<br><br> 

<a href=""https://80000hours.org/podcast/episodes/aj-jacobs-on-writing-reframing-problems-as-puzzles/?utm_campaign=podcast__aj-jacobs&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>Links to learn more, summary and full transcript.</b></a><br><br>

This is the first time I’ve hosted the podcast, and I’m hoping to convince people to listen with this attempt at clever show notes that change style each paragraph to reference different A.J. experiments. I don’t actually think it’s that clever, but all of my other ideas seemed worse. I really have no idea how people will react to this episode; I loved it, but I definitely think I’m more entertaining than almost anyone else will. (<a href=""https://80k.link/AJ2""><i>Radical Honesty.</i></a>)<br><br> 

We do talk about some useful stuff — one of which is the concept of micro goals. When you wake up in the morning, just commit to putting on your workout clothes. Once they’re on, maybe you’ll think that you might as well get on the treadmill — just for a minute. And once you’re on for 1 minute, you’ll often stay on for 20. So I’m not asking you to commit to listening to the whole episode — just to put on your headphones. (<a href=""https://80k.link/AJ3""><i>Drop Dead Healthy.</i></a>)<br><br> 

Another reason to listen is for the facts:<br><br> 

• The Bayer aspirin company invented heroin as a cough suppressant
•  Coriander is just the British way of saying cilantro
•  Dogs have a third eyelid to protect the eyeball from irritants
•  and A.J. read all 44 million words of the Encyclopedia Britannica from A to Z, which drove home the idea that we know so little about the world (although he does now know that opossums have 13 nipples). (<a href=""https://80k.link/AJ4""><i>The Know-It-All.</i></a>)<br><br> 

One extra argument for listening: If you interpret the second commandment literally, then it tells you not to make a likeness of anything in heaven, on earth, or underwater — which rules out basically all images. That means no photos, no TV, no movies. So, if you want to respect the Bible, you should definitely consider making podcasts your main source of entertainment (as long as you’re not listening on the Sabbath). (<a href=""https://80k.link/AJ5""><i>The Year of Living Biblically.</i></a>)<br><br> 

I’m so thankful to A.J. for doing this. But I also want to thank Julie, Jasper, Zane and Lucas who allowed me to spend the day in their home; the construction worker who told me how to get to my subway platform on the morning of the interview; and <a href=""https://80k.link/CU3"">Queen Jadwiga</a> for making bagels popular in the 1300s, which kept me going during the recording. (<a href=""https://80k.link/AJ6""><i>Thanks a Thousand.</i></a>)<br><br> 

We also discuss:<br><br> 

• Blackmailing yourself<br> 
• The most extreme ideas A.J.’s ever considered<br> 
• Doing good as a writer<br> 
• And much more.<br><br> 

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b><br><br>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/eed1ea6a-a44a-11ea-892e-127f4bdf8959/images/main.jpg?1601340450092,http://feedproxy.google.com/~r/80000HoursPodcast/~5/X6zjOt2_96s/79---a.j.-jacobs-on-radical-honesty-following-the-whole-bible-and-reframing-global-problems-as-puzzles.mp3,False
13,#78 - Danny Hernandez on forecasting and the drivers of AI progress,"Companies use about 300,000 times more computation training the best AI systems today than they did in 2012 and algorithmic innovations have also made them 25 times more efficient at the same tasks.<br><br>  

These are the headline results of two recent papers — <a href=""https://openai.com/blog/ai-and-compute/"">AI and Compute</a> and <a href=""https://openai.com/blog/ai-and-efficiency/"">AI and Efficiency</a> — from the <em>Foresight Team</em> at OpenAI. In today's episode I spoke with one of the authors, Danny Hernandez, who joined OpenAI after helping develop better forecasting methods at <a href=""https://www.twitch.tv/"">Twitch</a> and <a href=""https://www.openphilanthropy.org/"">Open Philanthropy</a>. <br><br>   

Danny and I talk about how to understand his team's results and what they mean (and don't mean) for how we should think about progress in AI going forward.<br><br>  

<a href=""https://80000hours.org/podcast/episodes/danny-hernandez-forecasting-ai-progress/?utm_campaign=podcast__danny-hernandez&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>Links to learn more, summary and full transcript.</b></a><br><br> 

Debates around the future of AI can sometimes be pretty abstract and theoretical. Danny hopes that providing rigorous measurements of some of the inputs to AI progress so far can help us better understand what causes that progress, as well as ground debates about the future of AI in a better shared understanding of the field.<br><br>   

If this research sounds appealing, you might be interested in applying to join OpenAI's Foresight team — <a href=""https://jobs.lever.co/openai/2b4e17f4-d3be-4ac9-be21-664c211c413a"">they're currently hiring research engineers</a>.<br><br>   

In the interview, Danny and I (Arden Koehler) also discuss a range of other topics, including:<br><br>  

• The question of which experts to believe<br> 
• Danny's journey to working at OpenAI<br> 
• The usefulness of ""decision boundaries""<br> 
• The importance of Moore's law for people who care about the long-term future<br> 
• What OpenAI's Foresight Team's findings might imply for policy<br> 
• The question whether progress in the performance of AI systems is linear<br> 
• The safety teams at OpenAI and who they're looking to hire<br> 
• One idea for finding someone to guide your learning<br> 
• The importance of hardware expertise for making a positive impact<br><br>  

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b><br><br> 


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/a0d93b94-9b74-11ea-b611-1204eba4b6f3/images/main.jpg?1601340450206,http://feedproxy.google.com/~r/80000HoursPodcast/~5/hWQNTBW77YA/78---danny-hernandez-on-forecasting-and-measuring-drivers-of-ai-progress-may-19.mp3,False
14,#77 - Professor Marc Lipsitch on whether we're winning or losing against COVID-19,"In March Professor Marc Lipsitch — Director of Harvard's <em>Center for Communicable Disease Dynamics</em> — abruptly found himself a global celebrity, his social media following growing 40-fold and journalists knocking down his door, as everyone turned to him for information they could trust.<br><br>

Here he lays out where the fight against COVID-19 stands today, why he's open to deliberately giving people COVID-19 to speed up vaccine development, and how we could do better next time.<br><br>

As Marc tells us, island nations like Taiwan and New Zealand are successfully suppressing SARS-COV-2. But everyone else is struggling.<br><br>

<a href=""https://80000hours.org/podcast/episodes/marc-lipsitch-winning-or-losing-against-covid19-and-epidemiology/?utm_campaign=podcast__marc-lipsitch&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>Links to learn more, summary and full transcript.</b></a><br><br>

Even Singapore, with plenty of warning and one of the best test and trace systems in the world, lost control of the virus in mid-April after successfully holding back the tide for 2 months.<br><br>

This doesn't bode well for how the US or Europe will cope as they ease their lockdowns. It also suggests it would have been exceedingly hard for China to stop the virus before it spread overseas.<br><br>

But sadly, there's no easy way out.<br><br>

The original estimates of COVID-19's infection fatality rate, of 0.5-1%, have turned out to be basically right. And the latest serology surveys indicate only 5-10% of people in countries like the US, UK and Spain have been infected so far, leaving us far short of herd immunity. To get there, even these worst affected countries would need to endure something like ten times the number of deaths they have so far.<br><br>

Marc has one good piece of news: research suggests that most of those who get infected do indeed develop immunity, for a while at least.<br><br>

To escape the COVID-19 trap sooner rather than later, Marc recommends we go hard on all the familiar options — vaccines, antivirals, and mass testing — but also open our minds to creative options we've so far left on the shelf.<br><br>

Despite the importance of his work, even now the training and grant programs that produced the community of experts Marc is a part of, are shrinking. We look at a new article he's written about how to instead build and improve the field of epidemiology, so humanity can respond faster and smarter next time we face a disease that could kill millions and cost tens of trillions of dollars.<br><br>

We also cover:<br><br>

• How listeners might contribute as future contagious disease experts, or donors to current projects<br>
• How we can learn from cross-country comparisons<br>
• Modelling that has gone wrong in an instructive way<br>
• What governments should stop doing<br>
• How people can figure out who to trust, and who has been most on the mark this time<br>
• Why Marc supports infecting people with COVID-19 to speed up the development of a vaccines<br>
• How we can ensure there's population-level surveillance early during the next pandemic<br>
• Whether people from other fields trying to help with COVID-19 has done more good than harm<br>
• Whether it's experts in diseases, or experts in forecasting, who produce better disease forecasts<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b><br><br>


<em>Producer: Keiran Harris.<br>
Audio mastering: Ben Cordell.<br>
Transcriptions: Zakee Ulhaq.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/65e57a50-995a-11ea-b994-122380125305/images/main.jpg?1601340450422,http://feedproxy.google.com/~r/80000HoursPodcast/~5/IQMjQoq6dD0/77---professor-marc-lipsitch-on-whether-were-winning-or-losing-against-covid-19.mp3,False
15,"Article: Ways people trying to do good accidentally make things worse, and how to avoid them","Today’s release is the second experiment in making audio versions of our articles. <br><br>

The first was a narration of Greg Lewis’ terrific <a href=""https://80000hours.org/problem-profiles/global-catastrophic-biological-risks/?utm_campaign=podcast__accidental-harm-audio&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>problem profile</b></a> on ‘Reducing global catastrophic biological risks’, which you can find on the podcast feed just before episode #74 - that is, our interview with Greg about the piece. <br><br>

If you want to check out the links in today’s article, you can find those <a href=""https://80000hours.org/articles/accidental-harm/?utm_campaign=podcast__accidental-harm-audio&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>here</b></a>. <br><br>

And if you have feedback on these, positive or negative, it’d be great if you could email us at podcast@80000hours.org.&nbsp;",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/5390c816-9467-11ea-abd5-1292e2b23893/images/main.jpg?1601340450651,http://feedproxy.google.com/~r/80000HoursPodcast/~5/jJQ7ec5EwLU/ways-people-trying-to-do-good-accidentally-make-things-worse-and-how-to-avoid-them-may-11-mp3.mp3,False
16,"#76 - Prof Tara Kirk Sell on misinformation, who's done well and badly, & what to reopen first","Amid a rising COVID-19 death toll, and looming economic disaster, we’ve been looking for good news — and one thing we're especially thankful for is the <i>Johns Hopkins Center for Health Security</i> (CHS). <br><br>

CHS focuses on protecting us from major biological, chemical or nuclear disasters, through research that informs governments around the world. While this pandemic surprised many, just last October the Center ran a simulation of a 'new coronavirus' scenario to identify weaknesses in our ability to quickly respond. Their expertise has given them a key role in figuring out how to fight COVID-19. <br><br>

Today’s guest, Dr Tara Kirk Sell, did her PhD in policy and communication during disease outbreaks, and has worked at CHS for 11 years on a range of important projects. <br><br>

• <a href=""https://80k.link/tks""><b>Links to learn more, summary and full transcript.</b></a> <br><br>

Last year she was a leader on <a href=""https://80k.link/84X""><b>Collective Intelligence for Disease Prediction</b></a>, designed to sound the alarm about upcoming pandemics before others are paying attention. <br><br>

Incredibly, the project almost closed in December, with COVID-19 just starting to spread around the world — but received new funding that allowed the project to respond quickly to the emerging disease. <br><br>

She also contributed to a recent <a href=""https://80k.link/BQN""><b>report</b></a> attempting to explain the risks of specific types of activities resuming when COVID-19 lockdowns end. <br><br>

We can't achieve zero risk — so differentiating activities on a spectrum is crucial. Choosing wisely can help us lead more normal lives without reviving the pandemic. <br><br>

Dance clubs will have to stay closed, but hairdressers can adapt to minimise transmission, and Tara, who happens to be an Olympic silver-medalist in swimming, suggests outdoor non-contact sports could resume soon without much risk. <br><br>

Her latest project deals with the <a href=""https://80k.link/RJH""><b>challenge of misinformation during disease outbreaks</b></a>. <br><br>

Analysing the Ebola communication crisis of 2014, they found that even trained coders with public health expertise sometimes needed help to distinguish between true and misleading tweets — showing the danger of a continued lack of definitive information surrounding a virus and how it’s transmitted. <br><br>

The challenge for governments is not simple. If they acknowledge how much they don't know, people may look elsewhere for guidance. But if they pretend to know things they don't, the result can be a huge loss of trust. <br><br>

Despite their intense focus on COVID-19, researchers at CHS know that this is no one-off event. Many aspects of our collective response this time around have been alarmingly poor, and it won’t be long before Tara and her colleagues need to turn their mind to next time. <br><br>

<em>You can now donate to CHS through <a href=""https://80k.link/FMG""><b>Effective Altruism Funds</b></a>. Donations made through EA Funds are tax-deductible in the US, the UK, and the Netherlands.</em>  <br><br>

Tara and Rob also discuss: <br><br>

• Who has overperformed and underperformed expectations during COVID-19? <br>
• Whe are people right to mistrust authorities? <br>
• The media’s responsibility to be right <br>
• What policy changes should be prioritised for next time <br>
• Should we prepare for future pandemic while the COVID-19 is still going? <br>
• The importance of keeping non-COVID health problems in mind <br>
• The psychological difference between staying home voluntarily and being forced to <br>
• Mistakes that we in the general public might be making <br>
• Emerging technologies with the potential to reduce global catastrophic biological risks <br><br>

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b> <br><br>

<em>Producer: Keiran Harris. <br>
Audio mastering: Ben Cordell. <br>
Transcriptions: Zakee Ulhaq.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/ed2571d0-9182-11ea-a3eb-0e2c7113b43d/images/main.jpg?1601340451119,http://feedproxy.google.com/~r/80000HoursPodcast/~5/sfX3ssOYIiU/76---tara-kirk-sell-on-misinformation-during-pandemics-and-emerging-technologies-with-the-potential-to-reduce-gcbrs-mp3-may-8.mp3,False
17,"#75 – Michelle Hutchinson on what people most often ask 80,000 Hours","Since it was founded, 80,000 Hours has done one-on-one calls to supplement our online content and offer more personalised advice. We try to help people get clear on their most plausible paths, the key uncertainties they face in choosing between them, and provide resources, pointers, and introductions to help them in those paths. <br><br>

I (Michelle Hutchinson) joined the team a couple of years ago after working at Oxford's <em>Global Priorities Institute</em>, and these days I'm 80,000 Hours' Head of Advising. Since then, chatting to hundreds of people about their career plans has given me some idea of the kinds of things it’s useful for people to hear about when thinking through their careers. <br><br>So we thought it would be useful to discuss some on the show for everyone to hear. <br><br>

• <a href=""https://80k.link/mh2""><b>Links to learn more, summary and full transcript.</b></a><br>
• <a href=""https://80k.link/mh-jb""><b>See over 500 vacancies on our job board.</b></a><br>
• <a href=""https://80k.link/mh-a""><b>Apply for one-on-one career advising.</b></a><br><br>

Among other common topics, we cover: <br><br>

• Why traditional careers advice involves thinking through what types of roles you enjoy followed by which of those are impactful, while we recommend going the other way: ranking roles on impact, and then going down the list to find the one you think you’d most flourish in. <br>
• That if you’re pitching your job search at the right level of role, you’ll need to apply to a large number of different jobs. So it's wise to broaden your options, by applying for both stretch and backup roles, and not over-emphasising a small number of organisations. <br>
• Our suggested process for writing a longer term career plan: 1. shortlist your best medium to long-term career options, then 2. figure out the key uncertainties in choosing between them, and 3. map out concrete next steps to resolve those uncertainties. <br>
• Why many listeners aren't spending enough time finding out about what the day-to-day work is like in paths they're considering, or reaching out to people for advice or opportunities. <br>
• The difficulty of maintaining the ambition to increase your social impact, while also being proud of and motivated by what you're already accomplishing. <br><br>

I also thought it might be useful to give people a sense of what I do and don’t do in advising calls, to help them figure out if they should sign up for it. <br><br>

If you’re wondering whether you’ll benefit from advising, bear in mind that it tends to be more useful to people: <br><br>

1. With similar views to 80,000 Hours on what the world’s most pressing problems are, because we’ve done most research on the problems we think it’s most important to address. <br>
2. Who don’t yet have close connections with people working at effective altruist organisations. <br>
3. Who aren’t strongly locationally constrained. <br><br>

If you’re unsure, it doesn’t take long to apply, and a lot of people say they find the application form itself helps them reflect on their plans. We’re particularly keen to hear from people from under-represented backgrounds. <br><br>

Also in this episode: <br><br>

• I describe mistakes I’ve made in advising, and career changes made by people I’ve spoken with. <br>
• Rob and I argue about what risks to take with your career, like when it’s sensible to take a study break, or start from the bottom in a new career path. <br>
• I try to forecast how I’ll change after I have a baby, Rob speculates wildly on what motherhood is like, and Arden and I mercilessly mock Rob. <br><br>

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b> <br><br>


<em>Producer: Keiran Harris. <br>
Audio mastering: Ben Cordell. <br>
Transcriptions: Zakee Ulhaq.</em> <br>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/9e607c36-8956-11ea-a149-0e6a0a387ea5/images/main.jpg?1601340451207,http://feedproxy.google.com/~r/80000HoursPodcast/~5/331wxLNCV4c/75--michelle-hutchinson-on-80000-hours-most-frequently-asked-questions-apr-27.mp3,False
18,#74 - Dr Greg Lewis on COVID-19 & catastrophic biological risks,"Our lives currently revolve around the global emergency of COVID-19; you’re probably reading this while confined to your house, as the death toll from the worst pandemic since 1918 continues to rise. <br><br>

The question of how to tackle COVID-19 has been foremost in the minds of many, <a href=""https://80k.link/gl-c19""><b>including here at 80,000 Hours</b></a>. <br><br>

Today's guest, Dr Gregory Lewis, acting head of the Biosecurity Research Group at Oxford University's Future of Humanity Institute, puts the crisis in context, explaining how COVID-19 compares to other diseases, pandemics of the past, and possible worse crises in the future. <br><br>

COVID-19 is a vivid reminder that we are unprepared to contain or respond to new pathogens. <br><br>

How would we cope with a virus that was even more contagious and even more deadly? Greg's work focuses on these risks -- of outbreaks that threaten our entire future through an unrecoverable collapse of civilisation, or even the extinction of humanity. <br><br>

<a href=""https://80k.link/gregl""><b>Links to learn more, summary and full transcript.</b></a><br><br>

If such a catastrophe were to occur, Greg believes it’s more likely to be caused by accidental or deliberate misuse of biotechnology than by a pathogen developed by nature. <br><br>

There are a few direct causes for concern: humans now have the ability to produce some of the most dangerous diseases in history in the lab; technological progress may enable the creation of pathogens which are nastier than anything we see in nature; and most biotechnology has yet to even be conceived, so we can’t assume all the dangers will be familiar. <br><br>

This is grim stuff, but it needn’t be paralysing. In the years following COVID-19, humanity may be inspired to better prepare for the existential risks of the next century: improving our science, updating our policy options, and enhancing our social cohesion. <br><br>

COVID-19 is a tragedy of stunning proportions, and its immediate threat is undoubtedly worthy of significant resources. <br><br>

But we will get through it; if a future biological catastrophe poses an <a href=""https://www.amazon.com/Precipice-Existential-Risk-Future-Humanity/dp/0316484911""><i>existential</i> risk</a>, we may not get a second chance. It is therefore vital to learn every lesson we can from this pandemic, and provide our descendants with the security we wish for ourselves. <br><br>

Today’s episode is the hosting debut of our Strategy Advisor, Howie Lempel. <br><br>

80,000 Hours has focused on COVID-19 for the last few weeks and published <a href=""https://80k.link/gl-c19""><b>over ten</b></a> pieces about it, and a substantial benefit of this interview was to help inform our own views. As such, at times this episode may feel like eavesdropping on a private conversation, and it is likely to be of most interest to people primarily focused on making the <a href=""https://80k.link/gl-fg""><b>long-term future</b></a> go as well as possible. <br><br>

In this episode, Howie and Greg cover: <br><br>

• Reflections on the first few months of the pandemic <br>
• Common confusions around COVID-19 <br>
• How COVID-19 compares to other diseases <br>
• What types of interventions have been available to policymakers <br>
• Arguments for and against working on <a href=""https://80k.link/gcbrs""><b>global catastrophic biological risks</b></a> (GCBRs) <br>
• How to know if you’re a good fit to work on GCBRs <br>
• The response of the <a href=""https://80k.link/gl-ea""><b>effective altruism community</b></a>, as well as 80,000 Hours in particular, to COVID-19 <br>
• And much more. <br><br>

<b>Get this episode by subscribing: type '80,000 Hours' into your podcasting app. Or read the linked transcript.</b> <br><br>


<i>Producer: Keiran Harris.</i> <br>
<i>Audio mastering: Ben Cordell.</i><br>
<i>Transcriptions: Zakee Ulhaq.</i>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/f7c12b0c-80c5-11ea-b6ce-0e5740bfbb89/images/main.jpg?1601340451294,http://feedproxy.google.com/~r/80000HoursPodcast/~5/kgoN1wIiFMA/74---dr-greg-lewis-on-covid-19-and-global-catastrophic-biological-risks-apr-16-mp3.mp3,False
19,Article: Reducing global catastrophic biological risks,"<p>In a few days we'll be putting out a conversation with Dr Greg Lewis, who studies how to prevent global catastrophic biological risks at Oxford's Future of Humanity Institute. </p><p>

Greg also wrote <a href=""https://80000hours.org/problem-profiles/global-catastrophic-biological-risks/?utm_campaign=podcast__gcbr-audio&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>a new problem profile</b></a> on that topic for our website, and reading that is a good lead-in to our interview with him. So in a bit of an experiment we decided to make this audio version of that article, narrated by the producer of the 80,000 Hours Podcast, Keiran Harris. </p><p>

We’re thinking about having audio versions of other important articles we write, so it’d be great if you could let us know if you’d like more of these. You can email us your view at podcast@80000hours.org. </p><p>

If you want to check out all of Greg’s graphs and footnotes that we didn’t include, and get links to learn more about GCBRs - you can find those <a href=""https://80000hours.org/problem-profiles/global-catastrophic-biological-risks/?utm_campaign=podcast__gcbr-audio&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>here</b></a>. </p><p>

And if you want to read more about COVID-19, the 80,000 Hours team has produced a fantastic package of 10 pieces about how to stop the pandemic. You can find those <a href=""https://80000hours.org/80000hours.org/covid-19/?utm_campaign=podcast__gcbr-audio&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>here</b></a>.  </p>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/3b9ad4b2-7f69-11ea-bdcc-0e4b74461eeb/images/main.jpg?1601340451382,http://feedproxy.google.com/~r/80000HoursPodcast/~5/1GU3FziLKQQ/article---reducing-global-catastrophic-biological-risks-mp3-april-15.mp3,False
20,"Emergency episode: Rob & Howie on the menace of COVID-19, and what both governments & individuals might do to help","<p>From home isolation Rob and Howie just recorded an episode on: <br><br>1. How many could die in the crisis, and the risk to your health personally. <br>2. What individuals might be able to do help tackle the coronavirus crisis. <br>3. What we suspect governments should do in response to the coronavirus crisis. <br>4. The importance of personally not spreading the virus, the properties of the SARS-CoV-2 virus, and how you can personally avoid it. <br>5. The many places society screwed up, how we can avoid this happening again, and why be optimistic.&nbsp;<br><br>

We have rushed this episode out to share information as quickly as possible in a fast-moving situation. If you would prefer to read you can find the <b><a href=""https://80000hours.org/podcast/episodes/rob-howie-coronavirus-crisis/#transcript"">transcript here</a></b>.<br><br>

We list a wide range of valuable resources and links in the <b><a href=""https://80000hours.org/podcast/episodes/rob-howie-coronavirus-crisis/"">blog post attached to the show</a></b> (over 60, including links to projects you can join). <br><br>

See our 'problem profile' on <a href=""https://80000hours.org/problem-profiles/global-catastrophic-biological-risks/""><b>global catastrophic biological risks</b></a> for information on these grave threats and how you can contribute to preventing them. <br><br>

We have also just added a&nbsp;<a href=""https://80000hours.org/covid-19/""><b>COVID-19 landing page</b></a> on our site. <br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b> <br><br>

<em>Producer: Keiran Harris.</em></p>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/2178bf0e-6a38-11ea-9e38-0e0b70a6519d/images/main.jpg?1601340451479,http://feedproxy.google.com/~r/80000HoursPodcast/~5/CiHzEMCG4Fw/emergency-episode---rob--howie-on-the-menace-of-covid-19-and-what-both-governments--individuals-might-do-to-help-mp3-mar-19.mp3,False
21,#73 - Phil Trammell on patient philanthropy and waiting to do good,"To do good, most of us look to use our time and money to affect the world around us today. But perhaps that's all wrong. <br><br>

If you took $1,000 you were going to donate and instead put it in the stock market — where it grew on average 5% a year — in 100 years you'd have $125,000 to give away instead. And in 200 years you'd have $17 million. <br><br>

This astonishing fact has driven today's guest, economics researcher Philip Trammell at Oxford's Global Priorities Institute, to <a href=""https://80k.link/pt-pdf""><b>investigate the case for and against</b></a> so-called 'patient philanthropy' in depth. If the case for patient philanthropy is as strong as Phil believes, many of us should be trying to improve the world in a very different way than we are now. <br><br>

He points out that on top of being able to dispense vastly more, whenever your trustees decide to use your gift to improve the world, they'll also be able to rely on the much broader knowledge available to future generations. A donor two hundred years ago couldn't have known distributing anti-malarial bed nets was a good idea. Not only did bed nets not exist — we didn't even know about germs, and almost nothing in medicine was justified by science. <br><br>

<i>ADDED: Does the COVID-19 emergency mean we should actually use resources right now? See Phil's <a href=""https://80000hours.org/podcast/episodes/phil-trammell-patient-philanthropy/#covid""><b>first thoughts on this question here.</b></a></i> <br><br>

• <a href=""https://80k.link/ptpp""><b>Links to learn more, summary and full transcript.</b></a> <br><br>

What similar leaps will our descendants have made in 200 years, allowing your now vast foundation to benefit more people in even greater ways? <br><br>

And there's a third reason to wait as well. What are the odds that we today live at the most critical point in history, when resources happen to have the greatest ability to do good? It's possible. But the future may be very long, so there has to be a good chance that some moment in the future will be both more pivotal and more malleable than our own. <br><br>

Of course, there are many objections to this proposal. If you start a foundation you hope will wait around for centuries, might it not be destroyed in a war, revolution, or financial collapse? <br><br>

Or might it not drift from its original goals, eventually just serving the interest of its distant future trustees, rather than the noble pursuits you originally intended? <br><br>

Or perhaps it could fail for the reverse reason, by staying true to your original vision — if that vision turns out to be as deeply morally mistaken as the Rhodes' Scholarships initial charter, which limited it to 'white Christian men'. <br><br>

Alternatively, maybe the world will change in the meantime, making your gift useless. At one end, humanity might destroy itself before your trust tries to do anything with the money. Or perhaps everyone in the future will be so fabulously wealthy, or the problems of the world already so overcome, that your philanthropy will no longer be able to do much good. <br><br>

Are these concerns, all of them legitimate, enough to overcome the case in favour of patient philanthropy? In today's conversation with researcher Phil Trammell and my 80,000 Hours colleague Howie Lempel, we try to answer that, and also discuss: <br><br>

• Real attempts at patient philanthropy in history and how they worked out <br>
• Should we have a mixed strategy, where some altruists are patient and others impatient? <br>
• Which causes most need money now, and which later? <br>
• What is the research frontier here? <br>
• What does this all mean for what listeners should do differently? <br><br>

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the transcript linked above.</b> <br><br>


<em>Producer: Keiran Harris.</em> <br>
<em>Audio mastering: Ben Cordell.</em> <br>
<em>Transcriptions: Zakee Ulhaq.</em> <br>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/6b68d184-6854-11ea-82a3-0e93e4083793/images/main.jpg?1601340451569,http://feedproxy.google.com/~r/80000HoursPodcast/~5/MywqeM0YnAo/73---phil-trammell-on-patient-philanthropy-and-waiting-to-do-good.mp3,False
22,#72 - Toby Ord on the precipice and humanity's potential futures,"This week Oxford academic and 80,000 Hours trustee Dr Toby Ord released his new book <a href=""https://theprecipice.com/""><i>The Precipice: Existential Risk and the Future of Humanity</i></a>. It's about how our long-term future could be better than almost anyone believes, but also how humanity's recklessness is putting that future at grave risk — in Toby's reckoning, a 1 in 6 chance of being extinguished this century. <br><br>

I loved the book and learned a great deal from it (<a href=""https://80k.link/buy-tp""><b>buy it here</b></a>, US and audiobook release March 24). While preparing for this interview I copied out 87 facts that were surprising, shocking or important. Here's a sample of 16: <br><br>

1. The probability of a supervolcano causing a civilisation-threatening catastrophe in the next century is estimated to be 100x that of asteroids and comets combined. <br><br>
2. The Biological Weapons Convention — a global agreement to protect humanity — has just four employees, and a smaller budget than an average McDonald’s. <br><br>
3. In 2008 a 'gamma ray burst' reached Earth from another galaxy, 10 billion light years away. It was still bright enough to be visible to the naked eye. We aren't sure what generates gamma ray bursts but one cause may be two neutron stars colliding. <br><br>
4. Before detonating the first nuclear weapon, scientists in the Manhattan Project feared that the high temperatures in the core, unprecedented for Earth, might be able to ignite the hydrogen in water. This would set off a self-sustaining reaction that would burn off the Earth’s oceans, killing all life above ground. They thought this was unlikely, but many atomic scientists feared their calculations could be missing something. As far as we know, the US President was never informed of this possibility, but similar risks were one reason Hitler stopped… <br><br>

<i>N.B. I've had to cut off this list as we only get 4,000 characters in these show notes, so:</i> <br><br>

<a href=""https://80k.link/ord-2""><b>Click here to read the whole list, see a full transcript, and find related links.</b></a> <br><br>

And if you like the list, you can <a href=""https://80000hours.org/the-precipice/""><b>get a free copy of the introduction and first chapter</b></a> by joining our mailing list. <br><br>

While I've been studying these topics for years and known Toby for the last eight, a remarkable amount of what's in <i>The Precipice</i> was new to me. <br><br>

Of course the book isn't a series of isolated amusing facts, but rather a systematic review of the many ways humanity's future could go better or worse, how we might know about them, and what might be done to improve the odds. <br><br>

And that's how we approach this conversation, first talking about each of the main threats, then how we can learn about things that have never happened before, then finishing with what a great future for humanity might look like and how it might be achieved. <br><br>

Toby is a famously good explainer of complex issues — a bit of a modern Carl Sagan character — so as expected this was a great interview, and one which Arden Koehler and I barely even had to work for. <br><br>

Some topics Arden and I ask about include: <br><br>

• What Toby changed his mind about while writing the book <br>
• Are people exaggerating when they say that climate change could actually end civilization? <br>
• What can we learn from historical pandemics? <br>
• Toby’s estimate of unaligned AI causing human extinction in the next century <br>
• Is this century the most important time in human history, or is that a narcissistic delusion? <br>
• Competing vision for humanity's ideal future <br>
• And more. <br><br>

<b>Get this episode by subscribing: type '80,000 Hours' into your podcasting app. Or read the linked transcript.</b> <br><br>


<i>Producer: Keiran Harris.</i> <br>
<i>Audio mastering: Ben Cordell.</i><br>
<i>Transcriptions: Zakee Ulhaq.</i>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/4359f866-60a8-11ea-87ff-0e1b40f62329/images/main.jpg?1601340451656,http://feedproxy.google.com/~r/80000HoursPodcast/~5/M9j3xqwGkw0/72---toby-ord-on-existential-risk-and-the-future-of-humanity.mp3,False
23,"#71 - Benjamin Todd on the key ideas of 80,000 Hours","The 80,000 Hours Podcast is about “the world’s most pressing problems and how you can use your career to solve them”, and in this episode we tackle that question in the most direct way possible. <br><br>

Last year we published a summary of all our <a href=""https://80k.link/ki-bt""><b>key ideas</b></a>, which links to many of our other articles, and which we are aiming to keep updated as our opinions shift. <br><br>

All of us added something to it, but the single biggest contributor was our CEO and today's guest, Ben Todd, who founded 80,000 Hours along with Will MacAskill back in 2012. <br><br>

This key ideas page is the most read on the site. By itself it can teach you a large fraction of the most important things we've discovered since we started investigating high impact careers. <br><br>

<b>•&nbsp;<a href=""https://80000hours.org/podcast/episodes/ben-todd-key-ideas-of-80000hours/?utm_campaign=podcast__ben-todd&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"">Links to learn more, summary and full transcript.</a></b> <br><br>

But it's perhaps more accurate to think of it as a mini-book, as it weighs in at over 20,000 words. <br><br>

Fortunately it's designed to be highly modular and it's easy to work through it over multiple sessions, scanning over the articles it links to on each topic. <br><br>

Perhaps though, you'd prefer to absorb our most essential ideas in conversation form, in which case this episode is for you. <br><br>

If you want to have a big impact with your career, and you say you're only going to read one article from us, we recommend you read our <a href=""https://80k.link/ki-bt""><b>key ideas</b></a> page. <br><br>

And likewise, if you're only going to listen to one of our podcast episodes, it should be this one. We have fun and set a strong pace, running through: <br><br>

• Common misunderstandings of our advice <br>
• A high level overview of what 80,000 Hours generally recommends <br>
• Our key moral positions <br>
• What are the most pressing problems to work on and why? <br>
• Which careers effectively contribute to solving those problems? <br>
• Central aspects of career strategy like how to weigh up career capital, personal fit, and exploration <br>
• As well as plenty more. <br><br>

One benefit of this podcast over the article is that we can more easily communicate uncertainty, and dive into the things we're least sure about, or didn’t yet cover within the article. <br><br>

Note though that our what’s in the article is more precisely stated, our advice is going to keep shifting, and we're aiming to keep the key ideas page current as our thinking evolves over time. This episode was recorded in November 2019, so if you notice a conflict between the page and this episode in the future, go with the page! <br><br>

<b>Get the episode by subscribing: type 80,000 Hours into your podcasting app. </b><br><br>


<em>Producer: Keiran Harris. </em><br>
<em>Audio mastering: Ben Cordell. </em><br>
<em>Transcriptions: Zakee Ulhaq. </em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/4224d342-5cea-11ea-9bae-0ebdefbc3bc9/images/main.jpg?1601340451743,http://feedproxy.google.com/~r/80000HoursPodcast/~5/ATKAhFhz7ww/71---ben-todd-on-the-key-ideas-of-80000-hours-mar-2.mp3,False
24,"Arden & Rob on demandingness, work-life balance & injustice (80k team chat #1)","Today's bonus episode of the podcast is a quick conversation between me and my fellow 80,000 Hours researcher Arden Koehler about a few topics, including the demandingness of morality, work-life balance, and emotional reactions to injustice. <br><br>

Arden is about to graduate with a philosophy PhD from New York University, so naturally we dive right into some challenging implications of utilitarian philosophy and how it might be applied to real life. Issues we talk about include: <br><br>

• If you’re not going to be completely moral, should you try being a bit more ethical, or give up? <br>
• Should you feel angry if you see an injustice, and if so, why? <br>
• How much should we ask people to live frugally? <br><br>

So far the feedback on the post-episode chats that we've done have been positive, so we thought we'd go ahead and try out this freestanding one. But fair warning: it's among the more difficult episodes to follow, and probably not the best one to listen to first, as you'll benefit from having more context!<br><br>

If you'd like to listen to more of Arden you can find her in episode 67, <b><a href=""https://80000hours.org/podcast/episodes/david-chalmers-nature-ethics-consciousness/"">David Chalmers on the nature and ethics of consciousness</a>,</b> or episode 66, <a href=""https://80000hours.org/podcast/episodes/peter-singer-advocacy-and-the-life-you-can-save/""><b>Peter Singer on being provocative, EA, and how his moral views have changed</b></a>. <br><br>

Here's more information on some of the issues we touch on:<br><br>

• <a href=""https://en.wikipedia.org/wiki/Consequentialism""><b>Consequentialism</b></a> on Wikipedia<br>
• <a href=""https://plato.stanford.edu/entries/dispositions/""><b>Appropriate dispositions</b></a> on the Stanford Encyclopaedia of Philosophy<br>
• <a href=""https://en.wikipedia.org/wiki/Demandingness_objection""><b>Demandingness objection</b></a> on Wikipedia<br>
• And a paper on <a href=""https://www.jstor.org/stable/20117752?seq=1""><b>epistemic normativity</b></a>. <br><br>

——— <br><br>

I mention the call for papers of the Academic Workshop on Global Priorities in the introduction — you can <a href=""https://www.eagxaustralia.com/workshop-on-global-priorities/""><b>learn more here</b></a>. <br><br>

And finally, Toby Ord — one of our founding Trustees and a Senior Research Fellow in Philosophy at Oxford University — has his new book <a href=""https://theprecipice.com/""><b>The Precipice: Existential Risk and the Future of Humanity</b></a> coming out next week. I've read it and very much enjoyed it. Find out where you can pre-order it <a href=""https://theprecipice.com/purchase""><b>here</b></a>. We'll have an interview with him coming up soon.",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/f975e7bc-5767-11ea-baf0-0ebe1d36953f/images/main.jpg?1601340451831,http://feedproxy.google.com/~r/80000HoursPodcast/~5/LVLGpKpAFKI/bonus-episode---arden-and-rob-on-demandingness-feb-25.mp3,False
25,#70 - Dr Cassidy Nelson on the 12 best ways to stop the next pandemic (and limit nCoV),"nCoV is alarming governments and citizens around the world. It has killed more than 1,000 people, brought the Chinese economy to a standstill, and continues to show up in more and more places. But bad though it is, it's much closer to a warning shot than a worst case scenario. The next emerging infectious disease could easily be more contagious, more fatal, or both.<br><br>

Despite improvements in the last few decades, humanity is still not nearly prepared enough to contain new diseases. We identify them too slowly. We can't do enough to reduce their spread. And we lack vaccines or drugs treatments for at least a year, if they ever arrive at all.<br><br>

• <a href=""https://80000hours.org/podcast/episodes/cassidy-nelson-12-ways-to-stop-pandemics/?utm_campaign=podcast__cassidy-nelson&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>Links to learn more, summary and full transcript.</b></a><br><br>

This is a precarious situation, especially with advances in biotechnology increasing our ability to modify viruses and bacteria as we like.<br><br>

In today's episode, Cassidy Nelson, a medical doctor and research scholar at Oxford University's <i>Future of Humanity Institute</i>, explains 12 things her research group think urgently need to happen if we're to keep the risk at acceptable levels. The ideas are:<br><br>

<b>Science</b><br><br>
1. Roll out genetic sequencing tests that lets you test someone for all known and unknown pathogens in one go.<br>
2. Fund research into faster ‘platform’ methods for going from pathogen to vaccine, perhaps using innovation prizes.<br>
3. Fund R&amp;D into broad-spectrum drugs, especially antivirals, similar to how we have generic antibiotics against multiple types of bacteria.
<br><br>
<b>Response</b>
<br><br>
4. Develop a national plan for responding to a severe pandemic, regardless of the cause. Have a backup plan for when things are so bad the normal processes have stopped working entirely.<br>
5. Rigorously evaluate in what situations travel bans are warranted. (They're more often counterproductive.)<br>
6. Coax countries into more rapidly sharing their medical data, so that during an outbreak the disease can be understood and countermeasures deployed as quickly as possible.<br>
7. Set up genetic surveillance in hospitals, public transport and elsewhere, to detect new pathogens before an outbreak — or even before patients develop symptoms.<br>
8. Run regular tabletop exercises within governments to simulate how a pandemic response would play out.
<br><br>
<b>Oversight</b>
<br><br>
9. Mandate disclosure of accidents in the biosafety labs which handle the most dangerous pathogens.<br>
10. Figure out how to govern DNA synthesis businesses, to make it harder to mail order the DNA of a dangerous pathogen.<br>
11. Require full cost-benefit analysis of 'dual-use' research projects that can generate global risks.<br>
 <br>
12. And finally, to maintain momentum, it's necessary to clearly assign responsibility for the above to particular individuals and organisations.<br><br>

These advances can be pursued by politicians and public servants, as well as academics, entrepreneurs and doctors, opening the door for many listeners to pitch in to help solve this incredibly pressing problem.<br><br>

In the episode Rob and Cassidy also talk about:<br><br>

• How Cassidy went from clinical medicine to a PhD studying novel pathogens with pandemic potential.<br>
• The pros, and significant cons, of travel restrictions.<br>
• Whether the same policies work for natural and anthropogenic pandemics.<br>
• Ways listeners can pursue a career in biosecurity.<br>
• Where we stand with nCoV as of today.<br><br>

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b><br><br>


<i>Producer: Keiran Harris.<br>
Transcriptions: Zakee Ulhaq.</i>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/2dacdfc0-4eaa-11ea-b06b-0e3f4a697fb5/images/main.jpg?1601340451924,http://feedproxy.google.com/~r/80000HoursPodcast/~5/o1jrrygmDuw/70---cassidy-nelson-on-the-12-best-ways-to-stop-the-next-pandemic-and-limit-ncov-128kbps.mp3,False
26,"#69 - Jeff Ding on China, its AI dream, and what we get wrong about both","The State Council of China's 2017 AI plan was the starting point of China’s AI planning; China’s approach to AI is defined by its top-down and monolithic nature; China is winning the AI arms race; and there is little to no discussion of issues of AI ethics and safety in China. How many of these ideas have you heard? <br><br>

In his paper <a href=""https://80k.link/dcaid""><b>Deciphering China's AI Dream</b></a>, today's guest, PhD student Jeff Ding, outlines why he believes none of these claims are true. <br><br> 

• <b><a href=""https://80k.link/jding"">Links to learn more, summary and full transcript.</a></b><br><br> 

• <b><a href=""https://80k.link/best-charity"">What’s the best charity to donate to?</a></b><br><br>

He first places China’s new AI strategy in the <b>context</b> of its past science and technology plans, as well as other countries’ AI plans. What is China actually doing in the space of AI development? <br><br>

Jeff emphasises that China's AI strategy did not appear out of nowhere with the 2017 state council AI development plan, which attracted a lot of overseas attention. Rather that was just another step forward in a long trajectory of increasing focus on science and technology. It's connected with a plan to develop an 'Internet of Things', and linked to a history of strategic planning for technology in areas like aerospace and biotechnology. <br><br>

And it was not just the central government that was moving in this space; companies were already pushing forward in AI development, and local level governments already had their own AI plans. You could argue that the central government was following their lead in AI more than the reverse. <br><br>

What are the different levers that China is pulling to try to spur AI development? <br><br>

Here, Jeff wanted to challenge the myth that China's AI development plan is based on a monolithic central plan requiring people to develop AI. In fact, bureaucratic agencies, companies, academic labs, and local governments each set up their own strategies, which sometimes conflict with the central government. <br><br>

Are China's AI <b>capabilities</b> especially impressive? In the paper Jeff develops a new index to measure and compare the US and China's progress in AI. <br><br>

Jeff’s AI Potential Index — which incorporates trends and capabilities in data, hardware, research and talent, and the commercial AI ecosystem — indicates China’s AI capabilities are about half those of America. His measure, though imperfect, dispels the notion that China's AI capabilities have surpassed the US or make it the world's leading AI power.  <br><br>

Following that 2017 plan, a lot of Western observers thought that to have a good national AI strategy we'd need to figure out how to play catch-up with China. Yet Chinese strategic thinkers and writers at the time actually thought that they were behind — because the Obama administration had issued a series of three white papers in 2016. <br><br>

Finally, Jeff turns to the potential <b>consequences</b> of China’s AI dream for issues of national security, economic development, AI safety and social governance. <br><br>

He claims that, despite the widespread belief to the contrary, substantive discussions about AI safety and ethics are indeed emerging in China. For instance, a new book from Tencent’s Research Institute is proactive in calling for stronger awareness of AI safety issues. <br><br>

In today’s episode, Rob and Jeff go through this widely-discussed report, and also cover: <br><br>

• The best analogies for thinking about the growing influence of AI <br>
• How do prominent Chinese figures think about AI? <br>
• Coordination with China<br>
• China’s social credit system <br>
• Suggestions for people who want to become professional China specialists <br>
• And more.<br><br>

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app.</b> <br><br>


<em>Producer: Keiran Harris. <br>
Audio mastering: Ben Cordell. <br>
Transcriptions: Zakee Ulhaq.</em><p></p>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/70fcfa7a-490b-11ea-bb0a-0ef900b9bcb9/images/main.jpg?1601340452012,http://feedproxy.google.com/~r/80000HoursPodcast/~5/T-JrAAa9nVY/69--jeffrey-ding-on-chinas-ai-dream-feb-6.mp3,False
27,Rob & Howie on what we do and don't know about 2019-nCoV,"<p>Two 80,000 Hours researchers, Robert Wiblin and Howie Lempel, record an experimental bonus episode about the new 2019-nCoV virus.</p><p>See this <a href="" https://80000hours.org/2020/02/experimental-episode-about-2019-ncov-coronavirus/"" style=""font-weight: bold;"">list of resources</a>,&nbsp;including many discussed in the episode,&nbsp;to learn more.</p><p>In the 1h15m conversation we cover:</p><p>• What is it?&nbsp;<br>• How many people have it?&nbsp;<br>• How contagious is it?&nbsp;<br>• What fraction of people who contract it die?<br>• How likely is it to spread out of control?<br>• What's the range of plausible fatalities worldwide?<br>• How does it compare to other epidemics?<br>• What don't we know and why?&nbsp;<br>• What actions should listeners take, if any?<br>• How should the complexities of the above be communicated by public health professionals?</p><p>Here's a link to the <a href=""https://foreignpolicy.com/2020/01/25/wuhan-coronavirus-safety-china/""><b>hygiene advice from Laurie Garrett</b></a> mentioned in the episode.</p><p>Recorded 2 Feb 2020.</p><p><i>The 80,000 Hours Podcast is produced by Keiran Harris.</i><br></p>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/2552e4f4-46a4-11ea-a277-0ed81a817f7d/images/main.jpg?1601340452098,http://feedproxy.google.com/~r/80000HoursPodcast/~5/Pl2mSnrNa24/rob-and-howie-discuss-ncov-with-edits-and-intro-v2.mp3,False
28,"#68 - Will MacAskill on the paralysis argument, whether we're at the hinge of history, & his new priorities","You’re given a box with a set of dice in it. If you roll an even number, a person's life is saved. If you roll an odd number, someone else will die. Each time you shake the box you get $10. Should you do it? <br><br>

A committed consequentialist might say, <em>""Sure! Free money!""</em> But most will think it obvious that you should say no. You've only gotten a tiny benefit, in exchange for moral responsibility over whether other people live or die. <br><br>

And yet, according to today’s return guest, philosophy Prof Will MacAskill, in a real sense we’re shaking this box every time we leave the house, and those who think shaking the box is wrong should probably also be shutting themselves indoors and minimising their interactions with others. <br><br>

• <a href=""https://80k.link/will2""><b>Links to learn more, summary and full transcript.</b></a><br>
• <a href=""https://80k.link/gpio""><b>Job opportunities at the Global Priorities Institute.</b></a><br><br>

To see this, imagine you’re deciding whether to redeem a coupon for a free movie. If you go, you’ll need to drive to the cinema. By affecting traffic throughout the city, you’ll have slightly impacted the schedules of thousands or tens of thousands of people. The average life is about 30,000 days, and over the course of a life the average person will have about two children. So — if you’ve impacted at least 7,500 days — then, statistically speaking, you've probably influenced the exact timing of a conception event. With 200 million sperm in the running each time, changing the moment of copulation, even by a fraction of a second, will almost certainly mean you've changed the identity of a future person. <br><br>

That different child will now impact all sorts of things as they go about their life, including future conception events. And then those new people will impact further future conceptions events, and so on. After 100 or maybe 200 years, basically everybody alive will be a different person because you went to the movies. <br><br>

As a result, you’ll have changed when many people die. Take car crashes as one example: about 1.3% of people die in car crashes. Over that century, as the identities of everyone change as a result of your action, many of the 'new' people will cause car crashes that wouldn't have occurred in their absence, including crashes that prematurely kill people alive today. <br><br>

Of course, in expectation, exactly the same number of people will have been saved from car crashes, and will die later than they would have otherwise. <br><br>

So, if you go for this drive, you’ll save hundreds of people from premature death, and cause the early death of an equal number of others. But you’ll get to see a free movie, worth $10. Should you do it? <br><br>

This setup forms the basis of ‘the paralysis argument’, explored in one of Will’s <a href=""https://80k.link/paral""><b>recent papers</b></a>. <br><br>

Because most 'non-consequentialists' endorse an act/omission distinction… <em>post truncated due to character limit, <a href=""https://80k.link/will2""><b>finish reading the full explanation here.</b></a></em> <br><br>

So what's the best way to fix this strange conclusion? We discuss a few options, but the most promising might bring people a lot closer to full consequentialism than is immediately apparent. In this episode Will and I also cover: <br><br>

• Are, or are we not, living in the most influential time in history? <br>
• The culture of the effective altruism community <br>
• Will's new lower estimate of the risk of human extinction <br>
• Why Will is now less focused on AI <br>
• The differences between Americans and Brits <br>
• Why feeling guilty about characteristics you were born with is crazy <br>
• And plenty more. <br><br>

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app.</b> <br><br>


<em>Producer: Keiran Harris. <br>
Audio mastering: Ben Cordell. <br>
Transcriptions: Zakee Ulhaq.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/3a4dd1e2-3e3a-11ea-a6ce-0e215caa46a1/images/main.jpg?1601340452187,http://feedproxy.google.com/~r/80000HoursPodcast/~5/NvuOtnIw8uQ/68--will-macaskill-on-the-paralysis-argument-and-the-hinge-of-history-mp3-jan-23-chapters.mp3,False
29,#44 Classic episode - Paul Christiano on finding real solutions to the AI alignment problem,"<b>Rebroadcast: this episode was originally released in October 2018. </b><br><br>

<p>Paul Christiano is one of the smartest people I know. After our first session produced such great material, we decided to do a second recording, resulting in our longest interview so far. While challenging at times I can strongly recommend listening — Paul works on AI himself and has a very unusually thought through view of how it will change the world. This is now the top resource I'm going to refer people to if they're interested in positively shaping the development of AI, and want to understand the problem better. Even though I'm familiar with Paul's writing I felt I was learning a great deal and am now in a better position to make a difference to the world.<br><br>

A few of the topics we cover are:<br><br>• Why Paul expects AI to transform the world gradually rather than explosively and what that would look like<br>
• Several concrete methods OpenAI is trying to develop to ensure AI systems do what we want even if they become more competent than us<br>
• Why AI systems will probably be granted legal and property rights<br>
• How an advanced AI that doesn't share human goals could still have moral value<br>
• Why machine learning might take over science research from humans before it can do most other tasks<br>
• Which decade we should expect human labour to become obsolete, and how this should affect your savings plan.<br><br>

<b>• <a href=""https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/?utm_campaign=podcast__paul-christiano&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"">Links to learn more, summary and full transcript.</a></b><br>
<b>• <a href=""https://rohinshah.com/alignment-newsletter/"">Rohin Shah's AI alignment newsletter.</a></b><br><br>

Here's a situation we all regularly confront: you want to answer a difficult question, but aren't quite smart or informed enough to figure it out for yourself. The good news is you have access to experts who *are* smart enough to figure it out. The bad news is that they disagree.<br><br>

If given plenty of time — and enough arguments, counterarguments and counter-counter-arguments between all the experts — should you eventually be able to figure out which is correct? What if one expert were deliberately trying to mislead you? And should the expert with the correct view just tell the whole truth, or will competition force them to throw in persuasive lies in order to have a chance of winning you over?<br><br>

In other words: does 'debate', in principle, lead to truth?<br><br>

According to Paul Christiano — researcher at the machine learning research lab OpenAI and legendary thinker in the effective altruism and rationality communities — this question is of more than mere philosophical interest. That's because 'debate' is a promising method of keeping artificial intelligence aligned with human goals, even if it becomes much more intelligent and sophisticated than we are.<br><br>

It's a method OpenAI is <a href=""https://blog.openai.com/debate/"">actively trying to develop</a>, because in the long-term it wants to train AI systems to make decisions that are too complex for any human to grasp, but without the risks that arise from a complete loss of human oversight.<br><br> 

If AI-1 is free to choose any line of argument in order to attack the ideas of AI-2, and AI-2 always seems to successfully defend them, it suggests that every possible line of argument would have been unsuccessful.<br><br>

But does that mean that the ideas of AI-2 were actually right? It would be nice if the optimal strategy in debate were to be completely honest, provide good arguments, and respond to counterarguments in a valid way. But we don't know that's the case.<br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/a1751eac-3728-11ea-87cf-0eb4505f1211/images/main.jpg?1601340452274,http://feedproxy.google.com/~r/80000HoursPodcast/~5/Gvusf6nAQ3o/44-classic-episode---paul-christiano-on-finding-real-solutions-to-the-ai-alignment-problem.mp3,False
30,"#33 Classic episode - Anders Sandberg on cryonics, solar flares, and the annual odds of nuclear war","<b>Rebroadcast: this episode was originally released in May 2018. </b><br><br>

Joseph Stalin had a life-extension program dedicated to making himself immortal. What if he had succeeded? <br><br>

According to&nbsp;<a href=""https://80000hours.org/podcast/episodes/bryan-caplan-case-for-and-against-education/?utm_campaign=podcast__anders-sandberg-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"" style=""font-weight: bold;"">Bryan Caplan</a>&nbsp;in episode #32, there’s an 80% chance that Stalin would still be ruling Russia today. Today’s guest disagrees. <br><br>

Like Stalin he has eyes for his own immortality - including an insurance plan that will cover the cost of cryogenically freezing himself after he dies - and thinks the technology to achieve it might be around the corner. <br><br>

Fortunately for humanity though, that guest is probably one of the nicest people on the planet: Dr Anders Sandberg of Oxford University. <br><br>

<a href=""https://80000hours.org/podcast/episodes/anders-sandberg-extending-life/?utm_campaign=podcast__anders-sandberg-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>Full transcript of the conversation, summary, and links to learn more.</b></a> <br><br>

The potential availability of technology to delay or even stop ageing means this disagreement matters, so he has been trying to model what would really happen if both the very best and the very worst people in the world could live forever - among many other questions. <br><br>

Anders, who studies low-probability high-stakes risks and the impact of technological change at the Future of Humanity Institute, is the first guest to appear twice on the 80,000 Hours Podcast and might just be the most interesting academic at Oxford. <br><br>

His research interests include more or less everything, and bucking the academic trend towards intense specialization has earned him a devoted fan base. <br><br>

Last time we asked him <a href=""https://80000hours.org/podcast/episodes/anders-sandberg-fermi-paradox/?utm_campaign=podcast__anders-sandberg-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"">why we don’t see aliens, and how to most efficiently colonise the universe</a>. In today’s episode we ask about Anders’ other recent papers, including: <br><br>

• Is it worth the money to freeze your body after death in the hope of future revival, like Anders has done? <br>• How much is our perception of the risk of nuclear war biased by the fact that we wouldn’t be alive to think about it had one happened? <br>• If biomedical research lets us slow down ageing would culture stagnate under the crushing weight of centenarians? <br>• What long-shot drugs can people take in their 70s to stave off death? <br>• Can science extend human (waking) life by cutting our need to sleep? <br>• How bad would it be if a solar flare took down the electricity grid? Could it happen? <br>
• If you’re a scientist and you discover something exciting but dangerous, when should you keep it a secret and when should you share it? <br>• Will lifelike robots make us more inclined to dehumanise one another? <br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app. </b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/523f7a66-31de-11ea-b2b9-0e12d87d2229/images/main.jpg?1601340452362,http://feedproxy.google.com/~r/80000HoursPodcast/~5/oGSVH3WuB_g/33-classic-episode---dr-anders-sandberg-on-cryonics-solar-flares-and-the-annual-risk-of-a-nuclear-war.mp3,False
31,"#17 Classic episode - Prof Will MacAskill on moral uncertainty, utilitarianism & how to avoid being a moral monster","<b>Rebroadcast: this episode was originally released in January 2018.</b><br><br>

Immanuel Kant is a profoundly influential figure in modern philosophy, and was one of the earliest proponents for universal democracy and international cooperation. He <a href=""https://80000hours.org/podcast/episodes/will-macaskill-moral-philosophy/?utm_campaign=podcast__will-macaskill&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"">also thought</a> that women have no place in civil society, that it was okay to kill illegitimate children, and that there was a ranking in the moral worth of different races. <br><br>
 
Throughout history we’ve consistently believed, as common sense, truly horrifying things by today’s standards. According to University of Oxford Professor Will MacAskill, it’s extremely likely that we’re in the same boat today. If we accept that we’re probably making major moral errors, how should we proceed?<br><br><b>•&nbsp;<a href=""https://80000hours.org/podcast/episodes/will-macaskill-moral-philosophy/?utm_campaign=podcast__will-macaskill&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"">Full transcript, key points &amp; links to articles discussed in the show.</a></b> <br><br>
 
If our morality is tied to common sense intuitions, we’re probably just preserving these biases and moral errors. Instead we need to develop a moral view that criticises common sense intuitions, and gives us a chance to move beyond them. And if humanity is going to spread to the stars it could be worth dedicating hundreds or thousands of years to moral reflection, lest we spread our errors far and wide. <br><br>
 
Will is an Associate Professor in Philosophy at Oxford University, author of Doing Good Better, and one of the co-founders of the effective altruism (EA) community. In this interview we discuss a wide range of topics: <br><br>
 
• How would we go about a ‘long reflection’ to fix our moral errors? <br>
• Will’s forthcoming book on how one should reason and act if you don't know which moral theory is correct. What are the practical implications of so-called ‘moral uncertainty’? <br>
• If we basically solve existential risks, what does humanity do next? <br>
• What are some of Will’s most unusual philosophical positions? <br>
• What are the best arguments for and against utilitarianism? <br>
• Given disagreements among philosophers, how much should we believe the findings of philosophy as a field? <br>
• What are some the biases we should be aware of within academia? <br>
• What are some of the downsides of becoming a professor? <br>
• What are the merits of becoming a philosopher? <br>
• How does the media image of EA differ to the actual goals of the community? <br>
• What kinds of things would you like to see the EA community do differently? <br>
• How much should we explore potentially controversial ideas? <br>
• How focused should we be on diversity? <br>
• What are the best arguments against effective altruism? <br><br>

<b>Get this episode by subscribing: type '80,000 Hours' into your podcasting app. </b><br><br> 

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em><p></p>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/659038ee-2b2b-11ea-bab9-0e25fd385935/images/main.jpg?1601340452449,http://feedproxy.google.com/~r/80000HoursPodcast/~5/nx97etvbm_E/17-classic-episode---prof-will-macaskill-on-moral-uncertainty-utilitarianism--how-to-avoid-being-a-moral-monster.mp3,False
32,#46 Classic episode - Hilary Greaves on moral cluelessness & tackling crucial questions in academia,"<p><b>Rebroadcast: this episode was originally released in October 2018. </b><br><br>

The barista gives you your coffee and change, and you walk away from the busy line. But you suddenly realise she gave you $1 less than she should have. Do you brush your way past the people now waiting, or just accept this as a dollar you’re never getting back? According to philosophy Professor Hilary Greaves - Director of Oxford University's <a href=""https://globalprioritiesinstitute.org"">Global Priorities Institute</a>, which <a href=""https://globalprioritiesinstitute.org/opportunities/"">is hiring</a> - this simple decision will completely change the long-term future by altering the identities of almost all future generations. <br><br>

How? Because by rushing back to the counter, you slightly change the timing of everything else people in line do during that day - including changing the timing of the interactions they have with everyone else. Eventually these causal links will reach someone who was going to conceive a child. <br><br>

By causing a child to be conceived a few fractions of a second earlier or later, you change the sperm that fertilizes their egg, resulting in a totally different person. So asking for that $1 has now made the difference between all the things that this actual child will do in their life, and all the things that the merely possible child - who didn't exist because of what you did - would have done if you decided not to worry about it. <br><br>

As that child's actions ripple out to everyone else who conceives down the generations, ultimately the entire human population will become different, all for the sake of your dollar. Will your choice cause a future Hitler to be born, or not to be born? Probably both! <br><br>

<b>•&nbsp;<a href=""https://80k.link/hgreaves"" rel=""nofollow"" target=""_blank"">Links to learn more, summary and full transcript.</a> </b><br><br>

Some find this concerning. The actual long term effects of your decisions are so unpredictable, it looks like you’re totally clueless about what's going to lead to the best outcomes. It might lead to decision paralysis - you won’t be able to take any action at all. <br><br>

Prof Greaves doesn’t share this concern for most real life decisions. If there’s no reasonable way to assign probabilities to far-future outcomes, then the possibility that you might make things better in completely unpredictable ways is more or less canceled out by equally likely opposite possibility. <br><br>

But, if instead we’re talking about a decision that involves highly-structured, systematic reasons for thinking there might be a general tendency of your action to make things better or worse -- for example if we increase economic growth -- Prof Greaves says that we don’t get to just ignore the unforeseeable effects. <br><br>

When there are complex arguments on both sides, it's unclear what probabilities you should assign to this or that claim. Yet, given its importance, whether you should take the action in question actually does depend on figuring out these numbers. So, what do we do? <br><br>

Today’s episode blends philosophy with an exploration of the mission and research agenda of the Global Priorities Institute: to develop the effective altruism movement within academia. We cover: <br><br>

• How controversial is the multiverse interpretation of quantum physics? <br>
• Given moral uncertainty, how should population ethics affect our real life decisions? <br>
• What are the consequences of cluelessness for those who based their donation advice on GiveWell style recommendations? <br>
• How could reducing extinction risk be a good cause for risk-averse people? <br><br>

<b>Get this episode by subscribing: type '80,000 Hours' into your podcasting app. </b><br><br> 

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/f99dffac-25d2-11ea-b0fd-0e48acdb4ef1/images/main.jpg?1601340452537,http://feedproxy.google.com/~r/80000HoursPodcast/~5/W6SXM-sCr-k/46-classic-episode---prof-hilary-greaves-on-moral-cluelessness--tackling-crucial-questions-in-academia.mp3,False
33,#67 - Prof David Chalmers on the nature and ethics of consciousness,"What is it like to be you right now? You're seeing this text on the screen, smelling the coffee next to you, and feeling the warmth of the cup. There’s a lot going on in your head — your conscious experience. <br><br>

Now imagine beings that are identical to humans, but for one thing: they lack this conscious experience. If you spill your coffee on them, they’ll jump like anyone else, but inside they'll feel no pain and have no thoughts: the lights are off. <br><br>

The concept of these so-called 'philosophical zombies' was popularised by today’s guest — celebrated philosophy professor David Chalmers — in order to explore the nature of consciousness. In a forthcoming book he poses a classic 'trolley problem': <br><br>

""Suppose you have a conscious human on one train track, and five non-conscious humanoid zombies on another. If you do nothing, a trolley will hit and kill the conscious human. If you flip a switch to redirect the trolley, you can save the conscious human, but in so doing kill the five non-conscious humanoid zombies. What should you do?"" <br><br>

Many people think you should divert the trolley, precisely because the lack of conscious experience means the moral status of the zombies is much reduced or absent entirely. <br><br>

So, which features of consciousness qualify someone for moral consideration? One view is that the only conscious states that matter are those that have a positive or negative quality, like pleasure and suffering. But Dave’s intuitions are quite different. <br><br>

• <a href=""https://80k.link/chalmers""><b>Links to learn more, summary and full transcript.</b></a> <br>
• <a href=""https://80k.link/aoa""><b>Advice on how to read our advice.</b></a> <br>
• <b>Anonymous answers on: <a href=""https://80k.link/habits"">bad habits</a>, <a href=""https://80k.link/risk"">risk</a> and <a href=""https://80k.link/fail"">failure.</a></b> <br><br>

Instead of zombies he asks us to consider 'Vulcans', who can see and hear and reflect on the world around them, but are incapable of experiencing pleasure or pain. <br><br>

Now imagine a further trolley problem: suppose you have a normal human on one track, and five Vulcans on the other. Should you divert the trolley to kill the five Vulcans in order to save the human? <br><br>

Dave firmly believes the answer is no, and if he's right, pleasure and suffering can’t be the only things required for moral status. The fact that Vulcans are conscious in other ways must matter in itself. <br><br>

Dave is one of the world's top experts on the philosophy of consciousness. He helped return the question <i>'what is consciousness?'</i> to the centre stage of philosophy with his 1996 book <i>'The Conscious Mind'</i>, which argued against then-dominant materialist theories of consciousness. <br><br>

This comprehensive interview, at over four hours long, outlines each contemporary theory of consciousness, what they have going for them, and their likely ethical implications. Those theories span the full range from illusionism, the idea that consciousness is in some sense an 'illusion', to panpsychism, according to which it's a fundamental physical property present in all matter. <br><br>

These questions are absolutely central for anyone who wants to build a positive future. If insects were conscious our treatment of them could already be an atrocity. If computer simulations of people will one day be conscious, how will we know, and how should we treat them? And what is it about consciousness that matters, if anything? <br><br>

Dave Chalmers is probably the best person on the planet to ask these questions, and Rob & Arden cover this and much more over the course of what is both our longest ever episode, and our personal favourite so far.  <br><br>

<i>Get this episode by subscribing to our show on the world’s most pressing problems and how to solve them: search for 80,000 Hours in your podcasting app.</i> <br><br>


<b>Producer: Keiran Harris.</b>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/0091a36a-203b-11ea-a4f3-0e09fb6f5fad/images/main.jpg?1601340452623,http://feedproxy.google.com/~r/80000HoursPodcast/~5/4WEJKcy08us/67---david-chalmers-on-progress-in-philosophy-virtual-reality-and-the-problem-of-consciousness2.mp3,False
34,"#66 - Prof Peter Singer on being provocative, effective altruism, & how his moral views have changed","In 1989, the professor of moral philosophy Peter Singer was all over the news for his inflammatory opinions about abortion. But the controversy stemmed from <em>Practical Ethics</em> — a book he’d actually released way back in 1979. It took a German translation ten years on for protests to kick off. <br><br>

According to Singer, he honestly didn’t expect this view to be as provocative as it became, and he certainly wasn’t aiming to stir up trouble and get attention. <br><br>

But after the protests and the increasing coverage of his work in German media, the previously flat sales of <a href=""https://80k.link/pebook""><b>Practical Ethics</b></a> shot up. And the negative attention he received ultimately led him to a weekly opinion column in <i>The New York Times</i>. <br><br>

<b>• Singer's book <em>The Life You Can Save</em> has just been re-released as a 10th anniversary edition, available as a free e-book and audiobook, read by a range of celebrities. <a href=""https://80k.link/tlycsbook"">Get it here</a>.</b><br>
<b>•&nbsp;<a href=""https://80k.link/singerinterview"">Links to learn more, summary and full transcript</a>.</b><br><br>

Singer points out that as a result of this increased attention, many more people also read the rest of the book — which includes chapters with a real ability to do good, covering global poverty, animal ethics, and other important topics. So should people actively try to court controversy with one view, in order to gain attention for another more important one? <br><br>

Perhaps sometimes, but controversy can also just have bad consequences. His critics may view him as someone who says whatever he thinks, hang the consequences, but Singer says that he gives public relations considerations plenty of thought. <br><br>

One example is that Singer opposes efforts to advocate for open borders. Not because he thinks a world with freedom of movement is a bad idea per se, but rather because it may help elect leaders like Mr Trump. <br><br>

Another is the focus of the effective altruism community. Singer certainly respects those who are focused on improving the long-term future of humanity, and thinks this is important work that should continue. But he’s troubled by the possibility of extinction risks becoming the public face of the movement. <br><br>
 
He suspects there's a much narrower group of people who are likely to respond to that kind of appeal, compared to those who are drawn to work on global poverty or preventing animal suffering. And that to really transform philanthropy and culture more generally, the effective altruism community needs to focus on smaller donors with more conventional concerns. <br><br>

Rob is joined in this interview by Arden Koehler, the newest addition to the 80,000 Hours team, both for the interview and a post-episode discussion. They only had an hour with Peter, but also cover: <br><br>

• What does he think is the most plausible alternatives to consequentialism? <br>
• Is it more humane to eat wild caught animals than farmed animals? <br>
• The re-release of The Life You Can Save <br>
• His most and least strategic career decisions <br>
• Population ethics, and other arguments for and against prioritising the long-term future <br>
• What led to his changing his mind on significant questions in moral philosophy? <br>
• And more. <br><br>

In the post-episode discussion, Rob and Arden continue talking about: <br><br>

• The pros and cons of keeping EA as one big movement <br>
• Singer’s thoughts on immigration<br>
• And consequentialism with side constraints.<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the linked transcript. </b><br><br>


<em>Producer: Keiran Harris. <br>
Audio mastering: Ben Cordell. <br>
Transcriptions: Zakee Ulhaq. <br>
Illustration of Singer: Matthias Seifarth.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/d49a9506-176f-11ea-ae99-0e4687eadef1/images/main.jpg?1601340452724,http://feedproxy.google.com/~r/80000HoursPodcast/~5/AHnbJO4yLx8/66---peter-singer-on-advocacy-and-moral-philosophy-dec-4-mp3-forecast-export-1.mp3,False
35,"#65 - Ambassador Bonnie Jenkins on 8 years pursuing WMD arms control, & diversity in diplomacy","<em>""…it started when the Soviet Union fell apart and there was a real desire to ensure security of nuclear materials and pathogens, and that scientists with [WMD-related] knowledge could get paid so that they wouldn't go to countries and sell that knowledge."" </em><br><br>

Ambassador Bonnie Jenkins has had an incredible career in diplomacy and global security. <br><br>

Today she’s a nonresident senior fellow at the <i>Brookings Institution</i> and president of <i>Global Connections Empowering Global Change</i>, where she works on global health, infectious disease and defence innovation. In 2017 she founded her own nonprofit, the <i>Women of Color Advancing Peace, Security and Conflict Transformation</i> (WCAPS). <br><br>

But in this interview we focus on her time as Ambassador at the <i>U.S. Department of State</i> under the Obama administration, where she worked for eight years as Coordinator for Threat Reduction Programs in the <i>Bureau of International Security and Nonproliferation</i>. <br><br>

In that role, Bonnie coordinated the Department of State’s work to prevent weapons of mass destruction (WMD) terrorism with programmes funded by other U.S. departments and agencies, and as well as other countries. <br><br>

• <a href=""https://80000hours.org/podcast/episodes/ambassador-bonnie-jenkins-peace-arms-control/?utm_campaign=podcast__bonnie-jenkins&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>Links to learn more, summary and full transcript.</b></a><br>
• <a href=""https://www.youtube.com/channel/UCEfASxwPxzsHlG5Rf1-4K9w/videos?view=0&amp;sort=p&amp;flow=grid""><b>Talks from over 100 other speakers at EA Global.</b></a><br>
• <b>Having trouble with podcast 'chapters' on this episode? Please report any problems to keiran at 80000hours dot org.</b> <br><br>

What was it like to be an ambassador focusing on an issue, rather than an ambassador of a country? Bonnie says the travel was exhausting. She could find herself in Africa one week, and Indonesia the next. She’d meet with folks going to New York for meetings at the UN one day, then hold her own meetings at the White House the next. <br><br>

Each event would have a distinct purpose. For one, she’d travel to Germany as a US Representative, talking about why the two countries should extend their partnership. For another, she could visit the <i>Food and Agriculture Organization</i> to talk about why they need to think more about biosecurity issues. No day was like the previous one. <br><br>

Bonnie was also a leading U.S. official in the launch and implementation of the <i>Global Health Security Agenda</i> discussed at length <a href=""https://80000hours.org/podcast/episodes/tom-inglesby-health-security/"">in episode 27</a>. <br><br>

Before returning to government in 2009, Bonnie served as program officer for U.S. Foreign and Security Policy at the <i>Ford Foundation</i>. She also served as counsel on the <i>9/11 Commission</i>. Bonnie was the lead staff member conducting research, interviews, and preparing commission reports on counterterrorism policies in the Office of the Secretary of Defense and on U.S. military plans targeting al-Qaeda before 9/11. <br><br>

And as if that all weren't curious enough four years ago Bonnie decided to go vegan. We talk about her work so far as well as: <br><br>

• How listeners can start a career like hers <br>
• Mistakes made by Mr Obama and Mr Trump <br>• Networking, the value of attention, and being a vegan in DC <br>
• And 2020 Presidential candidates.<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b><br><br>


<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/c9a479ec-0b20-11ea-b9e8-0e8b4fa1fdf9/images/main.jpg?1601340452814,http://feedproxy.google.com/~r/80000HoursPodcast/~5/IaE0RgL3-gw/65---ambassador-bonnie-jenkins-on-8-years-pursuing-wmd-arms-control--diversity-in-diplomacy.mp3,False
36,"#64 - Bruce Schneier on surveillance without tyranny, secrets, & the big risks in computer security","<b>November 3 2020, 10:32PM:</b> CNN, NBC, and FOX report that Donald Trump has narrowly won Florida, and with it, re-election.&nbsp;&nbsp;<br><br>

<b>November 3 2020, 11:46PM:</b> The NY Times and Wall Street Journal report that some group has successfully hacked electronic voting systems across the country, including Florida. The malware has spread to tens of thousands of machines and deletes any record of its activity, so the returning officer of Florida concedes they actually have no idea who won the state — and don't see how they can figure it out. <br><br>

What on Earth happens next? <br><br>

Today’s guest — world-renowned computer security expert Bruce Schneier — thinks this scenario is plausible, and the ensuing chaos would sow so much distrust that half the country would never accept the election result. <br><br>

Unfortunately the US has no recovery system for a situation like this, unlike Parliamentary democracies, which can just rerun the election a few weeks later.<p></p><p>

•  <a href=""https://80k.link/schneier-sn""><b>Links to learn more, summary and full transcript.</b></a><br>
•  Motivating article: <a href=""https://80k.link/iscfgcrr""><b>Information security careers for global catastrophic risk reduction</b></a> by Zabel and Muehlhauser</p><p>

The constitution says the state legislature decides, and they can do so however they like; one tied local election in Texas was settled by playing a hand of poker. <br><br>

Elections serve two purposes. The first is the obvious one: to pick a winner. The second, but equally important, is to convince the loser to go along with it — which is why hacks often focus on convincing the losing side that the election wasn't fair. <br><br>

Schneier thinks there's a need to agree how this situation should be handled before something like it happens, and America falls into severe infighting as everyone tries to turn the situation to their political advantage. <br><br>

And to fix our voting systems, we urgently need two things: a voter-verifiable paper ballot and risk-limiting audits. <br><br>

According to Schneier, computer security experts look at current electronic voting machines and can barely believe their eyes. But voting machine designers never understand the security weakness of what they're designing, because they have a bureaucrat's rather than a hacker's mindset. <br><br>

The ideal computer security expert walks into a shop and thinks, ""You know, here's how I would shoplift."" They automatically see where the cameras are, whether there are alarms, and where the security guards aren't watching. <br><br>

In this episode we discuss this hacker mindset, and how to use a career in security to protect democracy and guard dangerous secrets from people who shouldn't get access to them.</p><p><br>

We also cover: <br><br>

• How can we have surveillance of dangerous actors, without falling back into authoritarianism? <br>
• When if ever should information about weaknesses in society's security be kept secret? <br>
• How secure are nuclear weapons systems around the world? <br>
• How worried should we be about deep-fakes? <br>• Schneier’s critiques of blockchain technology <br>
• How technologists should be vital in shaping policy <br>
• What are the most consequential computer security problems today? <br>
• Could a career in information security be very useful for reducing global catastrophic risks? <br>
• And more.</p><p><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the linked transcript.</b><br><br>


<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/6a959f5e-f74c-11e9-83ef-0eee8760da3a/images/main.jpg?1601340452906,http://feedproxy.google.com/~r/80000HoursPodcast/~5/ml7c2c4vlF8/64---bruce-schneier-on-the-big-risks-in-computer-security-secrets-and-ubiquitous-surveillance.mp3,False
37,"Rob Wiblin on plastic straws, nicotine, doping, & whether changing the long-term is really possible","Today's episode is a compilation of interviews I recently recorded for two other shows, <i>Love Your Work</i> and <i>The Neoliberal Podcast</i>.&nbsp;<br><br>

If you've listened to absolutely everything on this podcast feed, you'll have heard four interviews with me already, but fortunately I don't think these two include much repetition, and I've gotten a decent amount of positive feedback on both.&nbsp;<br><br>

First up, I speak with David Kadavy on his show,&nbsp;<a href=""https://kadavy.net/blog/archive/love-your-work/""><b>Love Your Work</b></a>.&nbsp;<br><br>

This is a particularly personal and relaxed interview. We talk about all sorts of things, including nicotine gum, plastic straw bans, whether recycling is important, how many lives a doctor saves, why interviews should go for at least 2 hours, how athletes doping could be good for the world, and many other fun topics.&nbsp;<br><br>

<p>• Our <b>annual impact survey</b> is about to close — I'd really appreciate if you could <b><a href=""https://80k.link/isrw"">take 3–10 minutes to fill it out now</a>.&nbsp;</b><br>
• <a href=""https://80000hours.org/2019/09/rob-wiblin-on-many-things/""><b>The blog post</b></a> about this episode.<br><br>

At some points we even actually discuss effective altruism and 80,000 Hours, but you can easily skip through those bits if they feel too familiar.&nbsp;<br><br>

The second interview is with Jeremiah Johnson on the <a href=""https://neoliberalproject.org/podcast""><b>Neoliberal Podcast</b></a>. It starts 2 hours and 15 minutes into this recording.&nbsp;<br><br>

Neoliberalism <a href=""https://medium.com/@s8mb/im-a-neoliberal-maybe-you-are-too-b809a2a588d6""><b>in the sense used by this show</b></a> is not the free market fundamentalism you might associate with the term. Rather it's a centrist or even centre-left view that supports things like social liberalism, multilateral international institutions, trade, high rates of migration, racial justice, inclusive institutions, financial redistribution, prioritising the global poor, market urbanism, and environmental sustainability.&nbsp;<br><br>

This is the more demanding of the two conversations, as listeners to that show have already heard of effective altruism, so we were able to get the best arguments Jeremiah could offer against focusing on improving the long term future of the world.&nbsp;<br><br>

Jeremiah is more of a fan of donating to evidence-backed global health charities recommended by GiveWell, and does so himself.&nbsp;<br><br>

I appreciate him having done his homework and forcing me to do my best to explain how well my views can stand up to counterarguments. It was a challenge for me to paint the whole picture in the half an hour we spent on longterm and I expect there's answers in there which will be fresh even for regular listeners.&nbsp;<br><br>

I hope you enjoy both conversations! Feel free to email me with any feedback.<br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/ecb35ebc-dfd8-11e9-b86b-0e3752e5fd66/images/main.jpg?1601340452998,http://feedproxy.google.com/~r/80000HoursPodcast/~5/GNLAmgulCAE/rob-wiblin-on-plastic-straws-nicotine-doping--whether-changing-the-long-term-is-really-possible.mp3,False
38,"Have we helped you have a bigger social impact? Our annual survey, plus other ways we can help you.","<p><b>1. Fill out our&nbsp;<a href=""https://80000hours.org/impact-survey/?utm_campaign=podcast__impact-survey-appeal&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"">annual impact survey here</a>.&nbsp;</b></p><p><b>2. Find a great vacancy on&nbsp;<a href=""https://80000hours.org/job-board/?utm_campaign=podcast__impact-survey-appeal&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"">our job board</a>.&nbsp;</b></p><p><b>3. Learn about our <a href=""https://80000hours.org/key-ideas/?utm_campaign=podcast__impact-survey-appeal&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"">key ideas, and get links to our top articles</a>.&nbsp;</b></p><p><b>4. Join <a href=""https://80000hours.org/newsletter/?utm_campaign=podcast__impact-survey-appeal&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"">our newsletter</a> for an email about what's new, every 2 weeks or so.&nbsp;</b></p><p><b>5. Or follow our pages on <a href=""https://facebook.com/80000hours"">Facebook</a> and <a href=""https://twitter.com/80000hours"">Twitter</a>.&nbsp;</b></p><p>——&nbsp;</p><p>Once a year 80,000 Hours runs a survey to find out whether we've helped our users have a larger social impact with their life and career.&nbsp;</p><p>We and our donors need to know whether our services, like this podcast, are helping people enough to continue them or scale them up, and it's only by hearing from you that we can make these decisions in a sensible way.&nbsp;</p><p>So, if 80,000 Hours' podcast, job board, articles, headhunting, advising or other projects have somehow contributed to your life or career plans, please take 3–10 minutes to let us know how.&nbsp;</p><p>You can also let us know where we've fallen short, which helps us fix problems with what we're doing.&nbsp;</p><p>We've refreshed the survey this year, hopefully making it easier to fill out than in the past.&nbsp;</p><p>We'll keep this appeal up for about two weeks, but if you fill it out now that means you definitely won't forget!&nbsp;</p><p>Thanks so much, and talk to you again in a normal episode soon.&nbsp;</p><p>— Rob</p><br><br>Tag for internal use: this RSS feed is originating in BackTracks.",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/63d0b9d2-d8b7-11e9-a35e-0e0bd6f362e0/images/main.jpg?1601340453087,http://feedproxy.google.com/~r/80000HoursPodcast/~5/Rm-UPK-zVj4/impact-survey-appeal-and-things-80k-does-sep-2019.mp3,False
39,"#63 - Vitalik Buterin on better ways to fund public goods, blockchain's failures, & effective giving","<p>Historically, progress in the field of cryptography has had <a href=""https://80k.link/garfinkel"">major consequences</a>. It has changed the course of major wars, made it possible to do business on the internet, and enabled private communication between both law-abiding citizens and dangerous criminals. Could it have similarly significant consequences in future?<br><br>

Today's guest — Vitalik Buterin — is world-famous as the lead developer of Ethereum, a successor to the cryptographic-currency Bitcoin, which added the capacity for smart contracts and decentralised organisations. Buterin first proposed Ethereum at the age of 20, and by the age of 23 its success had likely made him a billionaire.<br><br>

At the same time, far from indulging hype about these so-called 'blockchain' technologies, he has been candid about the limited good accomplished by Bitcoin and other currencies developed using cryptographic tools — and the breakthroughs that will be needed before they can have a meaningful social impact. In his own words, *""blockchains as they currently exist are in many ways a joke, right?""*<br><br>

But Buterin is not just a realist. He's also an idealist, who has been helping to advance big ideas for new social institutions that might help people better coordinate to pursue their shared goals.<br><br>

<a href=""https://80k.link/buterin""><b>Links to learn more, summary and full transcript.</b></a><br><br>

By combining theories in economics and mechanism design with advances in cryptography, he has been pioneering the new interdiscriplinary field of 'cryptoeconomics'. Economist Tyler Cowen has<a href=""https://80k.link/tyler-buterin"">observed that</a>, <em>""at 25, Vitalik appears to repeatedly rediscover important economics results from famous papers, without knowing about the papers at all.""</em><br><br> 

Along with <a href=""https://80k.link/weyl-buterin"">previous guest Glen Weyl</a>, Buterin has helped develop a model for so-called <a href=""https://80k.link/qf"">'quadratic funding'</a>, which in principle could transform the provision of 'public goods'. That is, goods that people benefit from whether they help pay for them or not.<br><br>

Examples of goods that are fully or partially 'public goods' include sound decision-making in government, international peace, scientific advances, disease control, the existence of smart journalism, preventing climate change, deflecting asteroids headed to Earth, and the elimination of suffering. Their underprovision in part reflects the difficulty of getting people to pay for anything when they can instead free-ride on the efforts of others. Anything that could reduce this failure of coordination might transform the world.<br><br>

But these and other related proposals face major hurdles. They're vulnerable to collusion, might be used to fund scams, and remain untested at a small scale — not to mention that anything with a square root sign in it is going to struggle to achieve societal legitimacy. Is the prize large enough to justify efforts to overcome these challenges?<br><br>

In today's extensive three-hour interview, Buterin and I cover:<br><br>

• What the blockchain has accomplished so far, and what it might achieve in the next decade;<br>
• Why many social problems can be viewed as a coordination failure to provide a public good;<br>
• Whether any of the ideas for decentralised social systems emerging from the blockchain community could really work;<br>
• His view of 'effective altruism' and 'long-termism';<br>
• Why he is optimistic about 'quadratic funding', but pessimistic about replacing existing voting with <a href=""https://80k.link/qv"">'quadratic voting'</a>;<br>
• Why humanity might have to abandon living in cities;<br>
• And much more.<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/ca789fe2-cdcc-11e9-a7f4-0eacf2b0baaa/images/main.jpg?1601340453174,http://feedproxy.google.com/~r/80000HoursPodcast/~5/xxXtyiMmcTE/63---vitalik-buterin-on-better-ways-to-fund-public-goods-blockchains-failures--effective-giving-fixed-edits-2.mp3,False
40,"#62 - Paul Christiano on messaging the future, increasing compute, & how CO2 impacts your brain","Imagine that – one day – humanity dies out. At some point, many millions of years later, intelligent life might well evolve again. Is there any message we could leave that would reliably help them out?<br><br>

In his second appearance on the 80,000 Hours Podcast, machine learning researcher and polymath Paul Christiano suggests we try to answer this question with a related thought experiment: are there any messages we might want to send back to our ancestors in the year 1700 that would have made history likely to go in a better direction than it did? It seems there probably are.<br><br>

• <a href=""https://80k.link/christiano-2""><b>Links to learn more, summary, and full transcript.</b></a><br>
• <a href=""https://80k.link/christiano-1""><b>Paul's first appearance on the show in episode 44.</b></a><br>
• <a href=""https://80k.link/christiano-outtake""><b>An out-take on decision theory.</b></a><br><br>

We could tell them hard-won lessons from history; mention some research questions we wish we'd started addressing earlier; hand over all the social science we have that fosters peace and cooperation; and at the same time steer clear of engineering hints that would speed up the development of dangerous weapons.<br><br>

But, as <a href=""https://sideways-view.com/2018/06/07/messages-to-the-future/"">Christiano points out</a>, even if we could satisfactorily figure out what we'd like to be able to tell our ancestors, that's just the first challenge. We'd need to leave the message somewhere that they could identify and dig up. While there are some promising options, this turns out to be remarkably hard to do, as anything we put on the Earth's surface quickly gets buried far underground.<br><br>

But even if we figure out a satisfactory message, <em>and</em> a ways to ensure it's found, a civilization this far in the future won't speak any language like our own. And being another species, they presumably won't share as many fundamental concepts with us as humans from 1700. If we knew a way to leave them thousands of books and pictures in a material that wouldn't break down, would they be able to decipher what we meant to tell them, or would it simply remain a mystery?<br><br>

That's just one of many playful questions discussed in today's episode with Christiano — a frequent writer who's willing to brave questions that others find too strange or hard to grapple with.<br><br>

We also talk about why <a href=""https://sideways-view.com/2019/05/25/analyzing-divestment/"">divesting</a> a little bit from harmful companies might be more useful than I'd been thinking. Or whether creatine might make us a bit smarter, and carbon dioxide filled conference rooms make us a lot stupider.<br><br>

Finally, we get a big update on progress in machine learning and efforts to make sure it's reliably aligned with our goals, which is Paul's main research project. He responds to the views that DeepMind's Pushmeet Kohli espoused <a href=""https://80k.link/pushmeet-kohli"">in a previous episode</a>, and we discuss whether we'd be better off if AI progress turned out to be most limited by algorithmic insights, or by our ability to manufacture enough computer processors.<br><br>

Some other issues that come up along the way include:<br><br>

• Are there any supplements people can take that make them think better?<br>
• What implications do our views on meta-ethics have for aligning AI with our goals?<br>
• Is there much of a risk that the future will contain anything optimised for causing harm?<br>
• An <a href=""https://80k.link/christiano-outtake"">out-take about the implications of decision theory</a>, which we decided was too confusing and confused to stay in the main recording.<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.</b><br><br>


<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/98d07d42-b78e-11e9-b068-0efe278d1810/images/main.jpg?1601340453263,http://feedproxy.google.com/~r/80000HoursPodcast/~5/b60KWZ0p4K8/62---paul-christiano-on-messaging-the-future-increasing-compute--how-co2-impacts-your-brain.mp3,False
41,"#61 - Helen Toner on emerging technology, national security, and China","<p>From 1870 to 1950, the introduction of electricity transformed life in the US and UK, as people gained access to lighting, radio and a wide range of household appliances for the first time. Electricity turned out to be a general purpose technology that could help with almost everything people did.<br><br>

Some think this is the best historical analogy we have for how machine learning could alter life in the 21st century.<br><br>

In addition to massively changing everyday life, past general purpose technologies have also changed the nature of war. For example, when electricity was introduced to the battlefield, commanders gained the ability to communicate quickly with units in the field over great distances.<br><br>

How might international security be altered if the impact of machine learning reaches a similar scope to that of electricity? Today's guest — Helen Toner — recently helped found the <em>Center for Security and Emerging Technology</em> at Georgetown University to help policymakers prepare for such disruptive technical changes that might threaten international peace.<br><br>

• <a href=""https://80k.link/helen-toner""><b>Links to learn more, summary and full transcript</b></a><br>
• <a href=""https://80k.link/phil-helen""><b>Philosophy is one of the hardest grad programs. Is it worth it, if you want to use ideas to change the world?</b></a> by Arden Koehler and Will MacAskill<br>
• <a href=""https://80k.link/policy-helen""><b>The case for building expertise to work on US AI policy, and how to do it</b></a> by Niel Bowerman<br>
• <a href=""https://80k.link/job-board-helen""><b>AI strategy and governance roles on the job board</b></a><br><br>

Their first focus is machine learning (ML), a technology which allows computers to recognise patterns, learn from them, and develop 'intuitions' that inform their judgement about future cases. This is something humans do constantly, whether we're playing tennis, reading someone's face, diagnosing a patient, or figuring out which business ideas are likely to succeed.<br><br>

Sometimes these ML algorithms can seem uncannily insightful, and they're only getting better over time. Ultimately a wide range of different ML algorithms could end up helping us with all kinds of decisions, just as electricity wakes us up, makes us coffee, and brushes our teeth -- all in the first five minutes of our day.<br><br>

Rapid advances in ML, and the many prospective military applications, have people worrying about an 'AI arms race' between the US and China. Henry Kissinger and the past CEO of Google Eric Schmidt recently wrote that AI could ""destabilize everything from nuclear détente to human friendships."" Some politicians talk of classifying and restricting access to ML algorithms, lest they fall into the wrong hands.<br><br>

But if electricity is the best analogy, you could reasonably ask — was there an arms race in electricity in the 19th century? Would that have made any sense? And could someone have changed the course of history by changing who first got electricity and how they used it, or is that a fantasy?<br><br>

In today's episode we discuss the research frontier in the emerging field of AI policy and governance, how to have a career shaping US government policy, and Helen's experience living and studying in China.<br><br>

We cover:<br><br>

• Why immigration is the main policy area that should be affected by AI advances today.<br>
• Why talking about an 'arms race' in AI is premature.<br>
• How Bobby Kennedy may have positively affected the Cuban Missile Crisis.<br>
• Whether it's possible to become a China expert and still get a security clearance.<br>
• Can access to ML algorithms be restricted, or is that just not practical?<br>
• Whether AI could help stabilise authoritarian regimes.<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b><br><br>


<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/5a85a7ac-a8a1-11e9-89e5-0e66e68c3a30/images/main.jpg?1601340453351,http://feedproxy.google.com/~r/80000HoursPodcast/~5/SeiEdrOfjmA/61---helen-toner-on-emerging-technology-national-security-and-china.mp3,False
42,"#60 - Prof Tetlock on why accurate forecasting matters for everything, and how you can do it better","Have you ever been infuriated by a doctor's unwillingness to give you an honest, probabilistic estimate about what to expect? Or a lawyer who won't tell you the chances you'll win your case?<br><br>

Their behaviour is so frustrating because accurately predicting the future is central to every action we take. If we can't assess the likelihood of different outcomes we're in a complete bind, whether the decision concerns war and peace, work and study, or Black Mirror and RuPaul's Drag Race.<br><br>

Which is why the research of Professor Philip Tetlock is relevant for all of us each and every day.<br><br>

He has spent 40 years as a meticulous social scientist, collecting millions of predictions from tens of thousands of people, in order to figure out how good humans really are at foreseeing the future, and what habits of thought allow us to do better.<br><br>

Along with other psychologists, he identified that many ordinary people are attracted to a 'folk probability' that draws just three distinctions — 'impossible', 'possible' and 'certain' — and which leads to major systemic mistakes. But with the right mindset and training we can become capable of accurately discriminating between differences as fine as 56% as against 57% likely.<br><br>

• <a href=""https://80k.link/tetlock-2""><b>Links to learn more, summary and full transcript</b></a><br>
• <a href=""https://80k.link/tetlock-calibration""><b>The calibration training app</b></a><br>
• <a href=""http://80k.link/civ""><b>Sign up for the Civ-5 counterfactual forecasting tournament</b></a><br>
• <a href=""https://80k.link/ai-impacts-summary""><b>A review of the evidence on good forecasting practices</b></a><br>
• <a href=""https://eaglobal.org""><b>Learn more about Effective Altruism Global</b></a><br><br>

In the aftermath of Iraq and WMDs the US intelligence community hired him to prevent the same ever happening again, and his guide — <i>Superforecasting: The Art and Science of Prediction</i> — became a bestseller back in 2014.<br><br>

That was five years ago. In today's interview, Tetlock explains how his research agenda continues to advance, today using the game <i>Civilization 5</i> to see how well we can predict what <i>would have</i>&nbsp;happened in elusive counterfactual worlds we never get to see, and discovering how simple algorithms can complement or substitute for human judgement.<br><br>

We discuss how his work can be applied to your personal life to answer high-stakes questions, like how likely you are to thrive in a given career path, or whether your business idea will be a billion-dollar unicorn — or fall apart catastrophically. (To help you get better at figuring those things out, our site now has a <a href=""https://80k.link/tetlock-calibration"">training app</a> developed by the Open Philanthropy Project and Clearer Thinking that teaches you to distinguish your '70 percents' from your '80 percents'.)<br><br>

We also bring some tough methodological questions raised by the author of a recent <a href=""https://aiimpacts.org/evidence-on-good-forecasting-practices-from-the-good-judgment-project-an-accompanying-blog-post/"">review of the forecasting literature</a>. And we find out what jobs people can take to make improving the reasonableness of decision-making in major institutions that shape the world their profession, as it has been for Tetlock over many decades.<br><br>

We view Tetlock's work as so core to living well that we've brought him back for a second and longer appearance on the show — his first was back in <a href=""https://80k.link/tetlock1"">episode 15</a>.<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b><br><br>


<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/f6bacf92-992b-11e9-8563-0e5c3cf3635a/images/main.jpg?1601340453440,http://feedproxy.google.com/~r/80000HoursPodcast/~5/J-MsDPEp754/60---prof-philip-tetlock-on-the-cutting-edge-in-forecasting-research-mp3-june-28.mp3,False
43,"#59 - Prof Cass Sunstein on how change happens, and why it's so often abrupt & unpredictable","<p>It can often feel hopeless to be an activist seeking social change on an obscure issue where most people seem opposed or at best indifferent to you. But according to a new book by Professor Cass Sunstein, they shouldn't despair. Large social changes are often abrupt and unexpected, arising in an environment of seeming public opposition.<br><br>

The Communist Revolution in Russia spread so swiftly it confounded even Lenin. Seventy years later the Soviet Union collapsed just as quickly and unpredictably.<br><br>

In the modern era we have gay marriage, #metoo and the Arab Spring, as well as nativism, Euroskepticism and Hindu nationalism.<br><br>

How can a society that so recently seemed to support the status quo bring about change in years, months, or even weeks?<br><br>

Sunstein — co-author of <i>Nudge</i>, Obama White House official, and by far the most cited legal scholar of the late 2000s — aims to unravel the mystery and figure out the implications in his new book <i>How Change Happens</i>.<br><br>

He pulls together three phenomena which social scientists have studied in recent decades: <i>preference falsification</i>, <i>variable thresholds for action</i>, and <i>group polarisation</i>. If Sunstein is to be believed, together these are a cocktail for social shifts that are chaotic and fundamentally unpredictable.<br><br>

• <a href=""https://80000hours.org/podcast/episodes/cass-sunstein-how-change-happens/?utm_campaign=podcast__cass-sunstein&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>Links to learn more, summary and full transcript.</b></a><br>
• <a href=""https://80000hours.org/2019/05/annual-review-dec-2018/?utm_campaign=podcast__cass-sunstein&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>80,000 Hours Annual Review 2018.</b></a><br>
• <a href=""https://80000hours.org/2019/05/annual-review-dec-2018/#how-to-donate""><b>How to donate to 80,000 Hours.</b></a><br><br>

In brief, people constantly misrepresent their true views, even to close friends and family. They themselves aren't quite sure how socially acceptable their feelings would have to become, before they revealed them, or joined a campaign for social change. And a chance meeting between a few strangers can be the spark that radicalises a handful of people, who then find a message that can spread their views to millions.<br><br>

According to Sunstein, it's <i>""much, much easier""</i> to create social change when large numbers of people secretly or latently agree with you. But 'preference falsification' is so pervasive that it's no simple matter to figure out when that's the case.<br><br>

In today's interview, we debate with Sunstein whether this model of cultural change is accurate, and if so, what lessons it has for those who would like to shift the world in a more humane direction. We discuss:<br><br>

• How much people misrepresent their views in democratic countries.<br>
• Whether the finding that groups with an existing view tend towards a more extreme position would stand up in the replication crisis.<br>
• When is it justified to encourage your own group to polarise?<br>
• Sunstein's difficult experiences as a pioneer of animal rights law.<br>
• Whether activists can do better by spending half their resources on public opinion surveys.<br>
• Should people be more or less outspoken about their true views?<br>
• What might be the next social revolution to take off?<br>
• How can we learn about social movements that failed and disappeared?<br>
• How to find out what people really think.<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems: type 80,000 Hours into your podcasting app. Or read the transcript on our site.</b><br><br>


<i>The 80,000 Hours Podcast is produced by Keiran Harris.</i></p>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/00b8d1f6-8eb9-11e9-bec4-0e5c3cf3635a/images/main.jpg?1601340453530,http://feedproxy.google.com/~r/80000HoursPodcast/~5/bHo2x8DZ9lM/59---cass-sunstein-on-how-change-happens.mp3,False
44,#58 - Pushmeet Kohli of DeepMind on designing robust & reliable AI systems and how to succeed in AI,"When you're building a bridge, responsibility for making sure it won't fall over isn't handed over to a few 'bridge not falling down engineers'. Making sure a bridge is safe to use and remains standing in a storm is completely central to the design, and indeed the entire project.<br><br>

When it comes to artificial intelligence, commentators often distinguish between enhancing the capabilities of machine learning systems and enhancing their safety. But to Pushmeet Kohli, principal scientist and research team leader at DeepMind, research to make AI robust and reliable is no more a side-project in AI design than keeping a bridge standing is a side-project in bridge design.<br><br>

Far from being an overhead on the 'real' work, it’s an essential part of making AI systems work at all. We don’t want AI systems to be out of alignment with our intentions, and that consideration must arise throughout their development.<br><br>

Professor Stuart Russell — co-author of the most popular AI textbook — <a href=""https://youtu.be/GYQrNfSmQ0M?t=2651"">has gone as far as to suggest</a> that if this view is right, it may be time to retire the term ‘AI safety research’ altogether.<br><br>

• <b>Want to be notified about high-impact opportunities to help ensure AI remains safe and beneficial? <a href=""https://80000hours.org/opportunities-in-ai/?utm_campaign=podcast__pushmeet-kohli&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"" title="""">Tell us a bit about yourself and we’ll get in touch if an opportunity matches your background and interests.</a></b><br><br>

• <a href=""https://80k.link/pushmeet-kohli""><b>Links to learn more, summary and full transcript.</b></a><br><br>

• <b>And a few <a href=""https://80k.link/deepmind-roles"">added thoughts on non-research roles</a>.</b><br><br>

With the goal of designing systems that are reliably consistent with desired specifications, DeepMind have <a href=""https://deepmind.com/blog/robust-and-verified-ai/"">recently published work</a> on important technical challenges for the machine learning community.<br><br>

For instance, Pushmeet is looking for efficient ways to test whether a system conforms to the desired specifications, even in peculiar situations, by creating an 'adversary' that proactively seeks out the worst failures possible. If the adversary can efficiently identify the worst-case input for a given model, DeepMind can catch rare failure cases before deploying a model in the real world. In the future single mistakes by autonomous systems may have very large consequences, which will make even small failure probabilities unacceptable. <br><br>

He's also looking into 'training specification-consistent models' and formal verification', while other researchers at DeepMind working on their <a href=""https://80k.link/dmsr1"">AI safety agenda</a> are figuring out how to <a href=""https://80k.link/dmsr2"">understand agent incentives</a>, <a href=""https://80k.link/dmsr3"">avoid side-effects</a>, and <a href=""https://medium.com/@deepmindsafetyresearch/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84"">model AI rewards</a>.<br><br>

In today’s interview, we focus on the convergence between broader AI research and robustness, as well as:<br><br>

• DeepMind’s work on the protein folding problem<br>
• Parallels between ML problems and past challenges in software development and computer security<br>
• How can you analyse the thinking of a neural network?<br>
• Unique challenges faced by DeepMind’s technical AGI safety team<br>
• How do you communicate with a non-human intelligence?<br>
• What are the biggest misunderstandings about AI safety and reliability?<br>
• Are there actually a lot of disagreements within the field?<br>
• The difficulty of forecasting AI development<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/73a4f462-8615-11e9-9340-0ee7196fda3c/images/main.jpg?1601340453616,http://feedproxy.google.com/~r/80000HoursPodcast/~5/J-OGQhgrKfU/58---pushmeet-kohli-of-deepmind-on-designing-robust--reliable-ai-systems-and-how-to-succeed-in-ai-mp3ii.mp3,False
45,"Rob Wiblin on human nature, new technology, and living a happy, healthy & ethical life","This is a cross-post of some interviews Rob did recently on two other podcasts — <a href=""https://mission.org/missiondaily/"">Mission Daily</a>&nbsp;(from 2m) and <a href=""https://goodlifepodcast.podbean.com/"">The Good Life</a>&nbsp;(from 1h13m).<br><br>

Some of the content will be familiar to regular listeners — but if you’re at all interested in Rob’s personal thoughts, there should be quite a lot of new material to make listening worthwhile.<br><br>

The first interview is with Chad Grills. They focused largely on new technologies and existential risks, but also discuss topics like:<br><br>

• Why Rob is wary of fiction<br>
• Egalitarianism in the evolution of hunter gatherers<br>
• How to stop social media screwing up politics<br>
• Careers in government versus business<br><br>

The second interview is with Prof Andrew Leigh - the Shadow Assistant Treasurer in Australia. This one gets into more personal topics than we usually cover on the show, like: <br><br>

• What advice would Rob give to his teenage self?<br>
• Which person has most shaped Rob’s view of living an ethical life?<br>
• Rob’s approach to giving to the homeless<br>
• What does Rob do to maximise his own happiness?<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/a0606768-75ab-11e9-bfa0-0e96c47ddb2a/images/main.jpg?1601340453705,http://feedproxy.google.com/~r/80000HoursPodcast/~5/QG8rt-oxS0Q/rob-wiblin-on-human-nature-new-technology-and-living-a-happy-healthy--ethical-life-128kbps-mp3.mp3,False
46,#57 - Tom Kalil on how to do the most good in government,"You’re 29 years old, and you’ve just been given a job in the White House. How do you quickly figure out how the US Executive Branch behemoth actually works, so that you can have as much impact as possible - before you quit or get kicked out?<br><br>

That was the challenge put in front of Tom Kalil in 1993.<br><br>

He had enough success to last a full 16 years inside the Clinton and Obama administrations, working to foster the development of the internet, then nanotechnology, and then cutting-edge brain modelling, among other things.<br><br>

But not everyone figures out how to move the needle. In today's interview, Tom shares his experience with how to increase your chances of getting an influential role in government, and how to make the most of the opportunity if you get in.<br><br>

<a href=""https://80k.link/tom-kalil""><b>Links to learn more, summary and full transcript.</b></a><br><br>
<a href=""https://80k.link/tom-kalil-coaching""><b>Interested in US AI policy careers? Apply for one-on-one career advice here.</b><br><br>
</a><a href=""https://cset.georgetown.edu/careers/""><b>Vacancies at the Center for Security and Emerging Technology.</b><br><br>
</a><a href=""https://80k.link/tom-kalil-job-board""><b>Our high-impact job board, which features other related opportunities.</b></a><br><br>

He believes that Congressional gridlock leads people to greatly underestimate how much the Executive Branch can and does do on its own every day. Decisions by individuals change how billions of dollars are spent; regulations are enforced, and then suddenly they aren't; and a single sentence in the State of the Union can get civil servants to pay attention to a topic that would otherwise go ignored.<br><br>

Over years at the White House Office of Science and Technology Policy, 'Team Kalil' built up a white board of principles. For example, 'the schedule is your friend': setting a meeting date with the President can force people to finish something, where they otherwise might procrastinate.<br><br>

Or 'talk to who owns the paper'. People would wonder how Tom could get so many lines into the President's speeches. The answer was ""figure out who's writing the speech, find them with the document, and tell them to add the line."" Obvious, but not something most were doing.<br><br>

Not everything is a precise operation though. Tom also tells us the story of NetDay, a project that was put together at the last minute because the President incorrectly believed it was already organised – and decided he was going to announce it in person.<br><br>

In today's episode we get down to nuts &amp; bolts, and discuss:<br><br>

• How did Tom spin work on a primary campaign into a job in the next White House?<br>
• Why does Tom think hiring is the most important work he did, and how did he decide who to bring onto the team?<br>
• How do you get people to do things when you don't have formal power over them?<br>
• What roles in the US government are most likely to help with the long-term future, or reducing existential risks?<br>
• Is it possible, or even desirable, to get the general public interested in abstract, long-term policy ideas?<br>
• What are 'policy entrepreneurs' and why do they matter?<br>
• What is the role for prizes in promoting science and technology? What are other promising policy ideas?<br>
• Why you can get more done by not taking credit.<br>
• What can the White House do if an agency isn't doing what it wants?<br>
• How can the effective altruism community improve the maturity of our policy recommendations?<br>
• How much can talented individuals accomplish during a short-term stay in government?<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app..</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/457e3002-6546-11e9-a608-0eb1e99d174e/images/main.jpg?1601340453811,http://feedproxy.google.com/~r/80000HoursPodcast/~5/xhjiHRJTO2o/57---tom-kalil-on-getting-things-done-in-government.mp3,False
47,"#56 - Persis Eskander on wild animal welfare and what, if anything, to do about it","Elephants in chains at travelling circuses; pregnant pigs trapped in coffin sized crates at factory farms; deers living in the wild. We should welcome the last as a pleasant break from the horror, right?<br><br>

Maybe, but maybe not. While we tend to have a romanticised view of nature, life in the wild includes a range of extremely negative experiences. <br><br>

Many animals are hunted by predators, and constantly have to remain vigilant about the risk of being killed, and perhaps experiencing the horror of being eaten alive. Resource competition often leads to chronic hunger or starvation. Their diseases and injuries are never treated. In winter animals freeze to death; in droughts they die of heat or thirst. <br><br>

There are fewer than 20 people in the world dedicating their lives to researching these problems.<br><br>

But according to Persis Eskander, researcher at the Open Philanthropy Project, if we sum up the negative experiences of all wild animals, their sheer number could make the scale of the problem larger than most other near-term concerns.<br><br>

<a href=""https://80000hours.org/podcast/episodes/persis-eskander-wild-animal-welfare/?utm_campaign=podcast__persis-eskander&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>Links to learn more, summary and full transcript.</b></a><br><br>

Persis urges us to recognise that nature isn’t inherently good or bad, but rather the result of an amoral evolutionary process. For those that can't survive the brutal indifference of their environment, life is often a series of bad experiences, followed by an even worse death.<br><br>

But should we actually intervene? How do we know what animals are sentient? How often do animals feel hunger, cold, fear, happiness, satisfaction, boredom, and intense agony? Are there long-term technologies that could eventually allow us to massively improve wild animal welfare?<br><br>

For most of these big questions, the answer is: we don’t know. And Persis thinks we're far away from knowing enough to start interfering with ecosystems. But that's all the more reason to start looking at these questions.<br><br> 

There are <em>some</em> concrete steps we could take today, like improving the way wild caught fish are slaughtered. Fish might lack the charisma of a lion or the intelligence of a pig, but if they have the capacity to suffer — and evidence suggests that they do — we should be thinking of ways to kill them painlessly rather than allowing them to suffocate to death over hours.<br><br>

In today’s interview we explore wild animal welfare as a new field of research, and discuss:<br><br>

• Do we have a moral duty towards wild animals or not?<br>
• How should we measure the number of wild animals?<br>
• What are some key activities that generate a lot of suffering or pleasure for wild animals that people might not fully appreciate?<br>
• Is there a danger in imagining how we as humans would feel if we were put into their situation?<br>
• Should we eliminate parasites and predators?<br>
• How important are insects?<br>
• How strongly should we focus on just avoiding humans going in and making things worse?<br>
• How does this compare to work on farmed animal suffering?<br>
• The most compelling arguments for humanity not dedicating resources to wild animal welfare<br>
• Is there much of a case for the idea that this work could improve the very long-term future of humanity?<br><br>

Rob is then joined by two of his colleagues — Niel Bowerman and Michelle Hutchinson — to quickly discuss:<br><br>

• The importance of figuring out your values<br>
• Chemistry, psychology, and other different paths towards working on wild animal welfare<br>
• How to break into new fields<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/499e1a60-5f99-11e9-8ce5-0e280b71e04c/images/main.jpg?1601340453917,http://feedproxy.google.com/~r/80000HoursPodcast/~5/WvY7Hzma2Es/56---persis-eskander-on-improving-wild-animal-welfare.mp3,False
48,#55 - Lutter & Winter on founding charter cities with outstanding governance to end poverty,"Governance matters. Policy change quickly took China from famine to fortune; Singapore from swamps to skyscrapers; and Hong Kong from fishing village to financial centre. Unfortunately, many governments are hard to reform and — to put it mildly — it's not easy to found a new country.<br><br>

This has prompted poverty-fighters and political dreamers to look for creative ways to get new and better 'pseudo-countries' off the ground. The poor could then voluntary migrate to in search of security and prosperity. And innovators would be free to experiment with new political and legal systems without having to impose their ideas on existing jurisdictions. <br><br>

The 'seasteading movement' imagined founding new self-governing cities on the sea, but obvious challenges have kept that one on the drawing board. Nobel Prize winner and World Bank President Paul Romer suggested 'charter cities', where a host country would volunteer for another country with better legal institutions to effectively govern some of its territory. But that idea too ran aground for political, practical and personal reasons.<br><br>

Now Mark Lutter and Tamara Winter, of <em>The Center for Innovative Governance Research</em> (CIGR), are reviving the idea of 'charter cities', with some modifications. Gone is the idea of transferring sovereignty. Instead these cities would look more like the 'special economic zones' that worked miracles for Taiwan and China among others. But rather than keep the rest of the country's rules with a few pieces removed, they hope to start from scratch, opting in to the laws they want to keep, in order to leap forward to ""best practices in commercial law.""<br><br>

<a href=""https://80k.link/lutter-and-winter""><b>Links to learn more, summary and full transcript.</b></a><br><br>

<b><a href=""https://80k.link/rob-on-the-good-life"">Rob on <em>The Good Life: Andrew Leigh in Conversation</em> — on 'making the most of your 80,000 hours'.</a></b><br><br>

The project has quickly gotten attention, with Mark and Tamara <a href=""https://80k.link/lutter-emergent"">receiving funding from Tyler Cowen's Emergent Ventures</a> (discussed in episode 45) and winning a <a href=""https://pioneer.app/blog/meet-the-pioneers-take-3/"">Pioneer tournament</a>.<br><br>

Starting afresh with a new city makes it possible to clear away thousands of harmful rules without having to fight each of the thousands of interest groups that will viciously defend their privileges. Initially the city can fund infrastructure and public services by gradually selling off its land, which appreciates as the city flourishes. And with 40 million people relocating to cities every year, there are plenty of prospective migrants.<br><br>

<em>CIGR</em> is fleshing out how these arrangements would work, advocating for them, and developing supporting services that make it easier for any jurisdiction to implement. They're currently in the process of influencing a new prospective satellite city in Zambia.<br><br>

Of course, one can raise many criticisms of this idea: Is it likely to be taken up? Is CIGR really doing the right things to make it happen? Will it really reduce poverty if it is?<br><br>

We discuss those questions, as well as:<br><br>

• How did Mark get a new organisation off the ground, with fundraising and other staff?<br>
• What made China's 'special economic zones' so successful?<br>
• What are the biggest challenges in getting new cities off the ground?<br>
• How did Mark find and hire Tamara? How did he know this was a good idea?<br>
• Should people care about this idea if they aren't focussed on tackling poverty?<br>
• Why aren't people already doing this?<br>
• Why does Tamara support more people starting families?<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/eaa46b80-5382-11e9-82d3-0e682b96a9c0/images/main.jpg?1601340454031,http://feedproxy.google.com/~r/80000HoursPodcast/~5/7eiJIAzM9Y8/55---mark-lutter--tamara-winters-on-ending-poverty-by-founding-new-cities-with-outstanding-governance.mp3,False
49,"#54 - OpenAI on publication norms, malicious uses of AI, and general-purpose learning algorithms","<p>OpenAI’s <a href=""https://openai.com/blog/learning-dexterity/"">Dactyl</a> is an AI system that can manipulate objects with a human-like robot hand. <a href=""https://openai.com/blog/openai-five/"">OpenAI Five</a> is an AI system that can defeat humans at the video game Dota 2. The strange thing is they were both developed using the same general-purpose reinforcement learning algorithm.<br><br>

How is this possible and what does it show?<br><br>

In today's interview Jack Clark, Policy Director at OpenAI, explains that from a computational perspective using a hand and playing Dota 2 are remarkably similar problems.<br><br>

A robot hand needs to hold an object, move its fingers, and rotate it to the desired position. In Dota 2 you control a team of several different people, moving them around a map to attack an enemy. <br><br>

Your hand has 20 or 30 different joints to move. The number of main actions in Dota 2 is 10 to 20, as you move your characters around a map.<br><br>

When you’re rotating an objecting in your hand, you sense its friction, but you don’t directly perceive the entire shape of the object. In Dota 2, you're unable to see the entire map and perceive what's there by moving around&nbsp;– metaphorically 'touching' the space.<br><br>

<b>Read our new in-depth article on becoming an AI policy specialist: <em><a href=""https://80k.link/openai-episode-to-guide"">The case for building expertise to work on US AI policy, and how to do it</a></em> </b><br><br>

<a href=""https://80k.link/openai-episode""><b>Links to learn more, summary and full transcript</b></a><br><br>

This is true of many apparently distinct problems in life. Compressing different sensory inputs down to a fundamental computational problem which we know how to solve only requires the right general-purpose software.<br><br>

The creation of such increasingly 'broad-spectrum' learning algorithms like&nbsp;has been a key story of the last few years, and this development like have unpredictable consequences, heightening the huge challenges that already exist in AI policy.<br><br>

Today’s interview is a mega-AI-policy-quad episode; Jack is joined by his colleagues <a href=""https://80k.link/openai-to-amanda-episode"">Amanda Askell</a> and <a href=""https://80k.link/openai-to-miles"">Miles Brundage</a>, on the day they released their fascinating and controversial large general language model <a href=""https://openai.com/blog/better-language-models/"">GPT-2</a>.<br><br>

We discuss:<br><br>

• What are the most significant changes in the AI policy world over the last year or two?<br>
• What capabilities are likely to develop over the next five, 10, 15, 20 years?<br>
• How much should we focus on the next couple of years, versus the next couple of decades?<br>
• How should we approach possible malicious uses of AI?<br>
• What are some of the potential ways OpenAI could make things worse, and how can they be avoided?<br>
• Publication norms for AI research<br>
• Where do we stand in terms of arms races between countries or different AI labs?<br>
• The case for creating newsletters<br>
• Should the AI community have a closer relationship to the military?<br>
• Working at OpenAI vs. working in the US government<br>
• How valuable is Twitter in the AI policy world?<br><br>

Rob is then joined by two of his colleagues&nbsp;– Niel Bowerman &amp; Michelle Hutchinson&nbsp;– to quickly discuss:<br><br>

• The reaction to OpenAI's release of GPT-2<br>
• Jack’s critique of our <a href=""https://80k.link/openai-episode-to-guide"">US AI policy</a> article<br>
• How valuable are roles in government?<br>
• Where do you start if you want to write content for a specific audience?<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.</b><br><br>


<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/1a203c70-4a70-11e9-8b7b-0ec2657f145c/images/main.jpg?1601340454133,http://feedproxy.google.com/~r/80000HoursPodcast/~5/j1Qb83w3qS8/54---openai-on-publication-norms-malicious-uses-and-computationally-similar-problems-in-ai-mp3.mp3,False
50,#53 - Kelsey Piper on the room for important advocacy within journalism,"<i>“Politics. Business. Opinion. Science. Sports. Animal welfare. Existential risk.”</i> Is this a plausible future lineup for major news outlets?<br><br>

Funded by the Rockefeller Foundation and given very little editorial direction, Vox's <i><a href=""https://www.vox.com/future-perfect"">Future Perfect</a></i>&nbsp;aspires to be more or less that.<br><br>

Competition in the news business creates pressure to write quick pieces on topical political issues that can drive lots of clicks with just a few hours' work.<br><br>

But according to Kelsey Piper, staff writer for this new section of Vox's website focused on effective altruist themes, <i>Future Perfect's</i>&nbsp;goal is to run in the opposite direction and make room for more substantive coverage that's not tied to the news cycle.<br><br>

They hope that in the long-term talented writers from other outlets across the political spectrum can also be attracted to tackle these topics.<br><br>

<a href=""https://80k.link/kelsey-piper""><b>Links to learn more, summary and full transcript.</b></a><br><br>

<a href=""https://80k.link/kelsey-piper-links""><b>Links to Kelsey's top articles.</b></a><br><br>

Some skeptics of the project have questioned whether this general coverage of global catastrophic risks actually helps reduce them.<br><br>

Kelsey responds: if you decide to dedicate your life to AI safety research, what’s the likely reaction from your family and friends? Do they think of you as someone about to join <i>""that weird Silicon Valley apocalypse thing""</i>? Or do they, having read about the issues widely, simply think <i>“Oh, yeah. That seems important. I'm glad you're working on it.”</i><br><br>

Kelsey believes that really matters, and is determined by broader coverage of these kinds of topics.<br><br>

If that's right, is journalism a plausible pathway for doing the most good with your career, or did Kelsey just get particularly lucky? After all, journalism is a shrinking industry without an obvious revenue model to fund many writers looking into the world's most pressing problems.<br><br>

Kelsey points out that one needn't take the risk of committing to journalism at an early age. Instead listeners can specialise in an important topic, while leaving open the option of switching into specialist journalism later on, should a great opportunity happen to present itself.<br><br>

In today’s episode we discuss that path, as well as:<br><br>

• What’s the day to day life of a Vox journalist like?<br>
• How can good journalism get funded?<br>
• Are there meaningful tradeoffs between doing what's in the interest of Vox and doing what’s good?<br>
• How concerned should we be about the risk of effective altruism being perceived as partisan?<br>
• How well can short articles effectively communicate complicated ideas?<br>
• Are there alternative business models that could fund high quality journalism on a larger scale?<br>
• How do you approach the case for taking AI seriously to a broader audience?<br>
• How valuable might it be for media outlets to do Tetlock-style forecasting?<br>
• Is it really a good idea to heavily tax billionaires?<br>
• How do you avoid the pressure to get clicks?<br>
• How possible is it to predict which articles are going to be popular?<br>
• How did Kelsey build the skills necessary to work at Vox?<br>
• General lessons for people dealing with very difficult life circumstances<br><br>

Rob is then joined by two of his colleagues – Keiran Harris &amp; Michelle Hutchinson – to quickly discuss:<br><br>

• The risk political polarisation poses to long-termist causes<br>
• How should specialists keep journalism available as a career option?<br>
• Should we create a news aggregator that aims to make someone as well informed as possible in big-picture terms?<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em><p></p>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/1e0ca428-3a48-11e9-8633-0eccce65e6e8/images/main.jpg?1601340454319,http://feedproxy.google.com/~r/80000HoursPodcast/~5/U1-DrjasAUQ/53---kelsey-piper-on-the-room-for-important-advocacy-within-journalism-mp3-128-kbps.mp3,False
51,Julia Galef and Rob Wiblin on an updated view of the best ways to help humanity,"<p>This is a cross-post of an interview Rob did with Julia Galef on her podcast <a href=""http://rationallyspeakingpodcast.org/"" target=""_blank""><b>Rationally Speaking</b></a>. Rob and Julia discuss how the career advice 80,000 Hours gives has changed over the years, and the biggest misconceptions about our views.<br><br>

The topics will be familiar to the most fervent fans of this show&nbsp;— but we think that if you’ve listened to less than about half of the episodes we've released so far, you’ll find something new to enjoy here. Julia may be familiar to you as the guest on episode 7 of the show, way back in September 2017.<br><br>

The conversation also covers topics like:<br><br>

• How many people should try to get a job in finance and donate their income?<br>
• The case for working to reduce global catastrophic risks in targeted ways, and historical precedents for this kind of work<br>
• Why reducing risk is a better way to help the future than increasing economic growth<br>
• What percentage of the world should ideally follow 80,000 Hours advice?<br><br>

<a href=""https://80k.link/julia-galef-rationally-speaking""><b>Links to learn more, summary and full transcript.</b></a><br><br>

If you’re interested in the cooling and expansion of the universe, which comes up on the show, you should definitely check out our <a href=""https://80000hours.org/podcast/episodes/anders-sandberg-fermi-paradox/?utm_campaign=podcast__julia-galef-2&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast""><b>29th episode with Dr Anders Sandberg.</b></a><br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into any podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/9cf25ebe-3236-11e9-8b8a-0ee37b0b018a/images/main.jpg?1601340454419,http://feedproxy.google.com/~r/80000HoursPodcast/~5/ToqoRO6njsM/julia-galef-and-rob-wiblin-on-an-updated-view-of-the-best-ways-to-help-humanity-mp3-128-kbps.mp3,False
52,#52 - Prof Glen Weyl on uprooting capitalism and democracy for a just society,"<p>Pro-market economists love to wax rhapsodic about the capacity of markets to pull together the valuable local information spread across all of society about what people want and how to make it.<br><br>

But when it comes to politics and voting - which also aim to aggregate the preferences and knowledge found in millions of individuals - the enthusiasm for finding clever institutional designs often turns to skepticism.<br><br>

Today's guest, freewheeling economist Glen Weyl, won't have it, and is on a warpath to reform liberal democratic institutions in order to save them. Just last year he wrote <em>Radical Markets: Uprooting Capitalism and Democracy for a Just Society</em> with Eric Posner, but has already moved on, saying <i>""in the 6 months since the book came out I've made more intellectual progress than in the whole 10 years before that.""</i><br><br>

Weyl believes we desperately need more efficient, equitable and decentralised ways to organise society, that take advantage of what each person knows, and his research agenda has already been making breakthroughs.<br><br>

<a href=""https://80k.link/glen-weyl-episode""><b>Links to learn more, summary and full transcript</b></a><br><br>

<a href=""https://80k.link/glen-weyl-job-board""><b>Our high impact job board</b></a><br><br>

<a href=""https://80k.link/glen-weyl-newsletter"" target=""_blank""><b>Join our newsletter</b></a><br><br>

Despite a history in the best economics departments in the world - Harvard, Princeton, Yale and the University of Chicago - he is too worried for the future to sit in his office writing papers. Instead he has left the academy to try to inspire a social movement, <a href=""https://80k.link/glen-weyl-radicalxchange""><b>RadicalxChange</b></a>, with a vision of social reform as expansive as his own.<br><br> 

<a href=""https://80k.link/glen-weyl-radicalxchange""><b>You can sign up for their conference in Detroit in March here</b></a><br><br>

Economist Alex Tabarrok called his latest proposal, known as 'liberal radicalism', <i>""a quantum leap in public-goods mechanism-design""</i> - we explain how it works in the show. But the proposal, however good in theory, might struggle in the real world because it requires large subsidies, and compensates for people's selfishness so effectively that it might even be an overcorrection.<br><br>

An earlier mechanism - 'quadratic voting' (QV) - would allow people to express the relative strength of their preferences in the democratic process. No longer would 51 people who support a proposal, but barely care about the issue, outvote 49 incredibly passionate opponents, predictably making society worse in the process. We explain exactly how in the episode.<br><br>

Weyl points to studies showing that people are more likely to vote strongly not only about issues they *care* more about, but issues they *know* more about. He expects that allowing people to specialise and indicate when they know what they're talking about will create a democracy that does more to aggregate careful judgement, rather than just passionate ignorance.<br><br>

But these and indeed all of Weyl's ideas have faced criticism. Some say the risk of unintended consequences is too great, or that they solve the wrong problem. Others see these proposals as unproven, impractical, or just another example of an intellectual engaged in grand social planning. I raise these concerns to see how he responds.<br><br>

As big a topic as all of that is, this extended conversation also goes into the blockchain, problems with the effective altruism community and how auctions could replace private property. Don't miss it.<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em></p>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/6892db34-2b34-11e9-bee7-0ece7a7d2472/images/main.jpg?1601340454522,http://feedproxy.google.com/~r/80000HoursPodcast/~5/jHzzN6UQa4o/52---glen-weyl-on-radical-institutional-reform-of-capitalism-and-democracy-mp3-128-kbps-2.mp3,False
53,#10 Classic episode - Dr Nick Beckstead on spending billions of dollars preventing human extinction,"<b>Rebroadcast: this episode was originally released in October 2017.</b><br><br>

What if you were in a position to give away billions of dollars to improve the world? What would you do with it? This is the problem facing Program Officers at the Open Philanthropy Project - people like Dr Nick Beckstead.<br><br>

Following a PhD in philosophy, Nick works to figure out where money can do the most good. He’s been involved in major grants in a wide range of areas, including ending factory farming through technological innovation, safeguarding the world from advances in biotechnology and artificial intelligence, and spreading rational compassion.<br><br>

<a href=""https://80000hours.org/podcast/episodes/nick-beckstead-giving-billions/?utm_campaign=podcast__nick-beckstead&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Links to learn more, episode summary & full transcript</b></a><br><br>

<a href=""https://80000hours.org/articles/high-impact-careers/?utm_campaign=podcast__nick-beckstead&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>These are the world’s highest impact career paths according to our research</b></a><br><br>

<a href=""https://80000hours.org/articles/extinction-risk/?utm_campaign=podcast__nick-beckstead&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Why despite global progress, humanity is probably facing its most dangerous time ever</b></a><br><br>

This episode is a tour through some of the toughest questions ‘effective altruists’ face when figuring out how to best improve the world, including:<br><br>

* Should we mostly try to help people currently alive, or future generations? Nick studied this question for years in his PhD thesis, <a href=""https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxuYmVja3N0ZWFkfGd4OjExNDBjZTcwNjMxMzRmZGE"">On the Overwhelming Importance of Shaping the Far Future</a>. (The first 31 minutes of this episode is a snappier version of <a href=""https://80k.link/toby-ord-from-nick"">my conversation with Toby Ord</a>.)<br>
* Is clean meat (aka *in vitro* meat) technologically feasible any time soon, or should we be looking for plant-based alternatives?<br>
* What are the greatest risks to human civilisation?<br>
* To stop malaria is it more cost-effective to use technology to <a href=""https://www.openphilanthropy.org/focus/scientific-research/miscellaneous/target-malaria-general-support"">eliminate mosquitos</a> than to distribute bed nets?<br>
* Should people who want to improve the future work for changes that will be very useful in a specific scenario, or just generally try to improve how well humanity makes decisions?<br>
* What specific jobs should our listeners take in order for Nick to be able to spend more money in useful ways to improve the world?<br>
* Should we expect the future to be better if the economy grows more quickly - or more slowly?<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/9d386422-2a3e-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340454626,http://feedproxy.google.com/~r/80000HoursPodcast/~5/rMQmI9dJlFY/568721673-80000-hours-classic-nick-beckstead-preventing-human-extinction.mp3,False
54,#51 - Martin Gurri on the revolt of the public & crisis of authority in the information age,"Politics in rich countries seems to be going nuts. What's the explanation? Rising inequality? The decline of manufacturing jobs? Excessive immigration?<br><br>

Martin Gurri spent decades as a CIA analyst and in his 2014 book <a href=""https://www.amazon.com/Revolt-Public-Crisis-Authority-Millennium/dp/1732265143""><em>The Revolt of The Public and Crisis of Authority in the New Millennium</em></a>, predicted political turbulence for an entirely different reason: new communication technologies were flipping the balance of power between the public and traditional authorities.<br><br>

In 1959 the President could control the narrative by leaning on his friends at four TV stations, who felt it was proper to present the nation's leader in a positive light, no matter their flaws. Today, it's impossible to prevent someone from broadcasting any grievance online, whether it's a contrarian insight or an insane conspiracy theory.<br><br>

<a href=""https://80000hours.org/podcast/episodes/martin-gurri-revolt-of-the-public/?utm_campaign=podcast__martin-gurri&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Links to learn more, summary and full transcript.</b></a><br><br>

According to Gurri, trust in society's institutions - police, journalists, scientists and more - has been undermined by constant criticism from outsiders, and exposed to a cacophony of conflicting opinions on every issue, the public takes fewer truths for granted. We are now free to see our leaders as the flawed human beings they always have been, and are not amused.<br><br>

Suspicious they are being betrayed by elites, the public can also use technology to coordinate spontaneously and express its anger. Keen to 'throw the bastards out' protesters take to the streets, united by what they don't like, but without a shared agenda or the institutional infrastructure to figure out how to fix things. Some popular movements have come to view any attempt to exercise power over others as suspect. <br><br>

If Gurri is to be believed, protest movements in Egypt, Spain, Greece and Israel in 2011 followed this script, while Brexit, Trump and the French yellow vests movement subsequently vindicated his theory.<br><br>

In this model, politics won't return to its old equilibrium any time soon. The leaders of tomorrow will need a new message and style if they hope to maintain any legitimacy in this less hierarchical world. Otherwise, we're in for decades of grinding conflict between traditional centres of authority and the general public, who doubt both their loyalty and competence. <br><br>

But how much should we believe this theory? Why do Canada and Australia remain pools of calm in the storm? Aren't some malcontents quite concrete in their demands? And are protest movements actually more common (or more nihilistic) than they were decades ago?<br><br>

In today's episode we ask these questions and add an hour-long discussion with two of Rob's colleagues - Keiran Harris and Michelle Hutchinson - to further explore the ideas in the book.<br><br>

The conversation covers:<br><br>

* How do we know that the internet is driving this rather than some other phenomenon?<br>
* How do technological changes enable social and political change?<br>
* The historical role of television<br>
* Are people also more disillusioned now with sports heroes and actors?<br>
* Which countries are finding good ways to make politics work in this new era?<br>
* What are the implications for the threat of totalitarianism?<br>
* What is this is going to do to international relations? Will it make it harder for countries to cooperate and avoid conflict?<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/816afb74-2a3e-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340454712,http://feedproxy.google.com/~r/80000HoursPodcast/~5/t1jAJOO9w0A/566341518-80000-hours-martin-gurri-revolt-of-the-public.mp3,False
55,#8 Classic episode - Lewis Bollard on how to end factory farming in our lifetimes,"<b>Rebroadcast: this episode was originally released in September 2017.</b><br><br>

Every year tens of billions of animals are raised in terrible conditions in factory farms before being killed for human consumption. Over the last two years Lewis Bollard – Project Officer for Farm Animal Welfare at the Open Philanthropy Project – has conducted extensive research into the best ways to eliminate animal suffering in farms as soon as possible. This has resulted in $30 million in grants to farm animal advocacy.<br><br>

<a href=""https://80000hours.org/2017/09/lewis-bollard-end-factory-farming/?utm_campaign=podcast__lewis-bollard&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Links to learn more, episode summary & full transcript</b></a><br><br>

<a href=""https://80000hours.org/job-board/factory-farming/?utm_campaign=podcast__lewis-bollard&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Jobs focussed on ending factory farming</a><br><br>

<a href=""https://80000hours.org/problem-profiles/factory-farming/"">Problem profile: factory farming</a><br><br>

We covered almost every approach being taken, which ones work, and how individuals can best contribute through their careers.<br><br>

We also had time to venture into a wide range of issues that are less often discussed, including:<br><br>

* Why Lewis thinks insect farming would be worse than the status quo, and whether we should look for ‘humane’ insecticides;<br>
* How young people can set themselves up to contribute to scientific research into meat alternatives;<br>
* How genetic manipulation of chickens has caused them to suffer much more than their ancestors, but could also be used to make them better off;<br>
* Why Lewis is skeptical of vegan advocacy;<br>
* Why he doubts that much can be done to tackle factory farming through legal advocacy or electoral politics;<br>
* Which species of farm animals is best to focus on first;<br>
* Whether fish and crustaceans are conscious, and if so what can be done for them;<br>
* And many others<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/5f1a3d8c-2a3e-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340454798,http://feedproxy.google.com/~r/80000HoursPodcast/~5/usj7PiT_UOE/560055180-80000-hours-classic-lewis-bollard-ending-factory-farming.mp3,False
56,#9 Classic episode - Christine Peterson on the '80s futurist movement & its lessons for today,"<b>Rebroadcast: this episode was originally released in October 2017.</b><br><br>

Take a trip to Silicon Valley in the 70s and 80s, when going to space sounded like a good way to get around environmental limits, people started cryogenically freezing themselves, and nanotechnology looked like it might revolutionise industry – or turn us all into grey goo.<br><br>

In this episode of the 80,000 Hours Podcast, Christine Peterson takes us back to her youth in the Bay Area, the ideas she encountered there, and what the dreamers she met did as they grew up.<br><br>

<a href=""https://80000hours.org/podcast/episodes/christine-peterson-computer-security/?utm_campaign=podcast__christine-peterson&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Links to learn more, episode summary & full transcript</b></a><br><br>

Today Christine helps runs the Foresight Institute, which fills a gap left by for-profit technology companies – predicting how new revolutionary technologies could go wrong, and ensuring we steer clear of the downsides.<br><br>

We also explore:<br><br>

* Whether the poor security of computer systems poses a catastrophic risk for the world. Could all our essential services be taken down at once? And if so, what can be done about it?<br>
* Can technology ‘move fast and break things’ without eventually breaking the world? Would it be better for technology to advance more quickly, or more slowly?<br>
* Will AIs designed for wide-scale automated hacking make computers more or less secure?<br>
* Would it be good to radically extend human lifespan? Is it sensible to cryogenically freeze yourself in the hope of being resurrected in the future?<br>
* Could atomically precise manufacturing (nanotechnology) really work? Why was it initially so controversial and why did people stop worrying about it?<br>
* Should people who try to do good in their careers work long hours and take low salaries? Or should they take care of themselves first of all?<br>
* How she thinks the the effective altruism community resembles the scene she was involved with when she was wrong, and where it might be going wrong.<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/49ca2906-2a3e-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340454887,http://feedproxy.google.com/~r/80000HoursPodcast/~5/HIezImEuDyY/555093495-80000-hours-classic-christine-peterson-insecure-computers.mp3,False
57,#50 - Dr David Denkenberger on how to feed all 8b people through an asteroid/nuclear winter,"If an asteroid impact or nuclear winter blocked the sun for years, our inability to grow food would result in billions dying of starvation, right? According to Dr David Denkenberger, co-author of <em><a href=""https://en.wikipedia.org/wiki/Feeding_Everyone_No_Matter_What"">Feeding Everyone No Matter What</a></em>: no. If he's to be believed, nobody need starve at all.<br><br>

Even without the sun, David sees the Earth as a bountiful food source. Mushrooms farmed on decaying wood. Bacteria fed with natural gas. Fish and mussels supported by sudden upwelling of ocean nutrients - and more.<br><br>

Dr Denkenberger is an Assistant Professor at the University of Alaska Fairbanks, and he's out to spread the word that while a nuclear winter might be horrible, experts have been mistaken to assume that mass starvation is an inevitability. In fact, the only thing that would prevent us from feeding the world is insufficient preparation.<br><br>

∙ <a href=""https://80000hours.org/podcast/episodes/david-denkenberger-allfed-and-feeding-everyone-no-matter-what/?utm_campaign=podcast__david-denkenberger&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Links to learn more, summary and full transcript</b></a><br><br>

Not content to just write a book pointing this out, David has gone on to found a growing non-profit - the <em><a href=""http://allfed.info/"">Alliance to Feed the Earth in Disasters (ALLFED)</a></em> - to prepare the world to feed everyone come what may. He expects that today 10% of people would find enough food to survive a massive disaster. In principle, if we did everything right, nobody need go hungry. But being more realistic about how much we're likely to invest, David thinks a plan to inform people ahead of time could save 30%, and a decent research and development scheme 80%.<br><br>

∙ 80,000 Hours' updated article on <a href=""https://80k.link/best-charity-david""><b>How to find the best charity to give to</b></a><br>
∙ A potential donor <a href=""https://forum.effectivealtruism.org/posts/SYeJnv9vYzq9oQMbQ/2017-donor-lottery-report""><b>evaluates ALLFED</b></a><br><br>

According to David's <a href=""https://link.springer.com/article/10.1007/s13753-016-0097-2"">published cost-benefit analyses</a>, work on this problem may be able to save lives, in expectation, for under $100 each, making it an incredible investment. <br><br>

These preparations could also help make humanity more resilient to global catastrophic risks, by forestalling an ‘everyone for themselves' mentality, which then causes trade and civilization to unravel.<br><br>

But some worry that David's cost-effectiveness estimates are exaggerations, so I challenge him on the practicality of his approach, and how much his non-profit's work would actually matter in a post-apocalyptic world. In our extensive conversation, we cover:<br><br>

* How could the sun end up getting blocked, or agriculture otherwise be decimated?<br>
* What are all the ways we could we eat nonetheless? What kind of life would this be?<br>
* Can these methods be scaled up fast?<br>
* What is his organisation, ALLFED, actually working on?<br>
* How does he estimate the cost-effectiveness of this work, and what are the biggest weaknesses of the approach?<br>
* How would more food affect the post-apocalyptic world? Won't people figure it out at that point anyway?<br>
* Why not just leave guidebooks with this information in every city?<br>
* Would these preparations make nuclear war more likely?<br>
* What kind of people is ALLFED trying to hire?<br>
* What would ALLFED do with more money?<br>
* How he ended up doing this work. And his other engineering proposals for improving the world, including ideas to prevent a supervolcano explosion.<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/17874a14-2a3e-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340454974,http://feedproxy.google.com/~r/80000HoursPodcast/~5/rO-m9yqp0us/50---dr-david-denkenberger-on-how-to-feed-all-8b-people-through-an-asteroid-or-nuclear-winter.mp3,False
58,#49 - Dr Rachel Glennerster on a year's worth of education for 30c & other development 'best buys',"If I told you it's possible to deliver an extra year of ideal primary-level education for under $1, would you believe me? Hopefully not - the claim is absurd on its face.<br><br>

But it may be true nonetheless. The very best education interventions are phenomenally cost-effective, and they're not the kinds of things you'd expect, says Dr Rachel Glennerster.<br><br>

She's Chief Economist at the UK's foreign aid agency DFID, and used to run J-PAL, the world-famous anti-poverty research centre based in MIT's Economics Department, where she studied the impact of a wide range of approaches to improving education, health, and governing institutions. According to Dr Glennerster:<br><br>

<em>""...when we looked at the cost effectiveness of education programs, there were a ton of zeros, and there were a ton of zeros on the things that we spend most of our money on. So more teachers, more books, more inputs, like smaller class sizes - at least in the developing world - seem to have no impact, and that's where most government money gets spent.""<br><br>

""But measurements for the top ones - the most cost effective programs - say they deliver 460 LAYS per £100 spent ($US130). LAYS are Learning-Adjusted Years of Schooling. Each one is the equivalent of the best possible year of education you can have - Singapore-level.""</em><br><br>

<a href=""https://80k.link/rachel-glennerster""><b>Links to learn more, summary and full transcript.</b></a><br><br>

<em>""...the two programs that come out as spectacularly effective... well, the first is just rearranging kids in a class.""<br><br>

""You have to test the kids, so that you can put the kids who are performing at grade two level in the grade two class, and the kids who are performing at grade four level in the grade four class, even if they're different ages - and they learn so much better. So that's why it's so phenomenally cost effective because, it really doesn't cost anything.""<br><br>

""The other one is providing information. So sending information over the phone [for example about how much more people earn if they do well in school and graduate]. So these really small nudges. Now none of those nudges will individually transform any kid's life, but they are so cheap that you get these fantastic returns on investment - and we do very little of that kind of thing.""</em><br><br>

In this episode, Dr Glennerster shares her decades of accumulated wisdom on which anti-poverty programs are overrated, which are neglected opportunities, and how we can know the difference, across a range of fields including health, empowering women and macroeconomic policy.<br><br>

Regular listeners will be wondering - have we forgotten all about the lessons from <a href=""https://80k.link/rachel-to-eva-episode"">episode 30 of the show with Dr Eva Vivalt</a>? She threw several buckets of cold water on the hope that we could accurately measure the effectiveness of social programs at all.<br><br>

According to Vivalt, her dataset of hundreds of randomised controlled trials indicates that social science findings don’t generalize well at all. The results of a trial at a school in Namibia tell us remarkably little about how a similar program will perform if delivered at another school in Namibia - let alone if it's attempted in India instead.<br><br>

Rachel offers a different and more optimistic interpretation of Eva's findings. To learn more and figure out who you sympathise with more, you'll just have to listen to the episode.<br><br>

Regardless, Vivalt and Glennerster agree that we should continue to run these kinds of studies, and today’s episode delves into the latest ideas in global health and development.<br><br>

<b>Get this episode by subscribing: type '80,000 Hours' into your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/ff66eb38-2a3d-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340455078,http://feedproxy.google.com/~r/80000HoursPodcast/~5/9KMaX092v3Q/547588083-80000-hours-rachel-glennerster-best-buys-in-international-development.mp3,False
59,#6 Classic episode - Dr Toby Ord on why the long-term future matters more than anything else,"<b>Rebroadcast: this episode was originally released in September 2017.</b><br><br>

Of all the people whose well-being we should care about, only a small fraction are alive today. The rest are members of future generations who are yet to exist. Whether they’ll be born into a world that is flourishing or disintegrating – and indeed, whether they will ever be born at all – is in large part up to us. As such, the welfare of future generations should be our number one moral concern.<br><br>

This conclusion holds true regardless of whether your moral framework is based on common sense, consequences, rules of ethical conduct, cooperating with others, virtuousness, keeping options open – or just a sense of wonder about the universe we find ourselves in.<br><br>

That’s the view of Dr Toby Ord, a philosophy Fellow at the University of Oxford and co-founder of the effective altruism community. In this episode Dr Ord makes the case that aiming for a positive long-term future is likely the best way to improve the world.<br><br>

<b><a href=""https://80000hours.org/podcast/episodes/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/?utm_campaign=podcast__toby-ord&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Links to learn more, episode summary & full transcript</a><br>
<a href=""https://80000hours.org/articles/future-generations/?utm_campaign=podcast__toby-ord&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Presenting the long-term value thesis</a> by Ben Todd<br>
<a href=""https://80000hours.org/articles/extinction-risk/?utm_campaign=podcast__toby-ord&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Why despite global progress, humanity is probably facing its most dangerous time ever</a> by Ben Todd
<br><a href=""https://www.youtube.com/watch?v=qV83HnxONQk"">Making sense of long-term indirect effects</a> by Rob Wiblin</b><br><br>

We then discuss common objections to long-termism, such as the idea that benefits to future generations are less valuable than those to people alive now, or that we can’t meaningfully benefit future generations beyond taking the usual steps to improve the present.<br><br>

Later the conversation turns to how individuals can and have changed the course of history, what could go wrong and why, and whether plans to colonise Mars would actually put humanity in a safer position than it is today.<br><br>

This episode goes deep into the most distinctive features of our advice. It’s likely the most in-depth discussion of how 80,000 Hours and the effective altruism community think about the long term future and why - and why we so often give it top priority. <br><br>

<b>We cover:</b><br><br>

* Why is the long-term future of humanity such a big deal, and perhaps the most important issue for us to be thinking about?<br>
* Five arguments that future generations matter<br>
* How bad would it be if humanity went extinct or civilization collapses? <br>
* Why do people start saying such strange things when this topic comes up?<br>
* Are there any other reasons to prioritize thinking about the long-term future of humanity that you wanted to raise before we move to objections?<br>
* What is this school of thought properly called?<br>

And much more.<br><br>

<b>Get this episode by subscribing: type '80,000 Hours' into your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/e5e60554-2a3d-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340455168,http://feedproxy.google.com/~r/80000HoursPodcast/~5/BFjNeTMrODk/544756371-80000-hours-classic-toby-ord-long-term-future.mp3,False
60,#15 Classic episode - Prof Tetlock on chimps beating Berkeley undergrads & when to defer to the wise,"<b>Rebroadcast: this episode was originally released in November 2017.</b><br><br>

Prof Philip Tetlock is a social science legend. Over forty years he has researched whose predictions we can trust, whose we can’t and why - and developed methods that allow all of us to be better at predicting the future.<br><br>

After the Iraq WMDs fiasco, the US intelligence services hired him to figure out how to ensure they’d never screw up that badly again. The result of that work – <em>Superforecasting</em> – was a media sensation in 2015.<br><br>

<a href=""https://80000hours.org/podcast/episodes/prof-tetlock-predicting-the-future/?utm_campaign=podcast__phil-tetlock&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Links to learn more, summary and full transcript</b></a><br><br>

It described Tetlock’s Good Judgement Project, which found forecasting methods so accurate they beat everyone else in open competition, including thousands of people in the intelligence services with access to classified information.<br><br>

Today he’s working to develop the best forecasting process ever, by combining top human and machine intelligence in the <a href=""https://www.hybridforecasting.com/"">Hybrid Forecasting Competition</a>, which you can sign up and participate in.<br><br>

We start by describing his key findings, and then push to the edge of what is known about how to foresee the unforeseeable:<br><br>

* Should people who want to be right just adopt the views of experts rather than apply their own judgement?<br>
* Why are Berkeley undergrads worse forecasters than dart-throwing chimps?<br>
* Should I keep my political views secret, so it will be easier to change them later?<br>
* How can listeners contribute to his latest cutting-edge research?<br>
* What do we know about our accuracy at predicting low-probability high-impact disasters?<br>
* Does his research provide an intellectual basis for populist political movements?<br>
* Was the Iraq War caused by bad politics, or bad intelligence methods?<br>
* What can we learn about forecasting from the 2016 election?<br>
* Can experience help people avoid overconfidence and underconfidence?<br>
* When does an AI easily beat human judgement?<br>
* Could more accurate forecasting methods make the world more dangerous?<br>
* How much does demographic diversity line up with cognitive diversity?<br>
* What are the odds we’ll go to war with China?<br>
* Should we let prediction tournaments run most of the government?<br><br>

<a href=""https://80000hours.org/problem-profiles/improving-institutional-decision-making/?utm_campaign=podcast__phil-tetlock&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Read our problem profile on improving institutional decision-making here</b></a><br><br>

<b>Get this episode by subscribing: type '80,000 Hours' into your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/cfaa07fe-2a3d-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340455262,http://feedproxy.google.com/~r/80000HoursPodcast/~5/fizrcOoafFQ/540944013-80000-hours-classic-prof-tetlock-predicting-the-future.mp3,False
61,#48 - Brian Christian on better living through the wisdom of computer science,"Please let us know if we've helped you: <a href=""https://80k.link/survey-christian""><b>Fill out our annual impact survey</b></a><br><br>

Ever felt that you were so busy you spent all your time paralysed trying to figure out where to start, and couldn't get much done? Computer scientists have a term for this - <em>thrashing</em> - and it's a common reason our computers freeze up. The solution, for people as well as laptops, is to 'work dumber': pick something at random and finish it, without wasting time thinking about the bigger picture.<br><br>

Bestselling author Brian Christian studied computer science, and in the book <b>Algorithms to Live By</b> he's out to find the lessons it can offer for a better life. He investigates into when to quit your job, when to marry, the best way to sell your house, how long to spend on a difficult decision, and how much randomness to inject into your life. In each case computer science gives us a theoretically optimal solution, and in this episode we think hard about whether its models match our reality. <br><br>

<a href=""https://80k.link/brian-podcast"" rel=""nofollow"" target=""_blank""><b>Links to learn more, summary and full transcript.</b></a><br><br>

One genre of problems Brian explores in his book are 'optimal stopping problems', the canonical example of which is ‘the secretary problem’. Imagine you're hiring a secretary, you receive *n* applicants, they show up in a random order, and you interview them one after another. You either have to hire that person on the spot and dismiss everybody else, or send them away and lose the option to hire them in future.<br><br>

It turns out most of life can be viewed this way - a series of unique opportunities you pass by that will never be available in exactly the same way again.<br><br>

So how do you attempt to hire the very best candidate in the pool? There's a risk that you stop before finding the best, and a risk that you set your standards too high and let the best candidate pass you by.<br><br>

Mathematicians of the mid-twentieth century produced an elegant optimal approach: spend exactly one over *e*, or approximately 37% of your search, just establishing a baseline without hiring anyone, no matter how promising they seem. Then immediately hire the next person who's better than anyone you've seen so far.<br><br>

It turns out that your odds of success in this scenario are also 37%. And the optimal strategy and the odds of success are identical regardless of the size of the pool. So as *n* goes to infinity you still want to follow this 37% rule, and you still have a 37% chance of success. Even if you interview a million people.<br><br>

But if you have the option to go back, say by apologising to the first applicant and begging them to come work with you, and you have a 50% chance of your apology being accepted, then the optimal explore percentage rises all the way to 61%. <br><br>

Today’s episode focuses on Brian’s book-length exploration of how insights from computer algorithms can and can't be applied to our everyday lives. We cover:<br><br>

* Computational kindness, and the best way to schedule meetings<br>
* How can we characterize a computational model of what people are actually doing, and is there a rigorous way to analyse just how good their instincts actually are?<br>
* What’s it like being a human confederate in the Turing test competition?<br>
* Is trying to detect fake social media accounts a losing battle?<br>
* The canonical explore/exploit problem in computer science:  the multi-armed bandit<br>
* What’s the optimal way to buy or sell a house?<br>
* Why is information economics so important?<br>
* What kind of decisions should people randomize more in life?<br>
* How much time should we spend on prioritisation?<br><br>

<b>Get this episode by subscribing: type '80,000 Hours' into your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/acb4830a-2a3d-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340455349,http://feedproxy.google.com/~r/80000HoursPodcast/~5/IPgEqJQwYEA/533741832-80000-hours-brian-christian-algorithms-to-live-by.mp3,False
62,#47 - Catherine Olsson & Daniel Ziegler on the fast path into high-impact ML engineering roles,"After dropping out of a machine learning PhD at Stanford, Daniel Ziegler needed to decide what to do next. He’d always enjoyed building stuff and wanted to shape the development of AI, so he thought a research engineering position at an org dedicated to aligning AI with human interests could be his best option.<br><br>

He decided to apply to OpenAI, and spent about 6 weeks preparing for the interview before landing the job. His PhD, by contrast, might have taken 6 years. Daniel thinks this highly accelerated career path may be possible for many others.<br><br>

On today’s episode Daniel is joined by Catherine Olsson, who has also worked at OpenAI, and left her computational neuroscience PhD to become a research engineer at Google Brain. She and Daniel share this piece of advice for those curious about this career path: just dive in. If you're trying to get good at something, just start doing that thing, and figure out that way what's necessary to be able to do it well.<br><br>

Catherine has even <a href=""https://80k.link/ml-engineering-career-transition-guide"" rel=""nofollow"" target=""_blank"">created a simple step-by-step guide</a> for 80,000 Hours, to make it as easy as possible for others to copy her and Daniel's success.<br><br>

<em>Please let us know how we've helped you: fill out <a href=""https://80k.link/survey-c-and-z""><b>our 2018 annual impact survey</b></a> so that 80,000 Hours can continue to operate and grow.</em><br><br>

<a href=""https://80k.link/olsson-and-ziegler-ml-engineering-and-safety"" rel=""nofollow"" target=""_blank""><b>Blog post with links to learn more, a summary & full transcript.</b></a><br><br>

Daniel thinks the key for him was nailing the job interview.<br><br>

OpenAI needed him to be able to demonstrate the ability to do the kind of stuff he'd be working on day-to-day. So his approach was to take a list of 50 key deep reinforcement learning papers, read one or two a day, and pick a handful to actually reproduce. He spent a bunch of time coding in Python and TensorFlow, sometimes 12 hours a day, trying to debug and tune things until they were actually working.<br><br>

Daniel emphasizes that the most important thing was to practice *exactly* those things that he knew he needed to be able to do. His dedicated preparation also led to an offer from the Machine Intelligence Research Institute, and so he had the opportunity to decide between two organisations focused on the global problem that most concerns him.<br><br> 

Daniel’s path might seem unusual, but both he and Catherine expect it can be replicated by others. If they're right, it could greatly increase our ability to get new people into important ML roles in which they can make a difference, as quickly as possible.<br><br>

Catherine says that her move from OpenAI to an ML research team at Google now allows her to bring a different set of skills to the table. Technical AI safety is a multifaceted area of research, and the many sub-questions in areas such as reward learning, robustness, and interpretability all need to be answered to maximize the probability that AI development goes well for humanity.<br><br> 

Today’s episode combines the expertise of two pioneers and is a key resource for anyone wanting to follow in their footsteps. We cover:<br><br>

* What are OpenAI and Google Brain doing?<br> 
* Why work on AI?<br>
* Do you learn more on the job, or while doing a PhD?<br>
* Controversial issues within ML<br>
* Is replicating papers a good way of determining suitability?<br>
* What % of software developers could make similar transitions?<br>
* How in-demand are research engineers?<br>
* The development of Dota 2 bots<br>
* Do research scientists have more influence on the vision of an org?<br>
* Has learning more made you more or less worried about the future?<br><br>

<b>Get this episode by subscribing: type '80,000 Hours' into your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/92db844c-2a3d-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340455438,http://feedproxy.google.com/~r/80000HoursPodcast/~5/rolkve8V9K0/523583478-80000-hours-olsson-and-ziegler-ml-engineering-and-safety.mp3,False
63,#46 - Prof Hilary Greaves on moral cluelessness & tackling crucial questions in academia,"The barista gives you your coffee and change, and you walk away from the busy line. But you suddenly realise she gave you $1 less than she should have. Do you brush your way past the people now waiting, or just accept this as a dollar you’re never getting back? According to philosophy Professor Hilary Greaves - Director of Oxford University's <a href=""https://globalprioritiesinstitute.org"">Global Priorities Institute</a>, which <a href=""https://globalprioritiesinstitute.org/opportunities/"">is hiring</a> - this simple decision will completely change the long-term future by altering the identities of almost all future generations.<br><br>

How? Because by rushing back to the counter, you slightly change the timing of everything else people in line do during that day - including changing the timing of the interactions they have with everyone else. Eventually these causal links will reach someone who was going to conceive a child.<br><br>

By causing a child to be conceived a few fractions of a second earlier or later, you change the sperm that fertilizes their egg, resulting in a totally different person. So asking for that $1 has now made the difference between all the things that this actual child will do in their life, and all the things that the merely possible child - who didn't exist because of what you did - would have done if you decided not to worry about it.<br><br>

As that child's actions ripple out to everyone else who conceives down the generations, ultimately the entire human population will become different, all for the sake of your dollar. Will your choice cause a future Hitler to be born, or not to be born? Probably both!<br><br>

<a href=""https://80000hours.org/podcast/episodes/hilary-greaves-global-priorities-institute/?utm_campaign=podcast__hilary-greaves&utm_source=80000+Hours+Podcast&utm_medium=podcast"" rel=""nofollow"" target=""_blank""><b>Links to learn more, summary and full transcript.</b></a><br><br>

Some find this concerning. The actual long term effects of your decisions are so unpredictable, it looks like you’re totally clueless about what's going to lead to the best outcomes. It might lead to decision paralysis - you won’t be able to take any action at all.<br><br>

Prof Greaves doesn’t share this concern for most real life decisions. If there’s no reasonable way to assign probabilities to far-future outcomes, then the possibility that you might make things better in completely unpredictable ways is more or less canceled out by equally likely opposite possibility.<br><br>

But, if instead we’re talking about a decision that involves highly-structured, systematic reasons for thinking there might be a general tendency of your action to make things better or worse -- for example if we increase economic growth -- Prof Greaves says that we don’t get to just ignore the unforeseeable effects. <br><br>

When there are complex arguments on both sides, it's unclear what probabilities you should assign to this or that claim. Yet, given its importance, whether you should take the action in question actually does depend on figuring out these numbers. So, what do we do?<br><br>

Today’s episode blends philosophy with an exploration of the mission and research agenda of the Global Priorities Institute: to develop the effective altruism movement within academia. We cover:<br><br>

* How controversial is the multiverse interpretation of quantum physics?<br />
* Given moral uncertainty, how should population ethics affect our real life decisions?<br />
* How should we think about archetypal decision theory problems?<br />
* What are the consequences of cluelessness for those who based their donation advice on GiveWell style recommendations?<br />
* How could reducing extinction risk be a good cause for risk-averse people?<br><br>

<b>Get this episode by subscribing: type '80,000 Hours' into your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/73697ce0-2a3d-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340455526,http://feedproxy.google.com/~r/80000HoursPodcast/~5/fTZP6hBew1Q/518454945-80000-hours-hilary-greaves-global-priorities-institute.mp3,False
64,"#45 - Tyler Cowen's case for maximising econ growth, stabilising civilization & thinking long-term","I've probably spent more time reading Tyler Cowen - Professor of Economics at George Mason University - than any other author. Indeed it's his incredibly popular blog <a href=""https://marginalrevolution.com"">Marginal Revolution</a> that prompted me to study economics in the first place. Having spent thousands of hours absorbing Tyler's work, it was a pleasure to be able to question him about his latest book and personal manifesto: <a href=""https://press.stripe.com/#stubborn-attachments""><em>Stubborn Attachments: A Vision for a Society of Free, Prosperous, and Responsible Individuals</em></a>.<br><br>

Tyler makes the case that, despite what you may have heard, we *can* make rational judgments about what is best for society as a whole. He argues:<br><br>

1. Our top moral priority should be preserving and improving humanity's long-term future<br>
2. The way to do that is to maximise the rate of sustainable economic growth<br>
3. We should respect human rights and follow general principles while doing so.<br><br>

We discuss why Tyler believes all these things, and I push back where I disagree. In particular: is higher economic growth actually an effective way to safeguard humanity's future, or should our focus really be elsewhere?<br><br>

In the process we touch on many of moral philosophy's most pressing questions: Should we discount the future? How should we aggregate welfare across people? Should we follow rules or evaluate every situation individually? How should we deal with the massive uncertainty about the effects of our actions? And should we trust common sense morality or follow structured theories?<br><br>

<a href=""https://80000hours.org/podcast/episodes/tyler-cowen-stubborn-attachments/?utm_campaign=podcast__tyler-cowen&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Links to learn more, summary and full transcript.</b></a><br><br>

After covering the book, the conversation ranges far and wide. Will we leave the galaxy, and is it a tragedy if we don't? Is a multi-polar world less stable? Will humanity ever help wild animals? Why do we both agree that Kant and Rawls are overrated?<br><br>

Today's interview is released on both the 80,000 Hours Podcast and Tyler's own show: <a href=""https://medium.com/conversations-with-tyler"">Conversation with Tyler</a>.<br><br>

Tyler may have had more influence on me than any other writer but this conversation is richer for our remaining disagreements. If the above isn't enough to tempt you to listen, we also look at:<br><br>

* Why couldn’t future technology make human life a hundred or a thousand times better than it is for people today?<br>
* Why focus on increasing the rate of economic growth rather than making sure that it doesn’t go to zero?<br>
* Why shouldn’t we dedicate substantial time to the successful introduction of genetic engineering?<br>
* Why should we completely abstain from alcohol and make it a social norm?<br>
* Why is Tyler so pessimistic about space? Is it likely that humans will go extinct before we manage to escape the galaxy?<br>
* Is improving coordination and international cooperation a major priority?<br>
* Why does Tyler think institutions are keeping up with technology?<br>
* Given that our actions seem to have very large and morally significant effects in the long run, are our moral obligations very onerous?<br>
* Can art be intrinsically valuable?<br>
* What does Tyler think Derek Parfit was most wrong about, and what was he was most right about that’s unappreciated today?<br><br>

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/575384ce-2a3d-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340455613,http://feedproxy.google.com/~r/80000HoursPodcast/~5/wblCz0tmF08/515664354-80000-hours-tyler-cowen-stubborn-attachments.mp3,False
65,"#44 - Dr Paul Christiano on how we'll hand the future off to AI, & solving the alignment problem","Paul Christiano is one of the smartest people I know. After our first session produced such great material, we decided to do a second recording, resulting in our longest interview so far. While challenging at times I can strongly recommend listening - Paul works on AI himself and has a very unusually thought through view of how it will change the world. This is now the top resource I'm going to refer people to if they're interested in positively shaping the development of AI, and want to understand the problem better. Even though I'm familiar with Paul's writing I felt I was learning a great deal and am now in a better position to make a difference to the world.<br><br>

A few of the topics we cover are:<br><br>

* Why Paul expects AI to transform the world gradually rather than explosively and what that would look like<br>
* Several concrete methods OpenAI is trying to develop to ensure AI systems do what we want even if they become more competent than us<br>
* Why AI systems will probably be granted legal and property rights<br>
* How an advanced AI that doesn't share human goals could still have moral value<br>
* Why machine learning might take over science research from humans before it can do most other tasks<br>
* Which decade we should expect human labour to become obsolete, and how this should affect your savings plan.<br><br>

<a href=""https://80000hours.org/podcast/episodes/paul-christiano-ai-alignment-solutions/?utm_campaign=podcast__paul-christiano&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Links to learn more, summary and full transcript.</b></a><br><br>

Important new article: <a href=""https://80k.link/high-impact-careers-paul""><b>These are the world’s highest impact career paths according to our research</b></a><br><br>

Here's a situation we all regularly confront: you want to answer a difficult question, but aren't quite smart or informed enough to figure it out for yourself. The good news is you have access to experts who *are* smart enough to figure it out. The bad news is that they disagree.<br><br>

If given plenty of time - and enough arguments, counterarguments and counter-counter-arguments between all the experts - should you eventually be able to figure out which is correct? What if one expert were deliberately trying to mislead you? And should the expert with the correct view just tell the whole truth, or will competition force them to throw in persuasive lies in order to have a chance of winning you over?<br><br>

In other words: does 'debate', in principle, lead to truth?<br><br>

According to Paul Christiano - researcher at the machine learning research lab OpenAI and legendary thinker in the effective altruism and rationality communities - this question is of more than mere philosophical interest. That's because 'debate' is a promising method of keeping artificial intelligence aligned with human goals, even if it becomes much more intelligent and sophisticated than we are.<br><br>

It's a method OpenAI is <a href=""https://blog.openai.com/debate/"">actively trying to develop</a>, because in the long-term it wants to train AI systems to make decisions that are too complex for any human to grasp, but without the risks that arise from a complete loss of human oversight.<br><br> 

If AI-1 is free to choose any line of argument in order to attack the ideas of AI-2, and AI-2 always seems to successfully defend them, it suggests that every possible line of argument would have been unsuccessful.<br><br>

But does that mean that the ideas of AI-2 were actually right? It would be nice if the optimal strategy in debate were to be completely honest, provide good arguments, and respond to counterarguments in a valid way. But we don't know that's the case.<br><br>

<b>Get this episode by subscribing: type '80,000 Hours' into your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/31d95a52-2a3d-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340455700,http://feedproxy.google.com/~r/80000HoursPodcast/~5/N94WDTMrtCU/507858087-80000-hours-paul-christiano-ai-alignment-solutions.mp3,False
66,#43 - Daniel Ellsberg on the institutional insanity that maintains nuclear doomsday machines,"In Stanley Kubrick’s iconic film Dr. Strangelove, the American president is informed that the Soviet Union has created a secret deterrence system which will automatically wipe out humanity upon detection of a single nuclear explosion in Russia. With US bombs heading towards the USSR and unable to be recalled, Dr Strangelove points out that “the whole point of this Doomsday Machine is lost if you keep it a secret – why didn’t you tell the world, eh?” The Soviet ambassador replies that it was to be announced at the Party Congress the following Monday: “The Premier loves surprises”. <br><br>

Daniel Ellsberg - leaker of the Pentagon Papers which helped end the Vietnam War and Nixon presidency - claims in his new book <a href=""https://www.amazon.com/Doomsday-Machine-Confessions-Nuclear-Planner/dp/1608196704/"">The Doomsday Machine: Confessions of a Nuclear War Planner</a> that Dr. Strangelove might as well be a documentary. After attending the film in Washington DC in 1964, he and a colleague wondered how so many details of their nuclear planning had leaked.<br><br>
<a href=""https://80000hours.org/podcast/episodes/daniel-ellsberg-doomsday-machines/?utm_campaign=podcast__daniel-ellsberg&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Links to learn more, summary and full transcript.</b></a><br><br>

The USSR did in fact develop a doomsday machine, Dead Hand, which probably remains active today. <br><br>

If the system can’t contact military leaders, it checks for signs of a nuclear strike, and if it detects them, automatically launches all remaining Soviet weapons at targets across the northern hemisphere.<br><br>

As in the film, the Soviet Union long kept Dead Hand completely secret, eliminating any strategic benefit, and rendering it a pointless menace to humanity.<br><br>

You might think the United States would have a more sensible nuclear launch policy. You’d be wrong.<br><br>

As Ellsberg explains, based on first-hand experience as a nuclear war planner in the 50s, that the notion that only the president is able to authorize the use of US nuclear weapons is a carefully cultivated myth.<br><br>

The authority to launch nuclear weapons is delegated alarmingly far down the chain of command – significantly raising the chance that a lone wolf or communication breakdown could trigger a nuclear catastrophe.<br><br>

The whole justification for this is to defend against a ‘decapitating attack’, where a first strike on Washington disables the ability of the US hierarchy to retaliate. In a moment of crisis, the Russians might view this as their best hope of survival.<br><br>

Ostensibly, this delegation removes Russia’s temptation to attempt a decapitating attack – the US can retaliate even if its leadership is destroyed. This strategy only works, though, if the tell the enemy you’ve done it.<br><br>

Instead, since the 50s this delegation has been one of the United States most closely guarded secrets, eliminating its strategic benefit, and rendering it another pointless menace to humanity.<br><br>
Strategically, the setup is stupid. Ethically, it is monstrous.<br><br>
So – how was such a system built? Why does it remain to this day? And how might we shrink our nuclear arsenals to the point they don’t risk the destruction of civilization?<br><br>

Daniel explores these questions eloquently and urgently in his book. Today we cover:<br><br>

* Why full disarmament today would be a mistake and the optimal number of nuclear weapons to hold<br>
* How well are secrets kept in the government?<br>
* What was the risk of the first atomic bomb test?<br>
* The effect of Trump on nuclear security<br>
* Do we have a reliable estimate of the magnitude of a ‘nuclear winter’?<br>
* Why Gorbachev allowed Russia’s covert biological warfare program to continue<br><br>

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/13e1f7c0-2a3d-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340455789,http://feedproxy.google.com/~r/80000HoursPodcast/~5/0gwDxfaF-HA/504884409-80000-hours-daniel-ellsberg-doomsday-machines.mp3,False
67,"#42 - Dr Amanda Askell on moral empathy, the value of information & the ethics of infinity","Consider two familiar moments at a family reunion.<br><br>

Our host, Uncle Bill, takes pride in his barbecuing skills. But his niece Becky says that she now refuses to eat meat. A groan goes round the table; the family mostly think of this as an annoying picky preference. But if seriously considered as a moral position, as they might if instead Becky were avoiding meat on religious grounds, it would usually receive a very different reaction.<br><br>

An hour later Bill expresses a strong objection to abortion. Again, a groan goes round the table; the family mostly think that he has no business in trying to foist his regressive preference on anyone. But if considered not as a matter of personal taste, but rather as a moral position - that Bill genuinely believes he’s opposing mass-murder - his comment might start a serious conversation.<br><br>

Amanda Askell, who recently completed a PhD in philosophy at NYU focused on the ethics of infinity, thinks that we often betray a complete lack of moral empathy. All sides of the political spectrum struggle to get inside the mind of people we disagree with and see issues from their point of view.<br><br>

<a href=""https://80k.link/amanda""><b>Links to learn more, summary and full transcript.</b></a><br><br>

This often happens because of confusion between preferences and moral positions.<br><br>

Assuming good faith on the part of the person you disagree with, and actually engaging with the beliefs they claim to hold, is perhaps the best remedy for our inability to make progress on controversial issues. <br><br>

One potential path for progress surrounds contraception; a lot of people who are anti-abortion are also anti-contraception. But they’ll usually think that abortion is much worse than contraception, so why can’t we compromise and agree to have much more contraception available?<br><br>

<a href=""http://www.rationalreflection.net/vegetarianism-abortion-and-moral-empathy/"">According to Amanda</a>, a charitable explanation for this is that people who are anti-abortion and anti-contraception engage in moral reasoning and advocacy based on what, in their minds, is the best of all possible worlds: one where people neither use contraception nor get abortions.<br><br>

So instead of arguing about abortion and contraception, we could discuss the underlying principle that one should advocate for the best possible world, rather than the best probable world.<br><br>

Successfully break down such ethical beliefs, absent political toxicity, and it might be possible to actually converge on a key point of agreement.<br><br>

Today’s episode blends such everyday topics with in-depth philosophy, including:<br><br>

* What is 'moral cluelessness' and how can we work around it?<br>
* Amanda's biggest criticisms of social justice activists, and of critics of social justice activists<br>
* Is there an ethical difference between prison and corporal punishment?<br>
* How to resolve 'infinitarian paralysis' - the inability to make decisions when infinities are involved.<br>
* What’s effective altruism doing wrong?<br>
* How should we think about jargon? Are a lot of people who don’t communicate clearly just scamming us?<br>
* How can people be more successful within the cocoon of school and university?<br>
* How did Amanda find doing a philosophy PhD, and how will she decide what to do now?<br><br>

<b>Links:</b><br>
<a href=""https://80k.link/congressional-staffer-amanda"">* Career review: Congressional staffer</a><br>
<a href=""https://80k.link/quitting"">* Randomised experiment on quitting</a><br>
<a href=""https://80k.link/psychology-replication-quiz-amanda"">* Psychology replication quiz</a><br>
<a href=""https://80k.link/comparative-advantage-amanda"">* Should you focus on your comparative advantage.</a><br><br>

<b>Get this episode by subscribing: type 80,000 Hours into your podcasting app.</b><br><br>

<em>The 80,000 Hours podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/f57fc03c-2a3c-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340455876,http://feedproxy.google.com/~r/80000HoursPodcast/~5/Zf7iFdaXlBk/497939877-80000-hours-amanda-askell-moral-empathy.mp3,False
68,"#41 - David Roodman on incarceration, geomagnetic storms, & becoming a world-class researcher","With 698 inmates per 100,000 citizens, the U.S. is by far the leader among large wealthy nations in incarceration. But what effect does imprisonment actually have on crime?<br /><br />

According to David Roodman, Senior Advisor to the Open Philanthropy Project, the marginal effect is zero.<br /><br />

* <a href=""https://80k.link/impactsurveyDR""><b>80,000 HOURS IMPACT SURVEY</b></a> - Let me know how this show has helped you with your career.<br>
* <a href=""https://80k.link/audiobook""><b>ROB'S AUDIOBOOK RECOMMENDATIONS</b></a><br><br>

This stunning rebuke to the American criminal justice system comes from the man Holden Karnofsky’s called ""the gold standard for in-depth quantitative research"", whose other investigations include the risk of geomagnetic storms, whether deworming improves health and test scores, and the development impacts of microfinance.<br /><br />

<a href=""https://80000hours.org/podcast/episodes/david-roodman-becoming-a-world-class-researcher/?utm_campaign=podcast__david-roodman&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Links to learn more, summary and full transcript.</b></a><br><br>

The effects of crime can be split into three categories; before, during, and after.<br /><br />

Does having tougher sentences deter people from committing crime?<br /><br /> 

After reviewing studies on gun laws and ‘three strikes’ in California, David concluded that the effect of deterrence is zero.<br /><br />

Does imprisoning more people reduce crime by incapacitating potential offenders?<br /><br /> 

Here he says yes, noting that crimes like motor vehicle theft have gone up in a way that seems pretty clearly connected with recent Californian criminal justice reforms (though the effect on violent crime is far lower).<br /><br />

Finally, do the after-effects of prison make you more or less likely to commit future crimes?<br /><br /> 

This one is more complicated.<br /><br />

Concerned that he was biased towards a comfortable position against incarceration, David did a cost-benefit analysis using both his favored reading of the evidence and the devil's advocate view; that there is deterrence and that the after-effects are beneficial.<br /><br />

For the devil’s advocate position David used the highest assessment of  the harm caused by crime, which suggests a year of prison prevents about $92,000 in crime. But weighed against a lost year of liberty, valued at $50,000, plus the cost of operating prisons, the numbers came out exactly the same.<br /><br /> 

So even using the least-favorable cost-benefit valuation of the least favorable reading of the evidence -- it just breaks even.<br /><br />

The argument for incarceration melts further when you consider the significant crime that occurs within prisons, de-emphasised because of a lack of data and a perceived lack of compassion for inmates.<br /><br />

In today’s episode we discuss how to conduct such impactful research, and how to proceed having reached strong conclusions.<br /><br />

We also cover:<br /><br />

* How do you become a world class researcher? What kinds of character traits are important?<br />
* Are academics aware of following perverse incentives?<br />
* What’s involved in data replication? How often do papers replicate?<br />
* The politics of large orgs vs. small orgs<br />
* Geomagnetic storms as a potential cause area<br />
* How much does David rely on interviews with experts?<br />
* The effects of deworming on child health and test scores<br />
* Should we have more ‘data vigilantes’?<br />
* What are David’s critiques of effective altruism?<br />
* What are the pros and cons of starting your career in the think tank world?<br /><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/db895db4-2a3c-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340455963,http://feedproxy.google.com/~r/80000HoursPodcast/~5/uShbAw9TrrA/491806653-80000-hours-david-roodman-becoming-a-world-class-researcher.mp3,False
69,#40 - Katja Grace on forecasting future technology & how much we should trust expert predictions,"Experts believe that artificial intelligence will be better than humans at driving trucks by 2027, working in retail by 2031, writing bestselling books by 2049, and working as surgeons by 2053. But how seriously should we take these predictions?<br /><br />

Katja Grace, lead author of <a href=""https://arxiv.org/pdf/1705.08807.pdf?_sp=c803ec8d-9f8f-4843-a81e-3284733403a0.1500631875031"" rel=""nofollow"" target=""_blank"">‘When Will AI Exceed Human Performance?’</a>, thinks we should treat such guesses as only weak evidence. But she also says there might be much better ways to forecast transformative technology, and that anticipating such advances could be one of our most important projects.<br /><br />

<b>Note: Katja's organisation AI Impacts <a href=""https://aiimpacts.org/jobs/"">is currently hiring part- and full-time researchers.</a></b><br><br>

There’s often pessimism around making accurate predictions in general, and some areas of artificial intelligence might be particularly difficult to forecast.<br /><br /> 

But there are also many things we’re able to predict confidently today -- like the climate of Oxford in five years -- that we no longer give ourselves much credit for.<br /><br />

Some aspects of transformative technologies could fall into this category. And these easier predictions could give us some structure on which to base the more complicated ones.<br /><br />

<a href=""https://80000hours.org/podcast/episodes/katja-grace-forecasting-technology/?utm_campaign=podcast__katja-grace&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Links to learn more, summary and full transcript.</b></a><br><br>

One controversial debate surrounds the idea of an intelligence explosion; how likely is it that there will be a sudden jump in AI capability?<br /><br />

And one way to tackle this is to investigate a more concrete question: what’s the base rate of any technology having a big discontinuity?<br /><br />

A significant historical example was the development of nuclear weapons. Over thousands of years, the efficacy of explosives didn’t increase by much. Then within a few years, it got thousands of times better. Discovering what leads to such anomalies may allow us to better predict the possibility of a similar jump in AI capabilities.<br /><br />

In today’s interview we also discuss:<br><br>

* Why is AI impacts one of the most important projects in the world?<br /> 
* How do you structure important surveys? Why do you get such different answers when asking what seem to be very similar questions?<br />
* How does writing an academic paper differ from posting a summary online?<br />
* When will unguided machines be able to produce better and cheaper work than humans for every possible task?<br />
* What’s one of the most likely jobs to be automated soon?<br />
* Are people always just predicting the same timelines for new technologies?<br />
* How do AGI researchers different from other AI researchers in their predictions?<br />
* What are attitudes to safety research like within ML? Are there regional differences?<br />
* How much should we believe experts generally?<br />
* How does the human brain compare to our best supercomputers? How many human brains are worth all the hardware in the world?<br />
* How quickly has the processing capacity for machine learning problems been increasing?<br />
* What can we learn from the development of previous technologies in figuring out how fast transformative AI will arrive?<br />
* What should we expect from a post AI dominated economy?<br />
* How much influence can people ever have on things that will happen in 20 years? Are there any examples of people really trying to do this?<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.</b><br><br>

<em>The 80,000 Hours podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/c22d3bba-2a3c-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340456051,http://feedproxy.google.com/~r/80000HoursPodcast/~5/zmbHjSd64yE/488487222-80000-hours-katja-grace-forecasting-technology.mp3,False
70,#39 - Spencer Greenberg on the scientific approach to solving difficult everyday questions,"Will Trump be re-elected? Will North Korea give up their nuclear weapons? Will your friend turn up to dinner?<br /><br />

Spencer Greenberg, founder of <a href=""https://www.clearerthinking.org"" rel=""nofollow"" target=""_blank"">ClearerThinking.org</a> has a process for working out such real life problems.<br /><br />

Let’s work through one here: how likely is it that you’ll enjoy listening to this episode?<br /><br />

The first step is to figure out your ‘prior probability’; what’s your estimate of how likely you are to enjoy the interview before getting any further evidence?<br /><br />

Other than applying common sense, one way to figure this out is called reference class forecasting: looking at similar cases and seeing how often something is true, on average.<br /><br />

Spencer is our first ever return guest. So one reference class might be, how many Spencer Greenberg episodes of the 80,000 Hours Podcast have you enjoyed so far? Being this specific limits bias in your answer, but with a sample size of at most 1 - you’d probably want to add more data points to reduce variability.<br /><br />

Zooming out, how many episodes of the 80,000 Hours Podcast have you enjoyed? Let’s say you’ve listened to 10, and enjoyed 8 of them. If so 8 out of 10 might be your prior probability.<br /><br />

But maybe the two you didn’t enjoy had something in common. If you’ve liked similar episodes in the past, you’d update in favour of expecting to enjoy it, and if you’ve disliked similar episodes in the past, you’d update negatively.<br /><br />

You can zoom out further; what fraction of long-form interview podcasts have you ever enjoyed?<br /><br />

Then you’d look to update whenever new information became available. Do the topics seem interesting? Did Spencer make a great point in the first 5 minutes? Was this description unbearably self-referential?<br /><br />

Speaking of the Question of Evidence: in a world where Spencer was not worth listening to, how likely is it that we’d invite him back for a second episode?<br /><br />

<a href=""https://80000hours.org/podcast/episodes/spencer-greenberg-bayesian-updating/?utm_campaign=podcast__spencer-greenberg-2&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Links to learn more, summary and full transcript.</b></a><br><br>

We’ll run through several diverse examples, and how to actually work out the changing probabilities as you update. But that’s only a fraction of the conversation. We also discuss:<br /><br />

* How could we generate 20-30 new happy thoughts a day? What would that do to our welfare?<br />
* What do people actually value? How do EAs differ from non EAs?<br />
* Why should we care about the distinction between intrinsic and instrumental values?<br />
* Would hedonic utilitarians really want to hook themselves up to happiness machines?<br />
* What types of activities are people generally under-confident about? Why?<br />
* When should you give a lot of weight to your prior belief?<br />
* When should we trust common sense?<br />
* Does power posing have any effect?<br />
* Are resumes worthless?<br />
* Did Trump explicitly collude with Russia? What are the odds of him getting re-elected?<br />
* What’s the probability that China and the US go to War in the 21st century?<br />
* How should we treat claims of expertise on diets?<br />
* Why were Spencer’s friends suspicious of Theranos for years?<br />
* How should we think about the placebo effect?<br />
* Does a shift towards rationality typically cause alienation from family and friends? How do you deal with that?<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.</b><br><br>

<em>The 80,000 Hours podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/a3306552-2a3c-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340456139,http://feedproxy.google.com/~r/80000HoursPodcast/~5/TK38vuXMABY/482232300-80000-hours-spencer-greenberg-bayesian-updating.mp3,False
71,#38 - Prof Ng on anticipating effective altruism decades ago & how to make a much happier world,"Will people who think carefully about how to maximize welfare eventually converge on the same views?<br><br>

The effective altruism community has spent a lot of time over the past 10 years debating how best to increase happiness and reduce suffering, and gradually narrowed in on the world’s poorest people, all animals capable of suffering, and future generations.<br><br>

Yew-Kwang Ng, Professor of Economics at Nanyang Technological University in Singapore,  was independently working on this exact question since the 70s. Many of his conclusions have ended up foreshadowing what is now conventional wisdom within effective altruism - though other views he holds remain controversial or little-known.<br><br>

For instance, he thinks we ought to explore increasing pleasure via direct brain stimulation, and that genetic engineering may be an important tool for increasing happiness in the future.<br><br>

His work has suggested that the welfare of most wild animals is on balance negative and he thinks that in the future this is a problem humanity might work to solve. Yet he thinks that greatly improved conditions for farm animals could eventually justify eating meat.<br><br>

He has spent most of his life advocating for the view that happiness, broadly construed, is the only intrinsically valuable thing.<br><br>

If it’s true that careful researchers will converge as Prof Ng believes, these ideas may prove as prescient as his other, now widely accepted, opinions.<br><br>

<a href=""https://80k.link/kwang-articles""><b>Link to our summary and appreciation of Kwang’s top publications and insights throughout a lifetime of research.</b></a><br><br>

Kwang has led an exceptional life. While in high school he was drawn to physics, mathematics, and philosophy, yet he chose to study economics because of his dream: to establish communism in an independent Malaya.<br><br>

But events in the Soviet Union and China, in addition to his burgeoning knowledge and academic appreciation of economics, would change his views about the practicability of communism. He would soon complete his journey from young revolutionary to academic economist, and eventually become a columnist writing in support of Deng Xiaoping’s Chinese economic reforms in the 80s.<br><br>

He got his PhD at Sydney University in 1971, and has since published over 250 refereed papers - covering economics, biology, politics, mathematics, philosophy, psychology, and sociology. <br><br>

He's most well-known for his work in welfare economics, and proposed ‘welfare biology’ as a new field of study. In 2007, he was made a Distinguished Fellow of the Economic Society of Australia, the highest award that the society bestows.<br><br>

<a href=""https://80000hours.org/podcast/episodes/yew-kwang-ng-anticipating-effective-altruism/?utm_campaign=podcast__yew-kwangng&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Links to learn more, summary and full transcript.</b></a><br><br>

In this episode we discuss how he developed some of his most unusual ideas and his fascinating life story, including:<br><br>

* Why Kwang believes that *’Happiness Is Absolute, Universal, Ultimate, Unidimensional, Cardinally Measurable and Interpersonally Comparable’*<br>
* What are the most pressing questions in economics?<br>
* Did Kwang have to worry about censorship from the Chinese government when promoting market economics, or concern for animal welfare?<br>
* Welfare economics and where Kwang thinks it went wrong<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/8ae8e8b6-2a3c-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340456226,http://feedproxy.google.com/~r/80000HoursPodcast/~5/GHyA88FDxrk/476762145-80000-hours-yew-kwang-ng-anticipating-effective-altruism.mp3,False
72,#37 - GiveWell picks top charities by estimating the unknowable. James Snowden on how they do it.,"What’s the value of preventing the death of a 5-year-old child, compared to a 20-year-old, or an 80-year-old?<br /><br />

The global health community has generally regarded the value as proportional to the number of health-adjusted life-years the person has remaining - but GiveWell, one of the world’s foremost charity evaluators, no longer uses that approach.<br><br>

They found that contrary to the years-remaining’ method, many of their staff actually value preventing the death of an adult more than preventing the death of a young child. However there’s plenty of disagreement: the team’s estimates of the relative value span a four-fold range.<br /><br />

As James Snowden - a research consultant at GiveWell - explains in this episode, there’s no way around making these controversial judgement calls based on limited information. If you try to ignore a question like this, you just implicitly take an unreflective stand on it instead. And for each charity they look into there’s 1 or 2 dozen of these highly uncertain parameters they need to estimate.<br /><br />

GiveWell has been trying to find better ways to make these decisions since its inception in 2007. Lives hang in the balance, so they want their staff to say what they really believe and bring their private knowledge to the table, rather than just defer to a imaginary consensus.<br /><br />

Their strategy is <a href=""https://www.givewell.org/how-we-work/our-criteria/cost-effectiveness/cost-effectiveness-models"" rel=""nofollow"" target=""_blank"">a massive spreadsheet</a> that lists dozens of things they need to estimate, and asking every staff member to give a figure and justification. Then once a year, the GiveWell team get together and try to identify what they really disagree about and think through what evidence it would take to change their minds.<br /><br />

<a href=""https://80000hours.org/podcast/episodes/james-snowden-givewell-research/?utm_campaign=podcast__james-snowden&utm_source=80000+Hours+Podcast&utm_medium=podcast"" rel=""nofollow"" target=""_blank"">Full transcript, summary of the conversation and links to learn more.</a><br /><br />

Often the people who have the greatest familiarity with a particular intervention are the ones who drive the decision, as others defer to them. But the group can also end up with very different figures, based on different prior beliefs about moral issues and how the world works. In that case then use the median of everyone’s best guess to make their key decisions.<br /><br />

In making his estimate of the relative badness of dying at different ages, James specifically considered two factors: how many years of life do you lose, and how much interest do you have in those future years? Currently, James believes that the worst time for a person to die is around 8 years of age.<br /><br />

We discuss his experiences with such calculations, as well as a range of other topics:<br /><br />

* Why GiveWell’s recommendations have changed more than it looks.<br />
* What are the biggest research priorities for GiveWell at the moment?<br />
* How do you take into account the long-term knock-on effects from interventions?<br />
* If GiveWell's advice were going to end up being very different in a couple years' time, how might that happen?<br />
* Are there any charities that James thinks are really cost-effective which GiveWell hasn't funded yet?<br />
* How does domestic government spending in the developing world compare to effective charities?<br />
* What are the main challenges with policy related interventions?<br />
* How much time do you spend discovering new interventions?<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/73f24012-2a3c-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340456313,http://feedproxy.google.com/~r/80000HoursPodcast/~5/yDJxkGTPyAY/472516044-80000-hours-james-snowden-givewell-research.mp3,False
73,#36 - Tanya Singh on ending the operations management bottleneck in effective altruism,"Almost nobody is able to do groundbreaking physics research themselves, and by the time his brilliance was appreciated, Einstein was hardly limited by funding. But what if you could find a way to unlock the secrets of the universe like Einstein nonetheless?<br><br>

Today’s guest, Tanya Singh, sees herself as doing something like that every day. She’s Executive Assistant to one of her intellectual heroes who she believes is making a huge contribution to improving the world: Professor Bostrom at Oxford University's Future of Humanity Institute (FHI).<br><br>

She couldn’t get more work out of Bostrom with extra donations, as his salary is already easily covered. But with her superior abilities as an Executive Assistant, Tanya frees up hours of his time every week, essentially ‘buying’ more Bostrom in a way nobody else can. She also help manage FHI more generally, in so doing freeing up more than an hour of other staff time for each hour she works. This gives her the leverage to do more good than other people or other positions.<br><br>

In our <a href=""https://80000hours.org/podcast/episodes/tara-mac-aulay-operations-mindset/?utm_campaign=podcast__tanya-singh&utm_source=80000+Hours+Podcast&utm_medium=podcast"">previous episode</a>, Tara Mac Aulay objected to viewing operations work as predominately a way of freeing up other people's time:<br><br>

“A good ops person doesn’t just allow you to scale linearly, but also can help figure out bottlenecks and solve problems such that the organization is able to do qualitatively different work, rather than just increase the total quantity”, Tara said.<br><br>

<a href=""https://80000hours.org/podcast/episodes/tanya-singh-operations-bottleneck/?utm_campaign=podcast__tanya-singh&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Full transcript, summary and links to learn more.</a><br><br>

Tara’s right that buying time for people at the top of their field is just one path to impact, though it’s one Tanya says she finds highly motivating. Other paths include enabling complex projects that would otherwise be impossible, allowing you to hire and grow much faster, and preventing disasters that could bring down a whole organisation - all things that Tanya does at FHI as well.<br><br>

In today’s episode we discuss all of those approaches, as we dive deeper into the broad class of roles we refer to as ‘operations management’. We cover the arguments we made in <a href=""https://80000hours.org/articles/operations-management/?utm_campaign=podcast__tanya-singh&utm_source=80000+Hours+Podcast&utm_medium=podcast"">‘Why operations management is one of the biggest bottlenecks in effective altruism’</a>, as well as:<br><br>

* Does one really need to hire people aligned with an org’s mission to work in ops?<br>
* The most notable operations successes in the 20th Century.<br>
* What’s it like being the only operations person in an org?<br>
* The role of a COO as compared to a CEO, and the options for career progression.<br>
* How do good operation teams allow orgs to scale quickly?<br>
* How much do operations staff get to set their org’s strategy?<br>
* Which personal weaknesses aren’t a huge problem in operations?<br>
* How do you automate processes? Why don’t most people do this?<br>
* Cultural differences between Britain and India where Tanya grew up.<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type 80,000 Hours into your podcasting app. Or read the transcript below.</b><br><br>

<em>The 80,000 Hours podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/5b296362-2a3c-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340456400,http://feedproxy.google.com/~r/80000HoursPodcast/~5/DbjRAP1hSGo/470335812-80000-hours-tanya-singh-operations-bottleneck.mp3,False
74,#35 - Tara Mac Aulay on the audacity to fix the world without asking permission,"<em>""You don't need permission. You don't need to be allowed to do something that's not in your job description. If you think that it's gonna make your company or your organization more successful and more efficient, you can often just go and do it.""</em><br><br>

How broken is the world? How inefficient is a typical organisation? Looking at Tara Mac Aulay’s life, the answer seems to be ‘very’.<br><br>

At 15 she took her first job - an entry-level position at a chain restaurant. Rather than accept her place, Tara took it on herself to massively improve the store’s shambolic staff scheduling and inventory management. After cutting staff costs 30% she was quickly promoted, and at 16 sent in to overhaul dozens of failing stores in a final effort to save them from closure.<br><br>

That’s just the first in a startling series of personal stories that take us to a hospital drug dispensary where pharmacists are wasting a third of their time, a chemotherapy ward in Bhutan that’s killing its patients rather than saving lives, and eventually the Centre for Effective Altruism, where Tara becomes CEO and leads it through start-up accelerator Y Combinator.<br><br>

In this episode Tara shows how the ability to do practical things, avoid major screw-ups, and design systems that scale, is both rare and precious. <br><br>

<a href=""https://80000hours.org/podcast/episodes/tara-mac-aulay-operations-mindset/?utm_campaign=podcast__tara-mac-aulay&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Full transcript, key quotes and links to learn more.</b></a><br><br>

People with an operations mindset spot failures others can't see and fix them before they bring an organisation down. This kind of resourcefulness can transform the world by making possible critical projects that would otherwise fall flat on their face.<br><br>

But as Tara's experience shows they need to figure out what actually motivates the authorities who often try to block their reforms.<br><br>

We explore how people with this skillset can do as much good as possible, what 80,000 Hours got wrong in our article <a href=""https://80000hours.org/articles/operations-management/?utm_campaign=podcast__tara-mac-aulay&utm_source=80000+Hours+Podcast&utm_medium=podcast"">'Why operations management is one of the biggest bottlenecks in effective altruism’</a>, as well as:<br><br>

* Tara’s biggest mistakes and how to deal with the delicate politics of organizational reform.<br>
* How a student can save a hospital millions with a simple spreadsheet model.<br>
* The sociology of Bhutan and how medicine in the developing world often makes things worse rather than better.<br>
* What most people misunderstand about operations, and how to tell if you have what it takes.<br>
* And finally, operations jobs people should consider applying for, such as those open now at the <a href=""https://www.centreforeffectivealtruism.org/careers/"">Centre for Effective Altruism</a>.<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/459c6b5c-2a3c-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340456488,http://feedproxy.google.com/~r/80000HoursPodcast/~5/KxkayVtrLh0/461614122-80000-hours-tara-mac-aulay-operations-mindset.mp3,False
75,Rob Wiblin on the art/science of a high impact career,"Today's episode is a cross-post of an interview I did with The Jolly Swagmen Podcast which came out this week. I recommend regular listeners skip to 24 minutes in to avoid hearing things they already know. Later in the episode I talk about my contrarian views, utilitarianism, how 80,000 Hours has changed and will change in the future, where I think EA is performing worst, how to use social media most effectively, and whether or not effective altruism is any sacrifice.<br><br>

Subscribe and get the episode by searching for '80,000 Hours' in your podcasting app.<br><br>

<a href=""https://josephnoelwalker.com/51-the-art-science-of-a-high-impact-career-rob-wiblin/"">Blog post of the episode to share, including a list of topics and links to learn more.</a><br><br>

""Most people want to help others with their career, but what’s the best way to do that? Become a doctor? A politician? Work at a non-profit? How can any of us figure out the best way to use our skills to improve the world?<br><br>

Rob Wiblin is the Director of Research at 80,000 Hours, an organisation founded in Oxford in 2011, which aims to answer just this question and help talented people find their highest-impact career path. He hosts a popular podcast on ‘the world’s most pressing problems and how you can use your career to solve them’.<br><br>

After seven years of research, the 80,000 Hours team recommends against becoming a teacher, or a doctor, or working at most non-profits. And they claim their research shows some common careers do 10 or 100x as much good as others.<br><br>

80,000 Hours was one of the organisations that kicked off the effective altruism movement, was a Y Combinator-backed non-profit, and has already shifted over 80 million career hours through its advice.<br><br>

Joe caught up with Rob in Berkeley, California, to discuss how 80,000 Hours assesses which of the world’s problems are most pressing, how you can build career capital and succeed in any role, and why you could easily save more lives than a doctor - if you think carefully about your impact.""<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/30557022-2a3c-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340456578,http://feedproxy.google.com/~r/80000HoursPodcast/~5/7DmD50X5O18/455404398-80000-hours-rob-wiblin-high-impact-careers.mp3,False
76,#34 - We use the worst voting system that exists. Here's how Aaron Hamlin is going to fix it.,"In 1991 Edwin Edwards won the Louisiana gubernatorial election. In 2001, he was found guilty of racketeering and received a 10 year invitation to Federal prison. The strange thing about that election? By 1991 Edwards was already notorious for his corruption. Actually, that’s not it.<br><br>

The truly strange thing is that Edwards was clearly the <em>good guy</em> in the race. How is that possible?<br><br>

His opponent was former Ku Klux Klan Grand Wizard David Duke.<br><br>

How could Louisiana end up having to choose between a criminal and a Nazi sympathiser?<br><br>

It’s not like they lacked other options: the state’s moderate incumbent governor Buddy Roemer ran for re-election. Polling showed that Roemer was massively preferred to both the career criminal and the career bigot, and would easily win a head-to-head election against either.<br><br>

Unfortunately, in Louisiana every candidate from every party competes in the first round, and the top two then go on to a second - a so-called ‘jungle primary’. Vote splitting squeezed out the middle, and meant that Roemer was eliminated in the first round.<br><br>

Louisiana voters were left with only terrible options, in a run-off election mostly remembered for the proliferation of bumper stickers reading <em>“Vote for the Crook. It’s Important.”</em><br><br>

We could look at this as a cultural problem, exposing widespread enthusiasm for bribery and racism that will take generations to overcome. But according to Aaron Hamlin, Executive Director of The Center for Election Science (CES), there’s a simple way to make sure we never have to elect someone hated by more than half the electorate: change how we vote.<br><br>

He advocates an alternative voting method called <em>approval voting</em>, in which you can vote for as many candidates as you want, not just one. That means that you can always support your honest favorite candidate, even when an election seems like a choice between the lesser of two evils.<br><br>

<a href=""https://80000hours.org/podcast/episodes/aaron-hamlin-voting-reform/?utm_campaign=podcast__aaron-hamlin&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Full transcript, links to learn more, and summary of key points.</b></a><br><br>

<a href=""https://www.eventbrite.com/o/the-center-for-election-science-14642817210""><b>If you'd like to meet Aaron he's doing events for CES in San Francisco, DC, Philadelphia, New York and Brooklyn over the next two weeks - RSVP here.</b></a><br><br>

While it might not seem sexy, this single change could transform politics. <a href=""https://en.wikipedia.org/wiki/Approval_voting"">Approval voting</a> is adored by voting researchers, who regard it as the best simple voting system available.<br><br>

Which do they regard as unquestionably the worst? <em>First-past-the-post</em> - precisely the disastrous system used and exported around the world by the US and UK.<br><br>

Aaron has a practical plan to spread approval voting across the US using ballot initiatives - and it just might be our best shot at making politics a bit less unreasonable.<br><br>

<a href=""https://electology.org/what-we-do"">The Center for Election Science</a> is a U.S. non-profit which aims to fix broken government by helping the world adopt smarter election systems. They recently received <a href=""https://www.openphilanthropy.org/giving/grants/the-center-for-election-science-general-support"">a $600,000 grant</a> from the Open Philanthropy Project to scale up their efforts.<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/1631a0f8-2a3c-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340456665,http://feedproxy.google.com/~r/80000HoursPodcast/~5/9GRBBnS0Mkg/452181057-80000-hours-aaron-hamlin-voting-reform.mp3,False
77,"#33 - Dr Anders Sandberg on what if we ended ageing, solar flares & the annual risk of nuclear war","Joseph Stalin had a life-extension program dedicated to making himself immortal. What if he had succeeded? <br><br>

According to our last guest, <a href=""https://80000hours.org/podcast/episodes/bryan-caplan-case-for-and-against-education/?utm_campaign=podcast__anders-sandberg-2&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Bryan Caplan</a>, there’s an 80% chance that Stalin would still be ruling Russia today. Today’s guest disagrees.<br><br>

Like Stalin he has eyes for his own immortality - including an insurance plan that will cover the cost of cryogenically freezing himself after he dies - and thinks the technology to achieve it might be around the corner. <br><br>

Fortunately for humanity though, that guest is probably one of the nicest people on the planet: Dr Anders Sandberg of Oxford University. <br><br>

<a href=""https://80000hours.org/podcast/episodes/anders-sandberg-extending-life/?utm_campaign=podcast__anders-sandberg-2&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Full transcript of the conversation, summary, and links to learn more.</b></a><br><br>

The potential availability of technology to delay or even stop ageing means this disagreement matters, so he has been trying to model what would really happen if both the very best and the very worst people in the world could live forever - among many other questions. <br><br>

Anders, who studies low-probability high-stakes risks and the impact of technological change at the Future of Humanity Institute, is the first guest to appear twice on the 80,000 Hours Podcast and might just be the most interesting academic at Oxford.  <br><br>

His research interests include more or less everything, and bucking the academic trend towards intense specialization has earned him a devoted fan base. <br><br>

***Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type *80,000 Hours* into your podcasting app.*** <br><br>

Last time we asked him <a href=""https://80000hours.org/podcast/episodes/anders-sandberg-fermi-paradox/?utm_campaign=podcast__anders-sandberg-2&utm_source=80000+Hours+Podcast&utm_medium=podcast"">why we don’t see aliens, and how to most efficiently colonise the universe</a>. In today’s episode we ask about Anders’ other recent papers, including:<br><br>

* Is it worth the money to freeze your body after death in the hope of future revival, like Anders has done? <br>
* How much is our perception of the risk of nuclear war biased by the fact that we wouldn’t be alive to think about it had one happened?<br>
* If biomedical research lets us slow down ageing would culture stagnate under the crushing weight of centenarians?<br>
* What long-shot drugs can people take in their 70s to stave off death?<br>
* Can science extend human (waking) life by cutting our need to sleep?<br>
* How bad would it be if a solar flare took down the electricity grid? Could it happen?<br>
* If you’re a scientist and you discover something exciting but dangerous, when should you keep it a secret and when should you share it?<br>
* Will lifelike robots make us more inclined to dehumanise one another?<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/ff76cffa-2a3b-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340456752,http://feedproxy.google.com/~r/80000HoursPodcast/~5/6yLYMnGJNlY/450196050-80000-hours-anders-sandberg-extending-life.mp3,False
78,"#32 - Bryan Caplan on whether his Case Against Education holds up, totalitarianism, & open borders","Bryan Caplan’s claim in *The Case Against Education* is striking: education doesn’t teach people much, we use little of what we learn, and college is mostly about trying to seem smarter than other people - so the government should slash education funding.<br><br>

It’s a dismaying - almost profane - idea, and one people are inclined to dismiss out of hand. But having read the book, I have to admit that Bryan can point to a surprising amount of evidence in his favour.<br><br>

After all, imagine this dilemma: you can have either a Princeton education without a diploma, or a Princeton diploma without an education. Which is the bigger benefit of college - learning or convincing people you’re smart? It’s not so easy to say.<br><br>

For this interview, I searched for the best counterarguments I could find and challenged Bryan on what seem like his weakest or most controversial claims.<br><br>

Wouldn’t defunding education be especially bad for capable but low income students? If you reduced funding for education, wouldn’t that just lower prices, and not actually change the number of years people study? Is it really true that students who drop out in their final year of college earn about the same as people who never go to college at all?<br><br>

What about studies that show that extra years of education boost IQ scores? And surely the early years of primary school, when you learn reading and arithmetic, *are* useful even if college isn’t.<br><br>

I then get his advice on who should study, what they should study, and where they should study, if he’s right that college is mostly about separating yourself from the pack. <br><br>

<a href=""https://80000hours.org/podcast/episodes/bryan-caplan-case-for-and-against-education/?utm_campaign=podcast__bryan-caplan&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Full transcript, links to learn more, and summary of key points.</b></a><br><br>

We then venture into some of Bryan’s other unorthodox views - like that immigration restrictions are a human rights violation, or that we should worry about the risk of global totalitarianism.<br><br>

Bryan is a Professor of Economics at George Mason University, and a blogger at *EconLog*. He is also the author of *Selfish Reasons to Have More Kids: Why Being a Great Parent is Less Work and More Fun Than You Think*, and *The Myth of the Rational Voter: Why Democracies Choose Bad Policies*.<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type *80,000 Hours* into your podcasting app.</b><br><br>

In this lengthy interview, Rob and Bryan cover:<br><br>

* How worried should we be about China’s new citizen ranking system as a means of authoritarian rule?<br>
* How will advances in surveillance technology impact a government’s ability to rule absolutely?<br>
* Does more global coordination make us safer, or more at risk? <br>
* Should the push for open borders be a major cause area for effective altruism? 
<br>
* Are immigration restrictions a human rights violation?<br>
* Why aren’t libertarian-minded people more focused on modern slavery?<br>
* Should altruists work on criminal justice reform or reducing land use regulations?<br>
* What’s the greatest art form: opera, or Nicki Minaj? <br>
* What are the main implications of Bryan’s thesis for society?<br>
* Is elementary school more valuable than university?<br>
* What does Bryan think are the best arguments against his view?<br>
* Do years of education affect political affiliation?<br>
* How do people really improve themselves and their circumstances?<br>
* Who should and who shouldn’t do a masters or PhD?<br>
* The value of teaching foreign languages in school<br>
* Are there some skills people can develop that have wide applicability?<br><br>

<b>Get this episode by subscribing: search for '80,000 Hours' in your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/e51d40ee-2a3b-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340456839,http://feedproxy.google.com/~r/80000HoursPodcast/~5/EF2z-3F8Ocw/447413007-80000-hours-bryan-caplan-case-for-and-against-education.mp3,False
79,#31 - Prof Dafoe on defusing the political & economic risks posed by existing AI capabilities,"The debate around the impacts of artificial intelligence often centres on ‘superintelligence’ - a general intellect that is much smarter than the best humans, in practically every field.<br><br>

But according to Allan Dafoe - Assistant Professor of Political Science at Yale University - even if we stopped at today's AI technology and simply collected more data, built more sensors, and added more computing capacity, extreme systemic risks could emerge, including:<br><br>

* Mass labor displacement, unemployment, and inequality;<br>
* The rise of a more oligopolistic global market structure, potentially moving us away from our liberal economic world order;<br>
* Imagery intelligence and other mechanisms for revealing most of the ballistic missile-carrying submarines that countries rely on to be able to respond to nuclear attack;<br>
* Ubiquitous sensors and algorithms that can identify individuals through face recognition, leading to universal surveillance;<br>
* Autonomous weapons with an independent chain of command, making it easier for authoritarian regimes to violently suppress their citizens.<br><br>

Allan is Co-Director of the Governance of AI Program, at the Future of Humanity Institute within Oxford University. His goals have been to understand the causes of world peace and stability, which in the past has meant studying why war has declined, the role of reputation and honor as drivers of war, and the motivations behind provocation in crisis escalation.<br><br>

<a href=""https://80000hours.org/podcast/episodes/allan-dafoe-politics-of-ai/?utm_campaign=podcast__allan-dafoe&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Full transcript, links to learn more, and summary of key points.</b></a><br><br>

His current focus is helping humanity safely navigate the invention of advanced artificial intelligence.<br><br>

I ask Allan:<br><br>

* What are the distinctive characteristics of artificial intelligence from a political or international governance point of view?<br>
* Is Allan’s work just a continuation of previous research on transformative technologies, like nuclear weapons?<br>
* How can AI be well-governed?<br>
* How should we think about the idea of arms races between companies or countries?<br>
* What would you say to people skeptical about the importance of this topic?<br>
* How urgently do we need to figure out solutions to these problems? When can we expect artificial intelligence to be dramatically better than today?<br>
* What’s the most urgent questions to deal with in this field?<br>
* What can people do if they want to get into the field?<br>
* Is there anything unusual that people can look for in themselves to tell if they're a good fit to do this kind of research?<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/d403b2c0-2a3b-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340456926,http://feedproxy.google.com/~r/80000HoursPodcast/~5/qKzZlt-L0YE/445798662-80000-hours-allan-dafoe-politics-of-ai.mp3,False
80,#30 - Dr Eva Vivalt on how little social science findings generalize from one study to another,"If we have a study on the impact of a social program in a particular place and time, how confident can we be that we’ll get a similar result if we study the same program again somewhere else?<br /><br />

Dr Eva Vivalt is a lecturer in the Research School of Economics at the Australian National University. She compiled a huge database of impact evaluations in global development - including 15,024 estimates from 635 papers across 20 types of intervention - to help answer this question.<br><br>

Her finding: not confident at all.<br><br>

The typical study result differs from the average effect found in similar studies so far by almost 100%. That is to say, if all existing studies of a particular education program find that it improves test scores by 10 points - the next result is as likely to be negative or greater than 20 points, as it is to be between 0-20 points.<br><br>

She also observed that results from smaller studies done with an NGO - often pilot studies - were more likely to look promising. But when governments tried to implement scaled-up versions of those programs, their performance would drop considerably. <br /><br />

For researchers hoping to figure out what works and then take those programs global, these failures of generalizability and ‘external validity’ should be disconcerting.<br /><br />

Is ‘evidence-based development’ writing a cheque its methodology can’t cash? Should this make us invest less in empirical research, or more to get actually reliable results?<br /><br />

Or as some critics say, is interest in impact evaluation distracting us from more important issues, like national or macroeconomic reforms that can’t be easily trialled?<br /><br />

We discuss this as well as Eva’s other research, including Y Combinator’s basic income study where she is a principal investigator.<br /><br />

<a href=""https://80000hours.org/podcast/episodes/eva-vivalt-social-science-generalizability/?utm_campaign=podcast__eva-vivalt&utm_source=80000+Hours+Podcast&utm_medium=podcast"" rel=""nofollow"" target=""_blank"">Full transcript, links to related papers, and highlights from the conversation.</a><br /><br />

<b>Links mentioned at the start of the show:</b><br />
* <a href=""https://80000hours.org/job-board/?utm_campaign=podcast__eva-vivalt&utm_source=80000+Hours+Podcast&utm_medium=podcast"" rel=""nofollow"" target=""_blank"">80,000 Hours Job Board</a><br />
* <a href=""https://www.surveymonkey.com/r/BX56XKV"" rel=""nofollow"" target=""_blank"">2018 Effective Altruism Survey</a><br /><br />

**Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type *80,000 Hours* into your podcasting app.**<br /><br />

Questions include:<br /><br />

* What is the YC basic income study looking at, and what motivates it?<br />
* How do we get people to accept clean meat?<br />
* How much can we generalize from impact evaluations?<br />
* How much can we generalize from studies in development economics?<br />
* Should we be running more or fewer studies?<br />
* Do most social programs work or not?<br />
* The academic incentives around data aggregation<br />
* How much can impact evaluations inform policy decisions?<br />
* How often do people change their minds?<br />
* Do policy makers update too much or too little in the real world?<br />
* How good or bad are the predictions of experts? How does that change when looking at individuals versus the average of a group?<br />
* How often should we believe positive results?<br />
* What’s the state of development economics?<br />
* Eva’s thoughts on our article on social interventions<br />
* How much can we really learn from being empirical?<br />
* How much should we really value RCTs?<br />
* Is an Economics PhD overrated or underrated?<br><br>

<b>Get this episode by subscribing to our podcast: search for '80,000 Hours' in your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/bca9cb0a-2a3b-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340457056,http://feedproxy.google.com/~r/80000HoursPodcast/~5/LhGeOpLBt9Y/444213924-80000-hours-eva-vivalt-social-science-generalizability.mp3,False
81,#29 - Dr Anders Sandberg on 3 new resolutions for the Fermi paradox & how to colonise the universe,"Part 2 out now: <b>#33 - Dr Anders Sandberg on what if we ended ageing, solar flares & the annual risk of nuclear war</b><br><br>

The universe is so vast, yet we don’t see any alien civilizations. If they exist, where are they? Oxford University’s Anders Sandberg has an original answer: they’re ‘sleeping’, and for a very compelling reason.<br><br>

Because of the thermodynamics of computation, the colder it gets, the more computations you can do. The universe is getting exponentially colder as it expands, and as the universe cools, one Joule of energy gets worth more and more. If they wait long enough this can become a 10,000,000,000,000,000,000,000,000,000,000x gain. So, if a civilization wanted to maximize its ability to perform computations – its best option might be to lie in wait for trillions of years.<br><br>

Why would a civilization want to maximise the number of computations they can do? Because conscious minds are probably generated by computation, so doing twice as many computations is like living twice as long, in subjective time. Waiting will allow them to generate vastly more science, art, pleasure, or almost anything else they are likely to care about.<br><br>

<a href=""https://80000hours.org/podcast/episodes/anders-sandberg-fermi-paradox/?utm_campaign=podcast__anders-sandberg&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Full transcript, related links, and key quotes.</a><br><br>

But there’s no point waking up to find another civilization has taken over and used up the universe’s energy. So they’ll need some sort of monitoring to protect their resources from potential competitors like us.<br><br>

It’s plausible that this civilization would want to keep the universe’s matter concentrated, so that each part would be in reach of the other parts, even after the universe’s expansion. But that would mean changing the trajectory of galaxies during this dormant period. That we don’t see anything like that makes it more likely that these aliens have local outposts throughout the universe, and we wouldn’t notice them until we broke their rules. But breaking their rules might be our last action as a species.<br><br>

This ‘aestivation hypothesis’ is the invention of Dr Sandberg, a Senior Research Fellow at the Future of Humanity Institute at Oxford University, where he looks at low-probability, high-impact risks, predicting the capabilities of future technologies and very long-range futures for humanity.<br><br>

In this incredibly fun conversation we cover this and other possible explanations to the Fermi paradox, as well as questions like:<br><br>

* Should we want optimists or pessimists working on our most important problems?<br>
* How should we reason about low probability, high impact risks?<br>
* Would a galactic civilization want to stop the stars from burning?<br>
* What would be the best strategy for exploring and colonising the universe?<br>
* How can you stay coordinated when you’re spread across different galaxies?<br>
* What should humanity decide to do with its future?<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/a788e256-2a3b-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340457143,http://feedproxy.google.com/~r/80000HoursPodcast/~5/CuZleGB_8pM/440509452-80000-hours-anders-sandberg-fermi-paradox.mp3,False
82,"#28 - Dr Cotton-Barratt on why scientists should need insurance, PhD strategy & fast AI progresses","A researcher is working on creating a new virus – one more dangerous than any that exist naturally. They believe they’re being as careful as possible. After all, if things go wrong, their own life and that of their colleagues will be in danger. But if an accident is capable of triggering a global pandemic – hundreds of millions of lives might be at risk. How much additional care will the researcher actually take in the face of such a staggering death toll?<br><br>

In a new paper Dr Owen Cotton-Barratt, a Research Fellow at Oxford University’s Future of Humanity Institute, argues it’s impossible to expect them to make the correct adjustments. If they have an accident that kills 5 people – they’ll feel extremely bad. If they have an accident that kills 500 million people, they’ll feel even worse – but there’s no way for them to feel 100 million times worse. The brain simply doesn’t work that way.<br><br>

So, rather than relying on individual judgement, we could create a system that would lead to better outcomes: research liability insurance. <br><br>

<a href=""https://80000hours.org/podcast/episodes/owen-cotton-barratt-regulating-risky-research/?utm_campaign=podcast__owen-cb&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Links to learn more, summary and full transcript.</a><br><br>

Once an insurer assesses how much damage a particular project is expected to cause and with what likelihood – in order to proceed, the researcher would need to take out insurance against the predicted risk. In return, the insurer promises that they’ll pay out – potentially tens of billions of dollars – if things go really badly.<br><br>

This would force researchers think very carefully about the cost and benefits of their work – and incentivize the insurer to demand safety standards on a level that individual researchers can’t be expected to impose themselves.<br><br>

***Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: type '80,000 Hours' into your podcasting app.***<br><br>

Owen is <a href=""http://www.fhi.ox.ac.uk/rsp/"">currently hiring</a> for a selective, two-year research scholars programme at Oxford.<br><br>

In this wide-ranging conversation Owen and I also discuss:<br><br>

* Are academics wrong to value personal interest in a topic over its importance?<br>
* What fraction of research has very large potential negative consequences?<br>
* Why do we have such different reactions to situations where the risks are known and unknown?<br>
* The downsides of waiting for tenure to do the work you think is most important.<br>
* What are the benefits of specifying a vague problem like ‘make AI safe’ more clearly?<br>
* How should people balance the trade-offs between having a successful career and doing the most important work?<br>
* Are there any blind alleys we’ve gone down when thinking about AI safety?<br>
* Why did Owen give to an organisation whose research agenda he is skeptical of?<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/94255d20-2a3b-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340457230,http://feedproxy.google.com/~r/80000HoursPodcast/~5/syeOGSLemQg/435909588-80000-hours-owen-cotton-barratt-regulating-risky-research.mp3,False
83,#27 - Dr Tom Inglesby on careers and policies that reduce global catastrophic biological risks,"How about this for a movie idea: a main character has to prevent a new contagious strain of Ebola spreading around the world. She’s the best of the best. So good in fact, that her work on early detection systems contains the strain at its source. Ten minutes into the movie, we see the results of her work – nothing happens. Life goes on as usual. She continues to be amazingly competent, and nothing continues to go wrong. Fade to black. Roll credits.<br><br>

If your job is to prevent catastrophes, success is when nobody has to pay attention to you. But without regular disasters to remind authorities why they hired you in the first place, they can’t tell if you’re actually achieving anything. And when budgets come under pressure you may find that success condemns you to the chopping block. <br><br>

Dr Tom Inglesby, Director of the Center for Health Security at the Johns Hopkins Bloomberg School of Public Health, worries this may be about to happen to the scientists working on the ‘Global Health Security Agenda’.<br><br>

In 2014 Ebola showed the world why we have to detect and contain new diseases before they spread, and that when it comes to contagious diseases the nations of the world sink or swim together. Fifty countries decided to work together to make sure all their health systems were up to the challenge. Back then Congress provided 5 years’ funding to help some of the world’s poorest countries build the basic health security infrastructure necessary to control pathogens before they could reach the US.<br><br>

<a href=""https://80000hours.org/podcast/episodes/tom-inglesby-health-security/?utm_campaign=podcast__tom-inglesby&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Links to learn more, job opportunities, and full transcript.</a><br><br>

But with Ebola fading from public memory and no recent tragedies to terrify us, Congress may not renew that funding and the project could fall apart. (Learn more about how you can help: http://www.nti.org/analysis/articles/protect-us-investments-global-health-security/ )<br><br>

But there are positive signs as well - the center Inglesby leads recently received a $16 million grant from the Open Philanthropy Project to further their work preventing global catastrophes. It also runs the [Emerging Leaders in Biosecurity Fellowship](http://www.centerforhealthsecurity.org/our-work/emergingbioleaders/) to train the next generation of biosecurity experts for the US government. And Inglesby regularly testifies to Congress on the threats we all face and how to address them.<br><br>

In this in-depth interview we try to provide concrete guidance for listeners who want to to pursue a career in health security. Some of the topics we cover include:<br><br>

* Should more people in medicine work on security?<br>
* What are the top jobs for people who want to improve health security and how do they work towards getting them?<br>
* What people can do to protect funding for the Global Health Security Agenda.<br>
* Should we be more concerned about natural or human caused pandemics? Which is more neglected?<br>
* Should we be allocating more attention and resources to global catastrophic risk scenarios?<br>
* Why are senior figures reluctant to prioritize one project or area at the expense of another?  <br>
* What does Tom think about the idea that in the medium term, human-caused pandemics will pose a far greater risk than natural pandemics, and so we should focus on specific counter-measures?<br>
* Are the main risks and solutions understood, and it’s just a matter of implementation? Or is the principal task to identify and understand them?<br>
* How is the current US government performing in these areas?<br>
* Which agencies are empowered to think about low probability high magnitude events?<br>
And more...<br><br>

<b>Get this episode by subscribing: search for '80,000 Hours' in your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/781dc0fe-2a3b-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340457316,http://feedproxy.google.com/~r/80000HoursPodcast/~5/n3KnT0EdKe4/430943736-80000-hours-tom-inglesby-health-security.mp3,False
84,#26 - Marie Gibbons on how exactly clean meat is made & what's needed to get it in every supermarket,"First, decide on the type of animal. Next, pick the cell type. Then take a small, painless biopsy, and put the cells in a solution that makes them feel like they’re still in the body. Once the cells are in this comfortable state, they'll proliferate. One cell becomes two, two becomes four, four becomes eight, and so on. Continue until you have enough cells to make a burger, a nugget, a sausage, or a piece of bacon, then concentrate them until they bind into solid meat.<br><br>

It's all surprisingly straightforward in principle according to Marie Gibbons​, a research fellow with The Good Food Institute, who has been researching how to improve this process at Harvard Medical School. We might even see clean meat sold commercially within a year.<br><br>

The real technical challenge is developing large bioreactors and cheap solutions so that we can make huge volumes and drive down costs.<br><br>

This interview covers the science and technology involved at each stage of clean meat production, the challenges and opportunities that face cutting-edge researchers like Marie, and how you could become one of them.<br><br>

<a href=""https://80000hours.org/podcast/episodes/marie-gibbons-clean-meat/?utm_campaign=podcast__marie-gibbons&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Full transcript, key points, and links to learn more.</a> <br><br>

Marie’s research focuses on turkey cells. But as she explains, with clean meat the possibilities extend well beyond those of traditional meat. Chicken, cow, pig, but also panda - and even dinosaurs could be on the menus of the future.<br><br>

Today’s episode is hosted by Natalie Cargill, a barrister in London with a background in animal advocacy. Natalie and Marie also discuss:<br><br>

* Why Marie switched from being a vet to developing clean meat<br>
* For people who want to dedicate themselves to animal welfare, how does working in clean meat fare compared to other career options? How can people get jobs in the area?<br>
* How did this become an established field?<br>
* How important is the choice of animal species and cell type in this process?<br>
* What are the biggest problems with current production methods?<br>
* Is this kind of research best done in an academic setting, a commercial setting, or a balance between the two?<br>
* How easy will it be to get consumer acceptance?<br>
* How valuable would extra funding be for cellular agriculture?<br>
* Can we use genetic modification to speed up the process?<br>
* Is it reasonable to be sceptical of the possibility of clean meat becoming financially competitive with traditional meat any time in the near future?<br><br>

<b>Get this episode by subscribing to our podcast on the world’s most pressing problems and how to solve them: search for '80,000 Hours' in your podcasting app.</b><br><br>

<em>The 80,000 Hours Podcast is produced by Keiran Harris.</em>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/5d7a6d60-2a3b-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340457404,http://feedproxy.google.com/~r/80000HoursPodcast/~5/UAdTOK1w76A/427544028-80000-hours-marie-gibbons-clean-meat.mp3,False
85,#25 - Prof Robin Hanson on why we have to lie to ourselves about why we do what we do,"On February 2, 1685, England’s King Charles II was struck by a sudden illness. Fortunately his physicians were the best of the best. To reassure the public they kept them abreast of  the King’s treatment regimen. King Charles was made to swallow a toxic metal; had blistering agents applied to his scalp; had pigeon droppings attached to his feet; was prodded with a red-hot poker; given forty drops of ooze from “the skull of a man that was never buried”; and, finally, had crushed stones from the intestines of an East Indian goat forced down his throat. Sadly, despite these heroic efforts, he passed away the following week. <br><br>

Why did the doctors go this far?<br><br>

Prof, Robin Hanson, Associate Professor of Economics at George Mason University suspects that on top of any medical beliefs they also had a hidden motive: it needed to be clear, to the king and the public, that the physicians cared enormously about saving His Royal Majesty. Only by going ‘all out’ would they be protected against accusations of negligence should the King die. <br><br>

<a href=""https://80000hours.org/2018/03/robin-hanson-on-lying-to-ourselves/?utm_campaign=podcast__robin-hanson&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Full transcript, summary, and links to articles discussed in the show.</a><br><br>

If you believe Hanson, the same desire to be seen to care about our family and friends explains much of what’s perverse about our medical system today.<br><br>

And not just medicine - Robin thinks we’re mostly kidding ourselves when we say our charities exist to help others, our schools exist to educate students and our politics are about choosing wise policies. <br><br>

So important are hidden motives for navigating our social world that we have to deny them to ourselves, lest we accidentally reveal them to others.<br><br>

Robin is a polymath economist, who has come up with surprising and novel insight in a range of fields including psychology, politics and futurology. In this extensive episode we discuss his latest book with Kevin Simler, *The Elephant in the Brain: Hidden Motives in Everyday Life*, but also:<br><br>

* What was it like being part of a competitor group to the ‘World Wide Web’, and being beaten to the post?<br />
* If people aren’t going to school to learn, what’s education all about?<br />
* What split brain patients tell us about our ability to justify anything<br />
* The hidden motivations that shape religions<br />
* Why we choose the friends we do<br />
* Why is our attitude to medicine mysterious?<br />
* What would it look like if people were focused on doing as much good as possible? <br />
* Are we better off donating now, when we’re older, or even wait until well after our deaths?<br />
* How much of the behavior of ‘effective altruists’ can we assume is genuinely motivated by wanting to do as much good as possible?<br />
* What does Robin mean when he refers to effective altruism as a youth movement? Is that a good or bad thing?<br />
* And much more...",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/409ff12e-2a3b-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340457491,http://feedproxy.google.com/~r/80000HoursPodcast/~5/6MYDe0Tgz1Y/421240986-80000-hours-robin-hanson-on-lying-to-ourselves.mp3,False
86,"#24 - Stefan Schubert on why it’s a bad idea to break the rules, even if it’s for a good cause","How honest should we be? How helpful? How friendly? If our society claims to value honesty, for instance, but in reality accepts an awful lot of lying – should we go along with those lax standards? Or, should we attempt to set a new norm for ourselves?<br><br>

Dr Stefan Schubert, a researcher at the Social Behaviour and Ethics Lab at Oxford University, has been modelling this in the context of the effective altruism community. He thinks people trying to improve the world should hold themselves to very high standards of integrity, because their minor sins can impose major costs on the thousands of others who share their goals.<br><br>

<a href=""https://80000hours.org/2018/03/stefan-schubert-considering-considerateness/?utm_campaign=podcast__stefan-schubert&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Summary, related links and full transcript.</b></a><br><br>

In addition, when a norm is uniquely important to our situation, we should be willing to question society and come up with something different and hopefully better.<br><br>

But in other cases, we can be better off sticking with whatever our culture expects, both to save time, avoid making mistakes, and ensure others can predict our behaviour.<br><br>

In this interview Stefan offers a range of views on the projects and culture that make up ‘effective altruism’ - including where it’s going right and where it’s going wrong.<br><br>

Stefan did his PhD in formal epistemology, before moving on to a postdoc in political rationality at the London School of Economics, while working on advocacy projects to improve truthfulness among politicians. At the time the interview was recorded Stefan was a researcher at the Centre for Effective Altruism in Oxford. <br><br>

We discuss:<br><br>

* Should we trust our own judgement more than others’?<br>
* How hard is it to improve political discourse?<br>
* What should we make of well-respected academics writing articles that seem to be completely misinformed?<br>
* How is effective altruism (EA) changing? What might it be doing wrong?<br>
* How has Stefan’s view of EA changed?<br>
* Should EA get more involved in politics, or steer clear of it? Would it be a bad idea for a talented graduate to get involved in party politics?<br>
* How much should we cooperate with those with whom we have disagreements?<br>
* What good reasons are there to be inconsiderate?<br>
* Should effective altruism potentially focused on a more narrow range of problems?<br><br>

*The 80,000 Hours podcast is produced by Keiran Harris.*<br><br>

**If you subscribe to our podcast, you can listen at leisure on your phone, speed up the conversation if you like, and get notified about future episodes. You can do so by searching ‘80,000 Hours’ wherever you get your podcasts.**",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/2e8235a6-2a3b-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340457577,http://feedproxy.google.com/~r/80000HoursPodcast/~5/VdvHEESo6FY/416404905-80000-hours-24-stefan-schubert-on-why-its-a-bad-idea-to-break-the-rules-even-if-its-for-a-good-cause.mp3,False
87,"#23 - How to actually become an AI alignment researcher, according to Dr Jan Leike","Want to help steer the 21st century’s most transformative technology? First complete an undergrad degree in computer science and mathematics. Prioritize harder courses over easier ones. Publish at least one paper before you apply for a PhD. Find a supervisor who’ll have a lot of time for you. Go to the top conferences and meet your future colleagues. And finally, get yourself hired.<br /><br />

That’s Dr Jan Leike’s advice on how to join him as a Research Scientist at DeepMind, the world’s leading AI team.<br /><br />

Jan is also a Research Associate at the Future of Humanity Institute at the University of Oxford, and his research aims to make machine learning robustly beneficial. His current focus is getting AI systems to learn good ‘objective functions’ in cases where we can’t easily specify the outcome we actually want.<br /><br />

<a href=""https://80000hours.org/2018/03/jan-leike-ml-alignment/?utm_campaign=podcast__jan-leike&utm_source=80000+Hours+Podcast&utm_medium=podcast"" rel=""nofollow"" target=""_blank""><b>Full transcript, summary and links to learn more.</b></a><br><br>

How might you know you’re a good fit for research? <br /><br />

Jan says to check whether you get obsessed with puzzles and problems, and find yourself mulling over questions that nobody knows the answer to. To do research in a team you also have to be good at clearly and concisely explaining your new ideas to other people.<br /><br />

We also discuss:<br /><br />

* Where Jan's views differ from those expressed by <a href=""https://80000hours.org/2017/07/podcast-the-world-needs-ai-researchers-heres-how-to-become-one/?utm_campaign=podcast__jan-leike&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Dario Amodei in episode 3</a><br>
* Why is AGI safety one of the world’s most pressing problems? <br />
* Common misconceptions about AI<br />
* What are some of the specific things DeepMind is researching?<br />
* The ways in which today’s AI systems can fail<br>
* What are the best techniques available today for teaching an AI the right objective function?<br>
* What’s it like to have some of the world’s greatest minds as coworkers?<br />
* Who should do empirical research and who should do theoretical research<br />
* What’s the DeepMind application process like?<br />
* The importance of researchers being comfortable with the unknown.<br /><br />

*The 80,000 Hours Podcast is produced by Keiran Harris.*",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/1d4519b6-2a3b-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340457665,http://feedproxy.google.com/~r/80000HoursPodcast/~5/bCJCEtlKG28/414482838-80000-hours-jan-leike-ml-alignment.mp3,False
88,#22 - Dr Leah Utyasheva on the non-profit that figured out how to massively cut suicide rates,"How people kill themselves varies enormously depending on which means are most easily available. In the United States, suicide by firearm stands out. In Hong Kong, where most people live in high rise buildings, jumping from a height is more common. And in some countries in Asia and Africa with many poor agricultural communities, the leading means is drinking pesticide.<br><br>

There’s a good chance you’ve never heard of this issue before. And yet, of the 800,000 people who kill themselves globally each year 20% die from pesticide self-poisoning.<br><br>

<a href=""https://80000hours.org/2018/03/leah-utyasheva-pesticide-suicide-prevention/?utm_campaign=podcast__leah-utyasheva&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Full transcript, summary and links to articles discussed in today's show.</b></a><br><br>

Research suggests most people who try to kill themselves with pesticides reflect on the decision for less than 30 minutes, and that less than 10% of those who don't die the first time around will try again.<br><br>

Unfortunately, the fatality rate from pesticide ingestion is 40% to 70%.<br><br>

Having such dangerous chemicals near people's homes is therefore an enormous public health issue not only for the direct victims, but also the partners and children they leave behind.<br><br>

Fortunately researchers like Dr Leah Utyasheva have figured out a very cheap way to massively reduce pesticide suicide rates.<br><br>

In this episode, Leah and I discuss:<br><br>

* How do you prevent pesticide suicide and what’s the evidence it works?<br>
* How do you know that most people attempting suicide don’t want to die?<br>
* What types of events are causing people to have the crises that lead to attempted suicide?<br>
* How much money does it cost to save a life in this way?<br>
* How do you estimate the probability of getting law reform passed in a particular country?<br>
* Have you generally found politicians to be sympathetic to the idea of banning these pesticides? What are their greatest reservations?<br>
* The comparison of getting policy change rather than helping person-by-person<br>
* The importance of working with locals in places like India and Nepal, rather than coming in exclusively as outsiders<br>
* What are the benefits of starting your own non-profit versus joining an existing org and persuading them of the merits of the cause?<br>
* Would Leah in general recommend starting a new charity? Is it more exciting than it is scary?<br>
* Is it important to have an academic leading this kind of work?<br>
* How did The Centre for Pesticide Suicide Prevention get seed funding?<br>
* How does the value of saving a life from suicide compare to savings someone from malaria<br>
* Leah’s political campaigning for the rights of vulnerable groups in Eastern Europe <br>
* What are the biggest downsides of human rights work?",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/08a428e4-2a3b-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340457752,http://feedproxy.google.com/~r/80000HoursPodcast/~5/xOvjwqM0yF4/409896831-80000-hours-leah-utyasheva-pesticide-suicide-prevention.mp3,False
89,#21 - Holden Karnofsky on times philanthropy transformed the world & Open Phil’s plan to do the same,"The Green Revolution averted mass famine during the 20th century. The contraceptive pill gave women unprecedented freedom in planning their own lives. Both are widely recognised as scientific breakthroughs that transformed the world. But few know that those breakthroughs only happened when they did because of a philanthropist willing to take a risky bet on a new idea.<br><br>

Today’s guest, Holden Karnofsky, has been looking for philanthropy’s biggest success stories because he’s Executive Director of the Open Philanthropy Project, which gives away over $100 million per year - and he’s hungry for big wins.<br><br>

<a href=""https://80000hours.org/2018/02/holden-karnofsky-open-philanthropy/?utm_campaign=podcast__holden-karnofsky&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Full transcript, related links, job opportunities and summary of the interview.</b></a><br><br>

In the 1940s, poverty reduction overseas was not a big priority for many. But the Rockefeller Foundation decided to fund agricultural scientists to breed much better crops for the developing world - thereby massively increasing their food production.<br><br>

In the 1950s, society was a long way from demanding effective birth control. Activist Margaret Sanger had the idea for the pill, and endocrinologist Gregory Pincus the research team – but they couldn’t proceed without a $40,000 research check from biologist and women’s rights activist Katherine McCormick.<br><br>

In both cases, it was philanthropists rather than governments that led the way.<br><br>

The reason, according to Holden, is that while governments have enormous resources, they’re constrained by only being able to fund reasonably sure bets. Philanthropists can transform the world by filling the gaps government leaves - but to seize that opportunity they have to hire outstanding researchers, think long-term and be willing to fail most of the time. <br><br>

Holden knows more about this type of giving than almost anyone. As founder of GiveWell and then the Open Philanthropy Project, he has been working feverishly since 2007 to find outstanding giving opportunities. This practical experience has made him one of the most influential figures in the development of the school of thought that has come to be known as effective altruism.<br><br>

We’ve recorded this episode now because [the Open Philanthropy Project is hiring](https://www.openphilanthropy.org/get-involved/jobs) for a large number of positions, which we think would allow the right person to have a very large positive influence on the world. They’re looking for a large number of entry lever researchers to train up, 3 specialist researchers into potential risks from advanced artificial intelligence, as well as a Director of Operations, Operations Associate and General Counsel.<br><br>

But the conversation goes well beyond specifics about these jobs. We also discuss:<br><br>

* How did they pick the problems they focus on, and how will they change over time?<br>
* What would Holden do differently if he were starting Open Phil again today?<br>
* What can we learn from the history of philanthropy?<br>
* What makes a good Program Officer.<br>
* The importance of not letting hype get ahead of the science in an emerging field.<br>
* The importance of honest feedback for philanthropists, and the difficulty getting it.<br>
* How do they decide what’s above the bar to fund, and when it’s better to hold onto the money?<br>
* How philanthropic funding can most influence politics.<br>
* What Holden would say to a new billionaire who wanted to give away most of their wealth.<br>
* Why Open Phil is building a research field around the safe development of artificial intelligence<br>
* Why they invested in OpenAI.<br>
* Academia’s faulty approach to answering practical questions.<br>
* What potential utopias do people most want, according to opinion polls? <br><br>

Keiran Harris helped produce today’s episode.",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/ed1e1c88-2a3a-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340457839,http://feedproxy.google.com/~r/80000HoursPodcast/~5/j4w18P3tHF8/406051449-80000-hours-21-holden-karnofsky-open-philanthropy.mp3,False
90,#20 - Bruce Friedrich on inventing outstanding meat substitutes to end speciesism & factory farming,"Before the US Civil War, it was easier for the North to morally oppose slavery. Why? Because unlike the South they weren’t profiting much from its existence. The fight for abolition was partly won because many no longer saw themselves as having a selfish stake in its continuation.<br><br>

Bruce Friedrich, executive director of The Good Food Institute (GFI), thinks the same may be true in the fight against speciesism. 98% of people currently eat meat. But if eating meat stops being part of most people’s daily lives -- it should be a lot easier to convince them that farming practices are just as cruel as they look, and that the suffering of these animals really matters.<br><br>

<a href=""https://80000hours.org/2018/02/bruce-friedrich-good-food-institute/?utm_campaign=podcast__bruce-friedrich&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Full transcript, related links, job opportunities and summary of the interview.</b></a><br><br>

That’s why GFI is “working with scientists, investors, and entrepreneurs” to create plant-based meat, dairy and eggs as well as clean meat alternatives to animal products. In 2016, Animal Charity Evaluators named GFI one of its recommended charities.<br><br>

In this interview I’m joined by my colleague Natalie Cargill, and we ask Bruce about:<br><br>

* What’s the best meat replacement product out there right now?<br>
* How effective is meat substitute research for people who want to reduce animal suffering as much as possible?<br>
* When will we get our hands on clean meat? And why does Bruce call it clean meat, rather than in vitro meat or cultured meat?<br>
* What are the challenges of producing something structurally identical to meat?<br>
* Can clean meat be healthier than conventional meat?<br>
* Do plant-based alternatives have a better shot at success than clean meat?<br>
* Is there a concern that, even if the product is perfect, people still won’t eat it? Why might that happen?<br>
* What’s it like being a vegan in a family made up largely of hunters and meat-eaters?<br>
* What kind of pushback should be expected from the meat industry?<br><br>

Keiran Harris helped produce today’s episode.",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/d86923fa-2a3a-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340457927,http://feedproxy.google.com/~r/80000HoursPodcast/~5/V3LIE85sQco/401771421-80000-hours-20-bruce-friedrich-good-food-institute.mp3,False
91,#19 - Samantha Pitts-Kiefer on working next to the White House trying to prevent nuclear war,"Rogue elements within a state’s security forces enrich dozens of kilograms of uranium. It’s then assembled into a crude nuclear bomb. The bomb is transported on a civilian aircraft to Washington D.C, and loaded onto a delivery truck. The truck is driven by an American citizen midway between the White House and the Capitol Building. The driver casually steps out of the vehicle, and detonates the weapon. There are more than 80,000 instant deaths. There are also at least 100,000 seriously wounded, with nowhere left to treat them. <br><br>

<a href=""https://80000hours.org/2018/02/samantha-pk-nuclear-security/?utm_campaign=podcast__samantha-pk&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Full blog post about this episode, including a transcript, summary and links to resources mentioned in the show</b></a><br><br>

It’s likely that one of those immediately killed would be Samantha Pitts-Kiefer, who works only one block away from the White House.<br><br>

Samantha serves as Senior Director of The Global Nuclear Policy Program at the Nuclear Threat Initiative, and warns that the chances of a nuclear terrorist attack are alarmingly high. Terrorist groups have expressed a desire for nuclear weapons, and the material required to build those weapons is scattered throughout the world at a diverse range of sites – some of which lack the necessary  security. <br><br>

When you combine the massive death toll with the accompanying social panic and economic disruption – the consequences of a nuclear 9/11 would be a disasterare almost unthinkable. And yet, Samantha reminds us – we must confront the possibility.<br><br>

Clearly, this is far from the only nuclear nightmare. We also discuss:<br><br>

* In the case of nuclear war, what fraction of the world's population would die?<br>
* What is the biggest nuclear threat?<br>
* How concerned should we be about North Korea?<br>
* How often has the world experienced nuclear near misses?<br>
* How might a conflict between India and Pakistan escalate to the nuclear level?<br>
* How quickly must a president make a decision in the result of a suspected first strike?<br>
* Are global sources of nuclear material safely secured?<br>
* What role does cyber security have in preventing nuclear disasters?<br>
* How can we improve relations between nuclear armed states?<br>
* What do you think about the campaign for complete nuclear disarmament?<br>
* If you could tell the US government to do three things, what are the key priorities today? <br>
* Is it practical to get members of congress to pay attention to nuclear risks?<br>
* Could modernisation of nuclear weapons actually make the world safer?",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/c5c80734-2a3a-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340458013,http://feedproxy.google.com/~r/80000HoursPodcast/~5/TYHuvRRxxAE/399391563-80000-hours-samantha-pk-nuclear-security.mp3,False
92,#18 - Ofir Reich on using data science to end poverty & the spurious action-inaction distinction,"Ofir Reich started out doing math in the military, before spending 8 years in tech startups - but then made a sharp turn to become a data scientist focussed on helping the global poor.<br><br>

At UC Berkeley’s Center for Effective Global Action he helps prevent tax evasion by identifying fake companies in India, enable Afghanistan to pay its teachers electronically, and raise yields for Ethiopian farmers by messaging them when local conditions make it ideal to apply fertiliser. Or at least that’s the hope - he’s also working on ways to test whether those interventions actually work.
<br><br>

<a href=""https://80000hours.org/2018/01/ofir-reich-data-science/?utm_campaign=podcast__ofir-reich&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Full post about this episode, including a transcript and relevant links to learn more.</b></a><br><br>

Why dedicate his life to helping the global poor?<br><br>

Ofir sees little moral difference between harming people and failing to help them. After all, if you had to press a button to keep all of your money from going to charity, and you pressed that button, would that be an action, or an inaction? Is there even an answer?<br><br>

After reflecting on cases like this, he decided that to not engage with a problem is an active choice, one whose consequences he is just as morally responsible for as if he were directly involved. On top of his life philosophy we also discuss:
<br><br>

* The benefits of working in a top academic environment<br>
* How best to start a career in global development<br>
* Are RCTs worth the money? Should we focus on big picture policy change instead? Or more economic theory?<br>
* How the delivery standards of nonprofits compare to top universities<br>
* Why he doesn’t enjoy living in the San Francisco bay area<br>
* How can we fix the problem of most published research being false?<br>
* How good a career path is data science?<br>
* How important is experience in development versus technical skills?<br>
* How he learned much of what he needed to know in the army<br>
* How concerned should effective altruists be about burnout?<br><br>

Keiran Harris helped produce today’s episode.",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/b11f0bd4-2a3a-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340458101,http://feedproxy.google.com/~r/80000HoursPodcast/~5/T_fHj-YScII/392364360-80000-hours-18-ofir-reich-data-science.mp3,False
93,"#17 - Prof Will MacAskill on moral uncertainty, utilitarianism & how to avoid being a moral monster","Immanuel Kant is a profoundly influential figure in modern philosophy, and was one of the earliest proponents for universal democracy and international cooperation. He <a href=""https://80000hours.org/2018/01/will-macaskill-moral-philosophy/?utm_campaign=podcast__will-macaskill&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"">also thought</a> that women have no place in civil society, that it was okay to kill illegitimate children, and that there was a ranking in the moral worth of different races.<br><br>
 
Throughout history we’ve consistently believed, as common sense, truly horrifying things by today’s standards. According to University of Oxford Professor Will MacAskill, it’s extremely likely that we’re in the same boat today. If we accept that we’re probably making major moral errors, how should we proceed?<br><br>

<a href=""https://80000hours.org/2018/01/will-macaskill-moral-philosophy/?utm_campaign=podcast__will-macaskill&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"">Full transcript, key points and links to articles and career guides discussed in the show.</a><br><br>
 
If our morality is tied to common sense intuitions, we’re probably just preserving these biases and moral errors. Instead we need to develop a moral view that criticises common sense intuitions, and gives us a chance to move beyond them. And if humanity is going to spread to the stars it could be worth dedicating hundreds or thousands of years to moral reflection, lest we spread our errors far and wide.<br><br>
 
Will is an Associate Professor in Philosophy at Oxford University, author of Doing Good Better, and one of the co-founders of the effective altruism community. In this interview we discuss a wide range of topics:<br><br>
 
* How would we go about a ‘long reflection’ to fix our moral errors?<br>
* Will’s forthcoming book on how one should reason and act if you don't know which moral theory is correct. What are the practical implications of so-called ‘moral uncertainty’?<br>
* If we basically solve existential risks, what does humanity do next?<br>
* What are some of Will’s most unusual philosophical positions?<br>
* What are the best arguments for and against utilitarianism?<br>
* Given disagreements among philosophers, how much should we believe the findings of philosophy as a field?<br>
* What are some the biases we should be aware of within academia?<br>
* What are some of the downsides of becoming a professor?<br>
* What are the merits of becoming a philosopher?<br>
* How does the media image of EA differ to the actual goals of the community?<br>
* What kinds of things would you like to see the EA community do differently?<br>
* How much should we explore potentially controversial ideas?<br>
* How focused should we be on diversity?<br>
* What are the best arguments against effective altruism?<br><br>

<b>Get free, one-on-one career advice</b><br><br>

We’ve helped hundreds of people compare their options, get introductions, and find high impact jobs. If you want to work on global priorities research or other important questions in academia, <a href=""https://80000hours.org/coaching/?utm_campaign=podcast__will-macaskill"">find out if our coaching can help you.</a>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/98bf83de-2a3a-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340458188,http://feedproxy.google.com/~r/80000HoursPodcast/~5/nM4HfkadjLM/385939754-80000-hours-prof-will-macaskill-on-moral-uncertainty.mp3,False
94,#16 - Dr Hutchinson on global priorities research & shaping the ideas of intellectuals,"In the 40s and 50s neoliberalism was a fringe movement within economics. But by the 80s it had become a dominant school of thought in public policy, and achieved major policy changes across the English speaking world. How did this happen?<br><br>

In part because its leaders invested heavily in training academics to study and develop their ideas. Whether you think neoliberalism was good or bad, its history demonstrates the impact building a strong intellectual base within universities can have.<br><br>

Michelle Hutchinson is working to get a different set of ideas a hearing in academia by setting up the Global Priorities Institute (GPI) at Oxford University. The Institute, which is <a href=""https://globalprioritiesinstitute.org/opportunities/"">currently hiring for three roles</a>, aims to bring together outstanding philosophers and economists to research how to most improve the world. The hope is that it will spark widespread academic engagement with effective altruist thinking, which will hone the ideas and help them gradually percolate into society more broadly.<br><br>

<a href=""https://80000hours.org/2017/12/michelle-hutchinson-global-priorities/?utm_campaign=podcast__michelle-hutchinson&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Link to the full blog post about this episode including transcript and links to learn more</a><br><br>

Its <a href=""https://globalprioritiesinstitute.org/wp-content/uploads/2017/12/gpi-research-agenda.pdf"">research agenda</a> includes questions like:<br><br>

* How do we compare the good done by focussing on really different types of causes?<br>
* How does saving lives actually affect the world relative to other things we could do?<br>
* What are the biggest wins governments should be focussed on getting?<br><br>

Before moving to GPI, Michelle was the Executive Director of Giving What We Can and a founding figure of the effective altruism movement. She has a PhD in Applied Ethics from Oxford on prioritization and global health.<br><br>

We discuss:<br><br>

* What is global priorities research and why does it matter?<br>
* How is effective altruism seen in academia? Is it important to convince academics of the value of your work, or is it OK to ignore them?<br>
* Operating inside a university is quite expensive, so is it even worth doing? Who can pay for this kind of thing?<br>
* How hard is it to do something innovative inside a university? How serious are the administrative and other barriers?<br>
* Is it harder to fundraise for a new institute, or hire the right people?<br>
* Have other social movements benefitted from having a prominent academic arm?<br>
* How can people prepare themselves to get research roles at a place like GPI?<br>
* Many people want to have roles doing this kind of research. How many are actually cut out for it? What should those who aren’t do instead?<br>
* What are the odds of the Institute’s work having an effect on the real world?<br><br>

<b>Get free, one-on-one career advice</b><br><br>

We’ve helped hundreds of people compare their options, get introductions, and find high impact jobs. If you want to work on global priorities research or other important questions in academia, <a href=""https://80000hours.org/coaching/?utm_campaign=podcast__michelle-hutchinson&utm_source=80000+Hours+Podcast&utm_medium=podcast"">find out if our coaching can help you.</a>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/857abd3e-2a3a-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340458274,http://feedproxy.google.com/~r/80000HoursPodcast/~5/KC0wPuabuvk/373158677-80000-hours-michelle-hutchinson-on-shaping-the-ideas-of-intellectuals.mp3,False
95,#15 - Prof Tetlock on how chimps beat Berkeley undergrads and when it’s wise to defer to the wise,"Prof Philip Tetlock is a social science legend. Over forty years he has researched whose predictions we can trust, whose we can’t and why - and developed methods that allow all of us to be better at predicting the future.<br><br>

After the Iraq WMDs fiasco, the US intelligence services hired him to figure out how to ensure they’d never screw up that badly again. The result of that work – <em>Superforecasting</em> – was a media sensation in 2015.<br><br>

<a href=""https://80000hours.org/2017/11/prof-tetlock-predicting-the-future/?utm_campaign=podcast__phil-tetlock&utm_source=80000+Hours+Podcast&utm_medium=podcast""><b>Full transcript, brief summary, apply for coaching and links to learn more.</b></a><br><br>

It described Tetlock’s Good Judgement Project, which found forecasting methods so accurate they beat everyone else in open competition, including thousands of people in the intelligence services with access to classified information.<br><br>

Today he’s working to develop the best forecasting process ever, by combining top human and machine intelligence in the <a href=""https://www.hybridforecasting.com/"">Hybrid Forecasting Competition</a>, which you can sign up and participate in.<br><br>

We start by describing his key findings, and then push to the edge of what is known about how to foresee the unforeseeable:<br><br>

* Should people who want to be right just adopt the views of experts rather than apply their own judgement?<br>
* Why are Berkeley undergrads worse forecasters than dart-throwing chimps?<br>
* Should I keep my political views secret, so it will be easier to change them later?<br>
* How can listeners contribute to his latest cutting-edge research?<br>
* What do we know about our accuracy at predicting low-probability high-impact disasters?<br>
* Does his research provide an intellectual basis for populist political movements?<br>
* Was the Iraq War caused by bad politics, or bad intelligence methods?<br>
* What can we learn about forecasting from the 2016 election?<br>
* Can experience help people avoid overconfidence and underconfidence?<br>
* When does an AI easily beat human judgement?<br>
* Could more accurate forecasting methods make the world more dangerous?<br>
* How much does demographic diversity line up with cognitive diversity?<br>
* What are the odds we’ll go to war with China?<br>
* Should we let prediction tournaments run most of the government?<br><br>

Listen to it.

<b>Get free, one-on-one career advice.</b> Want to work on important social science research like Tetlock? We’ve helped hundreds of people compare their options and get introductions. <a href=""https://80000hours.org/coaching/?utm_campaign=podcast__phil-tetlock&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Find out if our coaching can help you.</a>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/6a0bd8f8-2a3a-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340458360,http://feedproxy.google.com/~r/80000HoursPodcast/~5/EGfrBdzx-ZI/358492934-80000-hours-prof-tetlock-on-how-chimps-beat-berkeley-undergrads-and-when-its-wise-to-defer-to-the-wise.mp3,False
96,#14 - Sharon Nunez & Jose Valle on going undercover to expose animal abuse,"What if you knew that ducks were being killed with pitchforks? Rabbits dumped alive into containers? Or pigs being strangled with forklifts? Would you be willing to go undercover to expose the crime?<br><br>

That’s a real question that confronts volunteers at Animal Equality (AE). In this episode we speak to Sharon Nunez and Jose Valle, who founded AE in 2006 and then grew it into a multi-million dollar international animal rights organisation. They’ve been chosen as one of the most effective animal protection orgs in the world by Animal Charity Evaluators for the last 3 consecutive years.<br><br>

<a href=""https://80000hours.org/2017/11/animal-equality-exposing-cruelty/?utm_campaign=podcast__sharon-nunez&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Blog post about the episode, including links and full transcript.</a><br><br>

<a href=""https://80000hours.org/2017/09/lewis-bollard-end-factory-farming/?utm_campaign=podcast__sharon-nunez&utm_source=80000+Hours+Podcast&utm_medium=podcast"">A related previous episode, strongly recommended: Lewis Bollard on how to end factory farming as soon as possible.</a><br><br>

In addition to undercover investigations AE has also designed a 3D virtual-reality farm experience called iAnimal360. People get to experience being trapped in a cage – in a room designed to kill then - and can’t just look away. How big an impact is this having on users?<br><br>

Sharon Nuñez and Jose Valle also tackle:<br><br>

* How do they track their goals and metrics week to week?<br>
* How much does an undercover investigation cost?<br>
* Why don’t people donate more to factory farmed animals, given that they’re the vast majority of animals harmed directly by humans?<br>
* How risky is it to attempt to build a career in animal advocacy?<br>
* What led to a change in their focus from bullfighting in Spain to animal farming?<br>
* How does working with governments or corporate campaigns compare with early strategies like creating new vegans/vegetarians?<br>
* Has their very rapid growth been difficult to handle?<br>
* What should our listeners study or do if they want to work in this area?<br>
* How can we get across the message that horrific cases are a feature - not a bug - of factory farming?<br>
* Do the owners or workers of factory farms ever express shame at what they do?",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/546df594-2a3a-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340458446,http://feedproxy.google.com/~r/80000HoursPodcast/~5/mgiCtEzgfuQ/355186310-80000-hours-sharon-nunez-jose-valle-on-going-undercover-to-expose-animal-abuse.mp3,False
97,#13 - Claire Walsh on testing which policies work & how to get governments to listen to the results,"In both rich and poor countries, government policy is often based on no evidence at all and many programs don’t work. This has particularly harsh effects on the global poor - in some countries governments only spend $100 on each citizen a year so they can’t afford to waste a single dollar.<br><br>

Enter MIT’s Poverty Action Lab (J-PAL). Since 2003 they’ve conducted experiments to figure out what policies actually help recipients, and then tried to get them implemented by governments and non-profits.<br><br>

Claire Walsh leads J-PAL’s Government Partnership Initiative, which works to evaluate policies and programs in collaboration with developing world governments, scale policies that have been shown to work, and generally promote a culture of evidence-based policymaking.<br><br>

<a href=""https://80000hours.org/2017/10/claire-walsh-evidence-in-development/?utm_campaign=podcast__claire-walsh&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Summary, links to career opportunities and topics discussed in the show.</a><br><br>

We discussed (her views only, not J-PAL’s):<br><br>

* How can they get evidence backed policies adopted? Do politicians in the developing world even care whether their programs actually work? Is the norm evidence-based policy, or policy-based evidence?<br>
* Is evidence-based policy an evidence-based strategy itself?<br>
* Which policies does she think would have a particularly large impact on human welfare relative to their cost?<br>
* How did she come to lead one of J-PAL’s departments at 29?<br>
* How do you evaluate the effectiveness of energy and environment programs (Walsh’s area of expertise), and what are the standout approaches in that area?<br>
* 80,000 Hours has warned people about the downsides of starting your career in a non-profit. Walsh started her career in a non-profit and has thrived, so are we making a mistake?<br>
* Other than J-PAL, what are the best places to work in development? What are the best subjects to study? Where can you go network to break into the sector?<br>
* Is living in poverty as bad as we think?<br><br>

And plenty of other things besides.<br><br>

We haven’t run an RCT to test whether this episode will actually help your career, but I suggest you listen anyway. Trust my intuition on this one.",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/4276b876-2a3a-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340458535,http://feedproxy.google.com/~r/80000HoursPodcast/~5/ES1GVVIWiek/349509882-80000-hours-13-claire-walsh-on-testing-which-policies-work-how-to-get-governments-to-listen-to-the-results.mp3,False
98,#12 - Dr Cameron works to stop you dying in a pandemic. Here’s what keeps her up at night.,"<em>“When you're in the middle of a crisis and you have to ask for money, you're already too late.”</em><br><br>

That’s Dr Beth Cameron, who leads <em>Global Biological Policy and Programs</em> at the Nuclear Threat Initiative.<br><br>

Beth should know. She has years of experience preparing for and fighting the diseases of our nightmares, on the <em>White House Ebola Taskforce</em>, in the <em>National Security Council</em> staff, and as the Assistant Secretary of Defense for <em>Nuclear, Chemical and Biological Defense Programs</em>.<br><br>

<a href=""https://80000hours.org/2017/10/beth-cameron-pandemic-preparedness/?utm_campaign=podcast__beth-cameron&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"">Summary, list of career opportunities, extra links to learn more and coaching application.</a><br><br>

Unfortunately, the countries of the world aren’t prepared for a crisis - and like children crowded into daycare, there’s a good chance something will make us all sick at once.<br><br>

During past pandemics countries have dragged their feet over who will pay to contain them, or struggled to move people and supplies where they needed to be. At the same time advanced biotechnology threatens to make it possible for terrorists to bring back smallpox - or create something even worse.<br><br>

In this interview we look at the current state of play in disease control, what needs to change, and how you can build the career capital necessary to make those changes yourself. That includes:<br><br>

* What and where to study, and where to begin a career in pandemic preparedness. Below you’ll find a lengthy list of people and places mentioned in the interview, and others we’ve had recommended to us.<br>
* How the Nuclear Threat Initiative, with just 50 people, collaborates with governments around the world to reduce the risk of nuclear or biological catastrophes, and whether they might want to hire you.<br>
* The best strategy for containing pandemics.<br>
* Why we lurch from panic, to neglect, to panic again when it comes to protecting ourselves from contagious diseases.<br>
* Current reform efforts within the World Health Organisation, and attempts to prepare partial vaccines ahead of time.<br>
* Which global health security groups most impress Beth, and what they’re doing.<br>
* What new technologies could be invented to make us safer.<br>
* Whether it’s possible to help solve the problem through mass advocacy.<br>
* Much more besides.<br><br>

<b><a href=""https://80000hours.org/coaching/?utm_campaign=podcast__beth-cameron"" title="""" class=""btn btn-primary"">Get free, one-on-one career advice to improve biosecurity</a></b><br><br>

Considering a relevant grad program like a biology PhD, medicine, or security studies? Able to apply for a relevant job already? We’ve helped dozens of people plan their careers to work on pandemic preparedness and put them in touch with mentors. <b>If you want to work on the problem discussed in this episode, you should apply for coaching:</b><br><br>

<a href=""https://80000hours.org/coaching/?utm_campaign=podcast__beth-cameron&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"" title="""" class=""btn btn-primary"">Read more</a>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/2bbf15f6-2a3a-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340458626,http://feedproxy.google.com/~r/80000HoursPodcast/~5/q7gproT0ccU/348492993-80000-hours-12-dr-cameron-works-to-stop-pandemics-killing-you-heres-what-keeps-her-up-at-night.mp3,False
99,#11 - Dr Spencer Greenberg on speeding up social science 10-fold & why plenty of startups cause harm,"Do most meat eaters think it’s wrong to hurt animals? Do Americans think climate change is likely to cause human extinction? What is the best, state-of-the-art therapy for depression? How can we make academics more intellectually honest, so we can actually trust their findings? How can we speed up social science research ten-fold? Do most startups improve the world, or make it worse?<br><br>

If you’re interested in these question, this interview is for you.<br><br>

<a href=""https://80000hours.org/2017/10/spencer-greenberg-social-science/?utm_campaign=podcast__spencer-greenberg&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"">Click for a full transcript, links discussed in the show, etc.</a><br><br>

A scientist, entrepreneur, writer and mathematician, Spencer Greenberg is constantly working to create tools to speed up and improve research and critical thinking. These include:<br><br>

* Rapid public opinion surveys to find out what most people actually think about animal consciousness, farm animal welfare, the impact of developing world charities and the likelihood of extinction by various different means;<br>
* Tools to enable social science research to be run en masse very cheaply;<br>
* ClearerThinking.org, a highly popular site for improving people’s judgement and decision-making;<br>
* Ways to transform data analysis methods to ensure that papers only show true findings;<br>
* Innovative research methods;<br>
* Ways to decide which research projects are actually worth pursuing.<br><br>

In this interview, Spencer discusses all of these and more. If you don’t feel like listening, that just shows that you have poor judgement and need to benefit from his wisdom even more!<br><br>

<b><a href=""https://80000hours.org/coaching/?utm_campaign=podcast__spencer-greenberg"" title="""" class=""btn btn-primary"">Get free, one-on-one career advice</a></b><br><br>

We’ve helped hundreds of people compare their options, get introductions, and find high impact jobs. <b>If you want to work on any of the problems discussed in this episode, find out if our coaching can help you.</b><br><br>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/140f9c8c-2a3a-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340458712,http://feedproxy.google.com/~r/80000HoursPodcast/~5/E196uDvNSPo/347339539-80000-hours-spencer-greenberg-on-speeding-up-social-science.mp3,False
100,#10 - Dr Nick Beckstead on how to spend billions of dollars preventing human extinction,"What if you were in a position to give away billions of dollars to improve the world? What would you do with it? This is the problem facing Program Officers at the Open Philanthropy Project - people like Dr Nick Beckstead.<br><br>

Following a PhD in philosophy, Nick works to figure out where money can do the most good. He’s been involved in major grants in a wide range of areas, including ending factory farming through technological innovation, safeguarding the world from advances in biotechnology and artificial intelligence, and spreading rational compassion.<br><br>

<a href=""https://80000hours.org/2017/10/nick-beckstead-giving-billions/?utm_campaign=podcast__nick-beckstead&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"">Full transcript, coaching application form, overview of the conversation, and links to resources discussed in the episode:</a><br><br>

This episode is a tour through some of the toughest questions ‘effective altruists’ face when figuring out how to best improve the world, including:<br><br>

* * Should we mostly try to help people currently alive, or future generations? Nick studied this question for years in his PhD thesis, <a href=""https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxuYmVja3N0ZWFkfGd4OjExNDBjZTcwNjMxMzRmZGE"">On the Overwhelming Importance of Shaping the Far Future</a>. (The first 31 minutes is a snappier version of <a href=""https://80k.link/toby-ord-from-nick"">my conversation with Toby Ord</a>.)<br>
* Is clean meat (aka *in vitro* meat) technologically feasible any time soon, or should we be looking for plant-based alternatives?<br>
* What are the greatest risks to human civilisation?<br>
* To stop malaria is it more cost-effective to use technology to <a href=""https://www.openphilanthropy.org/focus/scientific-research/miscellaneous/target-malaria-general-support"">eliminate mosquitos</a> than to distribute bed nets?<br>
* Should people who want to improve the future work for changes that will be very useful in a specific scenario, or just generally try to improve how well humanity makes decisions?<br>
* What specific jobs should our listeners take in order for Nick to be able to spend more money in useful ways to improve the world?<br>
* Should we expect the future to be better if the economy grows more quickly - or more slowly?<br><br>

<b>Get free, one-on-one career advice</b><br><br>

We’ve helped dozens of people compare between their options, get introductions, and jobs important for the the long-run future. <a href=""https://80000hours.org/coaching/?utm_campaign=nick-beckstead-podcast"" title="""" class=""btn btn-primary"">If you want to work on any of the problems discussed in this episode, find out if our coaching can help you.</a><br><br>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/f4b70f96-2a39-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340458798,http://feedproxy.google.com/~r/80000HoursPodcast/~5/LQmKlEj-dfo/346373135-80000-hours-10-nick-beckstead-on-how-to-spend-billions-of-dollars-to-prevent-human-extinction.mp3,False
101,"#9 - Christine Peterson on how insecure computers could lead to global disaster, and how to fix it","Take a trip to Silicon Valley in the 70s and 80s, when going to space sounded like a good way to get around environmental limits, people started cryogenically freezing themselves, and nanotechnology looked like it might revolutionise industry – or turn us all into grey goo.<br><br>

<a href=""https://80000hours.org/2017/10/christine-peterson-computer-security/?utm_campaign=podcast__christine-peterson&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"">Full transcript, coaching application form, overview of the conversation, and extra resources to learn more:</a><br><br>

In this episode of the 80,000 Hours Podcast Christine Peterson takes us back to her youth in the Bay Area, the ideas she encountered there, and what the dreamers she met did as they grew up. We also discuss how she came up with the term ‘open source software’ (and how she had to get someone else to propose it).<br><br>

Today Christine helps runs the Foresight Institute, which fills a gap left by for-profit technology companies – predicting how new revolutionary technologies could go wrong, and ensuring we steer clear of the downsides.<br><br>

We dive into:<br><br>

* Whether the poor security of computer systems poses a catastrophic risk for the world. Could all our essential services be taken down at once? And if so, what can be done about it?<br>
* Can technology ‘move fast and break things’ without eventually breaking the world? Would it be better for technology to advance more quickly, or more slowly?<br>
* How Christine came up with the term ‘open source software’ (and why someone else had to propose it).<br>
* Will AIs designed for wide-scale automated hacking make computers more or less secure?<br>
* Would it be good to radically extend human lifespan? Is it sensible to cryogenically freeze yourself in the hope of being resurrected in the future?<br>
* Could atomically precise manufacturing (nanotechnology) really work? Why was it initially so controversial and why did people stop worrying about it?<br>
* Should people who try to do good in their careers work long hours and take low salaries? Or should they take care of themselves first of all?<br>
* How she thinks the the effective altruism community resembles the scene she was involved with when she was wrong, and where it might be going wrong.<br><br>

<b>Get free, one-on-one career advice</b><br><br>

We’ve helped dozens of people compare between their options, get introductions, and jobs important for the the long-run future. <a href=""https://80000hours.org/coaching/?utm_campaign=christine-peterson-podcast"" title="""" class=""btn btn-primary"">If you want to work on any of the problems discussed in this episode, find out if our coaching can help you.</a><br><br>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/dcfbb442-2a39-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340458885,http://feedproxy.google.com/~r/80000HoursPodcast/~5/toWT7RkHXR4/345341004-80000-hours-christine-peterson-on-how-insecure-computers-could-lead-to-global-disaster-and-how-to-fix-it.mp3,False
102,#8 - Lewis Bollard on how to end factory farming in our lifetimes,"Every year tens of billions of animals are raised in terrible conditions in factory farms before being killed for human consumption. Over the last two years Lewis Bollard – Project Officer for Farm Animal Welfare at the Open Philanthropy Project – has conducted extensive research into the best ways to eliminate animal suffering in farms as soon as possible. This has resulted in $30 million in grants to farm animal advocacy.<br><br>

<a href=""https://80000hours.org/2017/09/lewis-bollard-end-factory-farming/?utm_campaign=podcast__lewis-bollard&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"">Full transcript, coaching application form, overview of the conversation, and extra resources to learn more:</a><br><br>

We covered almost every approach being taken, which ones work, and how individuals can best contribute through their careers.<br><br>

We also had time to venture into a wide range of issues that are less often discussed, including:<br><br>

* Why Lewis thinks insect farming would be worse than the status quo, and whether we should look for ‘humane’ insecticides;<br>
* How young people can set themselves up to contribute to scientific research into meat alternatives;<br>
* How genetic manipulation of chickens has caused them to suffer much more than their ancestors, but could also be used to make them better off;<br>
* Why Lewis is skeptical of vegan advocacy;<br>
* Why he doubts that much can be done to tackle factory farming through legal advocacy or electoral politics;<br>
* Which species of farm animals is best to focus on first;<br>
* Whether fish and crustaceans are conscious, and if so what can be done for them;<br>
* Many other issues listed below in the <i>Overview of the discussion</i>.<br><br>

<b>Get free, one-on-one career advice</b><br><br>

We’ve helped dozens of people compare between their options, get introductions, and jobs important for the the long-run future. <a href=""https://80000hours.org/coaching/?utm_campaign=lewis-bollard-podcast"" title="""" class=""btn btn-primary"">If you want to work on any of the problems discussed in this episode, find out if our coaching can help you.</a><br><br>

<b>Overview of the discussion</b><br><br>

**2m40s** What originally drew you to dedicate your career to helping animals and why did Open Philanthropy end up focusing on it?<br>
**5m40s** Do you have any concrete way of assessing the severity of animal suffering? <br>
**7m10s** Do you think the environmental gains are large compared to those that we might hope to get from animal welfare improvement?<br>
**7m55s** What grants have you made at Open Phil? How did you go about deciding which groups to fund and which ones not to fund?<br>
**9m50s** Why does Open Phil focus on chickens and fish? Is this the right call?<br>
<a href=""https://80000hours.org/2017/09/lewis-bollard-end-factory-farming?utm_campaign=podcast__lewis-bollard&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"">More...</a>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/bdf8a000-2a39-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340458972,http://feedproxy.google.com/~r/80000HoursPodcast/~5/_fj8SDT3Kqo/344291989-80000-hours-8-lewis-bollard-on-how-to-end-animal-agriculture-in-our-lifetimes.mp3,False
103,"#7 - Julia Galef on making humanity more rational, what EA does wrong, and why Twitter isn’t all bad","The scientific revolution in the 16th century was one of the biggest societal shifts in human history, driven by the discovery of new and better methods of figuring out who was right and who was wrong. <br><br>

Julia Galef - a well-known writer and researcher focused on improving human judgment, especially about high stakes questions - believes that if we could again develop new techniques to predict the future, resolve disagreements and make sound decisions together, it could dramatically improve the world across the board. We brought her in to talk about her ideas.<br><br>

<b>This interview complements a <a href=""https://80000hours.org/problem-profiles/improving-institutional-decision-making/?utm_campaign=podcast__julia-galef&utm_source=80000+Hours+Podcast&utm_medium=podcast"">new detailed review</a> of whether and how to follow Julia’s career path.</b>

<a href=""https://80000hours.org/2017/09/is-it-time-for-a-new-scientific-revolution-julia-galef-on-how-to-make-humans-smarter/?utm_campaign=podcast__julia-galef&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Apply for personalised coaching, see what questions are asked when, and read extra resources to learn more.</a><br><br>

Julia has been host of the Rationally Speaking podcast since 2010, co-founder of the Center for Applied Rationality in 2012, and is currently working for the Open Philanthropy Project on an investigation of expert disagreements.<br><br>

In our conversation we ended up speaking about a wide range of topics, including:<br><br>

* Her research on how people can have productive intellectual disagreements.<br>
* Why she once planned to become an urban designer.<br>
* Why she doubts people are more rational than 200 years ago.<br>
* What makes her a fan of Twitter (while I think it’s dystopian).<br>
* Whether people should write more books.<br>
* Whether it’s a good idea to run a podcast, and how she grew her audience.<br>
* Why saying you don’t believe X often won’t convince people you don’t.<br>
* Why she started a PhD in economics but then stopped.<br>
* Whether she would recommend an unconventional career like her own.<br>
* Whether the incentives in the intelligence community actually support sound thinking.<br>
* Whether big institutions will actually pick up new tools for improving decision-making if they are developed.<br>
* How to start out pursuing a career in which you enhance human judgement and foresight.<br><br>

<b>Get free, one-on-one career advice to help you improve judgement and decision-making</b><br><br>

We’ve helped dozens of people compare between their options, get introductions, and jobs important for the the long-run future. **<b>If you want to work on any of the problems discussed in this episode, find out if our coaching can help you:</b>**<br><br>

<a href=""https://80000hours.org/coaching/?utm_campaign=podcast__julia-galef&utm_source=80000+Hours+Podcast&utm_medium=podcast"" title="""" class=""btn btn-primary"">APPLY FOR COACHING</a><br><br>

<b>Overview of the conversation</b><br><br>

**1m30s** So what projects are you working on at the moment?<br>
**3m50s** How are you working on the problem of expert disagreement?<br>
**6m0s** Is this the same method as the double crux process that was developed at the Center for Applied Rationality?<br>
**10m** Why did the Open Philanthropy Project decide this was a very valuable project to fund?<br>
**13m** Is the double crux process actually that effective?<br>
**14m50s** Is Facebook dangerous?<br>
**17m** What makes for a good life? Can you be mistaken about having a good life?<br>
**19m** Should more people write books?<br>
<a href=""https://80000hours.org/2017/09/is-it-time-for-a-new-scientific-revolution-julia-galef-on-how-to-make-humans-smarter/?utm_campaign=podcast__julia-galef&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Read more...</a>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/a6d3673e-2a39-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340459058,http://feedproxy.google.com/~r/80000HoursPodcast/~5/_QTxz2ze8aQ/342196093-80000-hours-julia-galef-on-making-humanity-more-rational-urban-design-and-why-twitter-isnt-all-bad.mp3,False
104,#6 - Dr Toby Ord on why the long-term future matters more than anything else & what to do about it,"Of all the people whose well-being we should care about, only a small fraction are alive today. The rest are members of future generations who are yet to exist. Whether they’ll be born into a world that is flourishing or disintegrating – and indeed, whether they will ever be born at all – is in large part up to us. As such, the welfare of future generations should be our number one moral concern.<br><br>

This conclusion holds true regardless of whether your moral framework is based on common sense, consequences, rules of ethical conduct, cooperating with others, virtuousness, keeping options open – or just a sense of wonder about the universe we find ourselves in.<br><br>

That’s the view of Dr Toby Ord, a philosophy Fellow at the University of Oxford and co-founder of the effective altruism community. In this episode of the 80,000 Hours Podcast Dr Ord makes the case that aiming for a positive long-term future is likely the best way to improve the world.<br><br>

<a href=""https://80000hours.org/articles/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/?utm_campaign=podcast__toby-ord&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Apply for personalised coaching, see what questions are asked when, and read extra resources to learn more.</a><br><br>

We then discuss common objections to long-termism, such as the idea that benefits to future generations are less valuable than those to people alive now, or that we can’t meaningfully benefit future generations beyond taking the usual steps to improve the present.<br><br>

Later the conversation turns to how individuals can and have changed the course of history, what could go wrong and why, and whether plans to colonise Mars would actually put humanity in a safer position than it is today.<br><br>

This episode goes deep into the most distinctive features of our advice. It’s likely the most in-depth discussion of how 80,000 Hours and the effective altruism community think about the long term future and why - and why we so often give it top priority. <br><br>

It’s best to subscribe, so you can listen at leisure on your phone, speed up the conversation if you like, and get notified about future episodes. You can do so by searching ‘80,000 Hours’ wherever you get your podcasts.<br><br>

<b>Want to help ensure humanity has a positive future instead of destroying itself? We want to help.</b><br><br>

We’ve helped 100s of people compare between their options, get introductions, and jobs important for the the long-run future. <b>If you want to work on any of the problems discussed in this episode, such as artificial intelligence or biosecurity, <a href=""https://80000hours.org/coaching/?utm_campaign=podcast__toby-ord&utm_source=80000+Hours+Podcast&utm_medium=podcast"" title="""" class=""btn btn-primary"">find out if our coaching can help you.</a></b><br><br>

<b>Overview of the discussion</b><br><br>

<b>3m30s</b> - Why is the long-term future of humanity such a big deal, and perhaps the most important issue for us to be thinking about?<br>
<b>9m05s</b> - Five arguments that future generations matter<br>
<b>21m50s</b> - How bad would it be if humanity went extinct or civilization collapses? <br>
<b>26m40s</b> - Why do people start saying such strange things when this topic comes up?<br>
<b>30m30s</b> - Are there any other reasons to prioritize thinking about the long-term future of humanity that you wanted to raise before we move to objections?<br>
<b>36m10s</b> - What is this school of thought called?<br>
<a href="" https://80000hours.org/articles/why-the-long-run-future-matters-more-than-anything-else-and-what-we-should-do-about-it/?utm_campaign=podcast__toby-ord&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Read more...</a><br><br>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/8dc0237c-2a39-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340459146,http://feedproxy.google.com/~r/80000HoursPodcast/~5/lJPyTAyQuLw/341193732-80000-hours-6-dr-toby-ord-on-why-the-long-term-future-matters-more-than-anything-else-what-to-do-about-it.mp3,False
105,#5 - Alex Gordon-Brown on how to donate millions in your 20s working in quantitative trading,"<p>Quantitative financial trading is one of the highest paying parts of the world’s highest paying industry. 25 to 30 year olds with outstanding maths skills can earn millions a year in an obscure set of ‘quant trading’ firms, where they program computers with predefined algorithms to allow them to trade very quickly and effectively.<br><br>

<b>Update: we're headhunting people for quant trading roles</b><br><br>
 
Want to be kept up to date about particularly promising roles we're aware of for earning to give in quantitative finance?</p><p> <a href=""https://80000hours.typeform.com/to/JiDHyM?utm_campaign=blog-post__agb-interview"" title="""" class=""btn btn-primary""><b>Get notified by letting us know here.</b></a><br><br>

This makes it an attractive place to work for people who want to ‘earn to give’, and we know several people who are able to donate over a million dollars a year to effective charities by working in quant trading. Who are these people? What is the job like? And is there a risk that their work harms the world in other ways? <br><br>

<a href=""https://80000hours.org/2017/08/the-life-of-a-quant-trader-how-to-earn-and-donate-millions-within-a-few-years/?utm_campaign=podcast__agb&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"">Apply for personalised coaching, see what questions are asked when, and read extra resources to learn more.</a><br><br>

I spoke at length with Alexander Gordon-Brown, who has worked as a quant trader in London for the last three and a half years and donated hundreds of thousands of pounds. We covered:<br><br>

* What quant traders do and how much they earn.<br>
* Whether their work is beneficial or harmful for the world.<br>
* How to figure out if you’re a good personal fit for quant trading, and if so how to break into the industry.<br>
* Whether he enjoys the work and finds it motivating, and what other careers he considered.<br>
* What variety of positions are on offer, and what the culture is like in different firms.<br>
* How he decides where to donate, and whether he has persuaded his colleagues to join him.<br><br>

<b>Want to earn to give for effective charities in quantitative trading? We want to help.</b><br><br>
 
We’ve helped dozens of people plan their earning to give careers, and put them in touch with mentors. If you want to work in quant trading, apply for our free coaching service.<br><br>

<b><a href=""https://80000hours.org/coaching/?utm_campaign=podcast__agb&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"" title="""" class=""btn btn-primary"">APPLY FOR COACHING</a></b><br><br>

<b>What questions are asked when?</b><br><br>

1m30s - What is quant trading and how much do they earn?<br>
4m45s - How do quant trading firms manage the risks they face and avoid bankruptcy?<br>
7m05s - Do other traders also donate to charity and has Alex convinced them?<br>
9m45s - How do they track the performance of each trader?<br>
13m00s - What does the daily schedule of a quant trader look like? What do you do in the morning, afternoon, etc?<br>
<a href=""https://80000hours.org/2017/08/the-life-of-a-quant-trader-how-to-earn-and-donate-millions-within-a-few-years/?utm_campaign=podcast__agb&amp;utm_source=80000+Hours+Podcast&amp;utm_medium=podcast"">More...</a></p>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/77a03424-2a39-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340459234,http://feedproxy.google.com/~r/80000HoursPodcast/~5/2iRhlnoYT1w/339872675-80000-hours-alex-gordon-brown-on-how-to-donate-millions-in-your-20s-via-quantitative-trading-august-2017.mp3,False
106,#4 - Howie Lempel on pandemics that kill hundreds of millions and how to stop them,"What disaster is most likely to kill more than 10 million human beings in the next 20 years? Terrorism? Famine? An asteroid?<br><br>

Actually it’s probably a pandemic: a deadly new disease that spreads out of control. We’ve recently seen the risks with Ebola and swine flu, but they pale in comparison to the Spanish flu which killed 3% of the world’s population in 1918 to 1920. A pandemic of that scale today would kill 200 million.<br><br>

In this in-depth interview I speak to Howie Lempel, who spent years studying pandemic preparedness for the Open Philanthropy Project. We spend the first 20 minutes covering his work at the foundation, then discuss how bad the pandemic problem is, why it’s probably getting worse, and what can be done about it.<br><br>

<a href=""https://80000hours.org/2017/08/podcast-we-are-not-worried-enough-about-the-next-pandemic/?utm_campaign=podcast__howie-lempel&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Full transcript, apply for personalised coaching to help you work on pandemic preparedness, see what questions are asked when, and read extra resources to learn more.</a><br><br> 

In the second half we go through where you personally could study and work to tackle one of the worst threats facing humanity.<br><br>

<em>Want to help ensure we have no severe pandemics in the 21st century? We want to help.</em><br><br>

We’ve helped dozens of people formulate their plans, and put them in touch with academic mentors. If you want to work on pandemic preparedness safety, apply for our free coaching service.<br><br>

<a href=""https://80000hours.org/coaching/?utm_campaign=podcast__howie-lempel&utm_source=80000+Hours+Podcast&utm_medium=podcast""></b>APPLY FOR COACHING</b></a><br><br>

<b>2m</b> - What does the Open Philanthropy Project do? What’s it like to work there?<br>
<b>16m27s</b> - What grants did OpenPhil make in pandemic preparedness? Did they work out?<br>
<b>22m56s</b> - Why is pandemic preparedness such an important thing to work on?<br>
<b>31m23s</b> - How many people could die in a global pandemic? Is Contagion a realistic movie?<br>
<b>37m05s</b> - Why the risk is getting worse due to scientific discoveries<br>
<b>40m10s</b> - How would dangerous pathogens get released?<br>
<b>45m27s</b> - Would society collapse if a billion people die in a pandemic?<br>
<b>49m25s</b> - The plague, Spanish flu, smallpox, and other historical pandemics<br>
<b>58m30s</b> - How are risks affected by sloppy research security or the existence of factory farming?<br>
<b>1h7m30s</b> - What's already being done? Why institutions for dealing with pandemics are really insufficient.<br>
<b>1h14m30s</b> - What the World Health Organisation should do but can’t.<br>
<b>1h21m51s</b> - What charities do about pandemics and why they aren’t able to fix things<br>
<b>1h25m50s</b> - How long would it take to make vaccines?<br>
<b>1h30m40s</b> - What does the US government do to protect Americans? It’s a mess.<br>
<b>1h37m20s</b> - What kind of people do you know work on this problem and what are they doing?<br>
<b>1h46m30s</b> - Are there things that we ought to be banning or technologies that we should be trying not to develop because we're just better off not having them?<br>
<b>1h49m35s</b> - What kind of reforms are needed at the international level?<br>
<b>1h54m40s</b> - Where should people who want to tackle this problem go to work?<br>
<b>1h59m50s</b> - Are there any technologies we need to urgently develop?<br>
<b>2h04m20s</b> - What about trying to stop humans from having contact with wild animals?<br>
<b>2h08m5s</b> - What should people study if they're young and choosing their major; what should they do a PhD in? Where should they study, and with who?<br>
<a href=""https://80000hours.org/2017/08/podcast-we-are-not-worried-enough-about-the-next-pandemic/?utm_campaign=podcast__howie-lempel&utm_source=80000+Hours+Podcast&utm_medium=podcast"">More...</a>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/5b0a70fe-2a39-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340459322,http://feedproxy.google.com/~r/80000HoursPodcast/~5/AHxrcQ7WegM/339164985-80000-hours-howie-lempel-on-why-pandemics-could-kill-hundreds-of-millions-and-how-to-stop-them-august-2017.mp3,False
107,#3 - Dr Dario Amodei on OpenAI and how AI will change the world for good and ill,"Just two years ago OpenAI didn’t exist. It’s now among the most elite groups of machine learning researchers. They’re trying to make an AI that’s smarter than humans and have $1b at their disposal.<br><br>
    
Even stranger for a Silicon Valley start-up, it’s not a business, but rather a non-profit founded by Elon Musk and Sam Altman among others, to ensure the benefits of AI are distributed broadly to all of society.    <br><br>
    
I did a long interview with one of its first machine learning researchers, Dr Dario Amodei, to learn about:<br><br>
    
* OpenAI’s latest plans and research progress.<br>
* His paper *Concrete Problems in AI Safety*, which outlines five specific ways machine learning algorithms can act in dangerous ways their designers don’t intend - something OpenAI has to work to avoid.<br>
* How listeners can best go about pursuing a career in machine learning and AI development themselves.<br><br>

<a href=""https://80000hours.org/2017/07/podcast-the-world-needs-ai-researchers-heres-how-to-become-one/?utm_campaign=podcast__amodei&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Full transcript, apply for personalised coaching to work on AI safety, see what questions are asked when, and read extra resources to learn more.</a><br><br>
    
1m33s - What OpenAI is doing, Dario’s research and why AI is important   <br> 
13m - Why OpenAI scaled back its Universe project    <br>
15m50s - Why AI could be dangerous    <br>
24m20s - Would smarter than human AI solve most of the world’s problems?   <br> 
29m - Paper on five concrete problems in AI safety    <br>
43m48s - Has OpenAI made progress?    <br>
49m30s - What this back flipping noodle can teach you about AI safety    <br>
55m30s - How someone can pursue a career in AI safety and get a job at OpenAI    <br>
1h02m30s - Where and what should people study?    <br>
1h4m15s - What other paradigms for AI are there?    <br>
1h7m55s - How do you go from studying to getting a job? What places are there to work?    <br>
1h13m30s - If there's a 17-year-old listening here what should they start reading first?    <br>
1h19m - Is this a good way to develop your broader career options? Is it a safe move?    <br>
1h21m10s - What if you’re older and haven’t studied machine learning? How do you break in?    <br>
1h24m - What about doing this work in academia?    <br>
1h26m50s - Is the work frustrating because solutions may not exist?    <br>
1h31m35s - How do we prevent a dangerous arms race?    <br>
1h36m30s - Final remarks on how to get into doing useful work in machine learning<br>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/3f2a4fe4-2a39-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340459409,http://feedproxy.google.com/~r/80000HoursPodcast/~5/i3_g2xPATjw/334142741-80000-hours-dario-amodei-on-openai-and-how-ai-will-change-the-world-for-good-and-ill-july-07.mp3,False
108,"#2 - Prof David Spiegelhalter on risk, stats and improving understanding of science","Recorded in 2015 by Robert Wiblin with colleague Jess Whittlestone at the Centre for Effective Altruism, and recovered from the dusty 80,000 Hours archives.<br><br>

David Spiegelhalter is a statistician at the University of Cambridge and something of an academic celebrity in the UK.<br><br>
 
Part of his role is to improve the public understanding of risk - especially everyday risks we face like getting cancer or dying in a car crash. As a result he’s regularly in the media explaining numbers in the news, trying to assist both ordinary people and politicians focus on the important risks we face, and avoid being distracted by flashy risks that don’t actually have much impact.<br><br> 

<a href=""https://80000hours.org/2017/06/podcast-prof-david-spiegelhalter-on-risk-statistics-and-improving-the-public-understanding-of-science/?utm_campaign=podcast__spiegelhalter&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Summary, full transcript and extra links to learn more.</a><br><br>

To help make sense of the uncertainties we face in life he has had to invent concepts like the microlife, or a 30-minute change in life expectancy.
(<a href=""https://en.wikipedia.org/wiki/Microlife"">https://en.wikipedia.org/wiki/Microlife</a>)<br><br>

We wanted to learn whether he thought a lifetime of work communicating science had actually had much impact on the world, and what advice he might have for people planning their careers today.",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/2fe596b0-2a39-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340459498,http://feedproxy.google.com/~r/80000HoursPodcast/~5/R_EwHpGK6nE/329202537-80000-hours-prof-david-spiegelhalter-on-risk-statistics-and-public-understanding-of-science.mp3,False
109,#1 - Miles Brundage on the world's desperate need for AI strategists and policy experts,"Robert Wiblin, Director of Research at 80,000 Hours speaks with Miles Brundage, research fellow at the University of Oxford's Future of Humanity Institute. Miles studies the social implications surrounding the development of new technologies and has a particular interest in artificial general intelligence, that is, an AI system that could do most or all of the tasks humans could do.<br><br>

This interview complements <a href=""https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/?utm_campaign=podcast__miles_brundage&utm_source=80000+Hours+Podcast&utm_medium=podcast"">our profile of the importance of positively shaping artificial intelligence</a> and <a href=""https://80000hours.org/articles/ai-policy-guide/?utm_campaign=podcast__miles_brundage&utm_source=80000+Hours+Podcast&utm_medium=podcast"">our guide to careers in AI policy and strategy</a><br><br>

<a href=""https://80000hours.org/2017/06/the-world-desperately-needs-ai-strategists-heres-how-to-become-one/?utm_campaign=podcast__miles_brundage&utm_source=80000+Hours+Podcast&utm_medium=podcast"">Full transcript, apply for personalised coaching to work on AI strategy, see what questions are asked when, and read extra resources to learn more. </a><br><br>",https://feeds.backtracks.fm/feeds/series/d8f05142-2a38-11e9-bd0b-0ebe27f14992/episodes/1e64319e-2a39-11e9-bd0b-0ebe27f14992/images/main.jpg?1601340459587,http://feedproxy.google.com/~r/80000HoursPodcast/~5/GwtWBOZHOqA/326365514-80000-hours-miles-brundage-and-robert-wiblin-on-ai-strategy-and-policy-careers-may-2017.mp3,False
